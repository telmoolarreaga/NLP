{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e74c7bd",
   "metadata": {},
   "source": [
    "# **Transformers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890e489",
   "metadata": {},
   "source": [
    "## **1. DistilBERT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4930f44",
   "metadata": {},
   "source": [
    "### **Bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1b00c",
   "metadata": {},
   "source": [
    "En este experimento se aborda la clasificaci√≥n autom√°tica del sesgo en noticias utilizando t√©cnicas de Deep Learning basadas en Transformers, espec√≠ficamente DistilBERT. El objetivo es construir un modelo capaz de predecir si un texto pertenece a una de tres categor√≠as de sesgo, aprendiendo a capturar patrones sem√°nticos y estil√≠sticos del lenguaje sin intervenci√≥n manual en la extracci√≥n de caracter√≠sticas.\n",
    "\n",
    "DistilBERT es una versi√≥n ligera de BERT, preentrenada sobre grandes corpus de texto en ingl√©s. Su principal ventaja es un menor tama√±o y menor n√∫mero de parametros conservando un 97% de la precision de BERT \n",
    "\n",
    "Los objetivos que esperamos son los siguientes:\n",
    "\n",
    "- Entrenar un modelo DistilBERT fine-tuneado sobre un subset del dataset de noticias\n",
    "- Evaluar el rendimiento en t√©rminos de accuracy y macro F1, m√©tricas apropiadas para clasificaci√≥n multiclase, especialmente con clases desbalanceadas.\n",
    "- Identificar si el modelo es capaz de captar patrones sem√°nticos que diferencian los distintos tipos de sesgo en el contenido textual.\n",
    "\n",
    "Tambi√©n esperamos varias complicaciones siendo la primera la falta de potencia computacional por nuestra parte lo que no puede llevar a problemas como truncamiento de texto,tama√±o de dataset y clases desbalanceadas al intentar limitar el modelo por la falta de potencia que tenemos \n",
    "\n",
    "Nuestras expectativas son conseguir que el modelo capture relaciones semanticas generales aunque no sean perfectas ni mucho menos y conseguir que sea un modelo lo suficientemente bueno par aque se pudiese usar como base para mejoras futuras con mejores capacidades y el uso de GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e45b8",
   "metadata": {},
   "source": [
    "#### 1. Carga y Preparaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d805a",
   "metadata": {},
   "source": [
    "Para entrenar un modelo de NLP como DistilBERT es fundamental partir de un dataset limpio y bien estructurado. En este caso se utiliza train_clean.csv, un archivo previamente procesado que contiene noticias ya depuradas. Este dataset ha pasado por pasos de limpieza de texto, tales como:\n",
    "\n",
    "- Eliminaci√≥n de caracteres especiales, enlaces y signos de puntuaci√≥n innecesarios.\n",
    "- Normalizaci√≥n de may√∫sculas/min√∫sculas.\n",
    "- Correcci√≥n de espacios y saltos de l√≠nea.\n",
    "\n",
    "Del dataset completo se seleccionan solo las columnas necesarias que son content_clean (el texto de la noticia ya procesado y listo para tokenizar) y  bias (la etiqueta que indica el tipo de sesgo de la noticia, que puede tener tres posibles valores). Esta selecci√≥n garantiza que el modelo reciba √∫nicamente la informaci√≥n relevante y evita ruido innecesario que podr√≠a afectar el aprendizaje.\n",
    "\n",
    "Una vez filtradas las columnas, se procede a dividir el dataset en tres subconjuntos:\n",
    "\n",
    "- Entrenamiento (Train): aproximadamente el 72% del dataset total. Se utiliza para ajustar los pesos del modelo durante el fine-tuning y es importante que contenga ejemplos de todas las clases para que el modelo pueda aprender las diferencias sem√°nticas entre ellas.\n",
    "\n",
    "- Validaci√≥n (Validation): alrededor del 8% del dataset total. Se utiliza durante el entrenamiento para evaluar el modelo en datos no vistos y ajustar hiperpar√°metros, como el learning rate o el n√∫mero de epochs y nos ayuda a detectar overfitting, ya que si el modelo mejora en el train pero empeora en validaci√≥n, significa que no generaliza bien.\n",
    "\n",
    "- Prueba (Test): 20% restante del dataset. Este conjunto solo se utiliza despu√©s de finalizar el entrenamiento para medir el rendimiento real del modelo en datos completamente nuevos y nos permite calcular m√©tricas como accuracy, macro F1, precision y recall de manera objetiva.\n",
    "\n",
    "Para asegurar que cada conjunto contenga una proporci√≥n representativa de cada clase, se usa el par√°metro stratify de train_test_split, esto evita que algunas clases queden subrepresentadas en el conjunto de validaci√≥n o test, lo que podr√≠a sesgar la evaluaci√≥n y dar m√©tricas poco fiables y es especialmente importante en datasets desbalanceados, donde algunas clases podr√≠an ser minoritarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3d9275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SISTERS\\miniconda3\\envs\\nlp310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1220bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_clean</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>besides his most recent trip to quetta mr raha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poll prestigious colleges wo nt make you happi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>house speaker paul ryan at a private dinner ea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn president donald trump has reason to hope ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the controversial immigrationreform bill that ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       content_clean  bias\n",
       "0  besides his most recent trip to quetta mr raha...     0\n",
       "1  poll prestigious colleges wo nt make you happi...     0\n",
       "2  house speaker paul ryan at a private dinner ea...     2\n",
       "3  cnn president donald trump has reason to hope ...     0\n",
       "4  the controversial immigrationreform bill that ...     2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el dataset\n",
    "df = pd.read_csv(\"data/data_clean/train_clean.csv\")\n",
    "\n",
    "# Nos quedamos solo con las columnas relevantes para la tarea\n",
    "df = df[[\"content_clean\", \"bias\"]].dropna()  # Eliminamos filas con valores faltantes\n",
    "\n",
    "# Mostramos las primeras filas del DataFrame \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b47054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20143\n",
      "Validation: 2239\n",
      "Test: 5596\n",
      "Train: 20143\n",
      "Validation: 2239\n",
      "Test: 5596\n"
     ]
    }
   ],
   "source": [
    "# Dividimos los datos originales en conjuntos de entrenamiento y prueba\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"content_clean\"].tolist(),  # Lista de textos limpios\n",
    "    df[\"bias\"].tolist(),           # Lista de etiquetas de sesgo\n",
    "    test_size=0.2,                 # 20% de los datos se reservan para el test\n",
    "    random_state=42,               # Semilla para reproducibilidad\n",
    "    stratify=df[\"bias\"]             \n",
    ")\n",
    "\n",
    "# Dividimos el conjunto de entrenamiento en entrenamiento y validaci√≥n\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts,                    \n",
    "    train_labels,                   \n",
    "    test_size=0.1,                  \n",
    "    random_state=42,                \n",
    "    stratify=train_labels            \n",
    ")\n",
    "\n",
    "# Mostramos la cantidad de ejemplos en cada conjunto\n",
    "print(f\"Train: {len(train_texts)}\")\n",
    "print(f\"Validation: {len(val_texts)}\")\n",
    "print(f\"Test: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4cca8",
   "metadata": {},
   "source": [
    "####  2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555e488",
   "metadata": {},
   "source": [
    "Antes de alimentar los textos al modelo, es necesario convertirlos en una representaci√≥n que DistilBERT pueda procesar: los tokens. Para ello, utilizamos el DistilBertTokenizerFast, que transforma cada noticia en una secuencia de IDs num√©ricos correspondientes a subpalabras reconocidas por el modelo.\n",
    "\n",
    "Se aplica truncamiento y padding para que todas las secuencias tengan la misma longitud m√°xima (128 tokens en este caso). Esto permite que los lotes de entrenamiento sean consistentes y que el modelo pueda procesarlos de forma eficiente.\n",
    "\n",
    "Se tokenizan conjuntos limitados de datos (train, validation y test) para reducir el tiempo de ejecuci√≥n en CPU, seleccionando los primeros 3000, 1000 y 1000 ejemplos respectivamente. Esta estrategia permite experimentar de manera r√°pida sin perder la representatividad general de los datos, aunque sacrifica parte de la cobertura completa del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ca379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el tokenizador r√°pido de DistilBERT preentrenado\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"  # Tokenizador compatible con el modelo preentrenado\n",
    ")\n",
    "\n",
    "# Funci√≥n que tokeniza una lista de textos y los prepara para el modelo\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,             # Trunca los textos que exceden la longitud m√°xima\n",
    "        padding=\"max_length\",        # Rellena los textos m√°s cortos hasta la longitud m√°xima\n",
    "        max_length=128,              # Longitud m√°xima de tokens por entrada\n",
    "        return_tensors=\"pt\"          # Devuelve los tensores \n",
    "    )\n",
    "\n",
    "# Limitar la cantidad de datos \n",
    "max_train = 3000\n",
    "max_val   = 1000\n",
    "max_test  = 1000\n",
    "\n",
    "train_encodings = tokenize(train_texts[:max_train])  # Tokenizamos el conjunto de entrenamiento\n",
    "val_encodings   = tokenize(val_texts[:max_val])      # Tokenizamos el conjunto de validaci√≥n\n",
    "test_encodings  = tokenize(test_texts[:max_test])    # Tokenizamos el conjunto de test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98787cda",
   "metadata": {},
   "source": [
    "####  3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4294a71",
   "metadata": {},
   "source": [
    "Una vez que los textos se han convertido en representaciones num√©ricas mediante la tokenizaci√≥n, el siguiente paso es organizarlos junto con sus etiquetas para que el modelo pueda aprender de manera eficiente. Aqu√≠ es donde entran en juego los Datasets de PyTorch.\n",
    "\n",
    "Los Datasets act√∫an como contenedores estructurados de los datos, que permiten:\n",
    "\n",
    "1. Almacenar y vincular datos y etiquetas de forma coherente:\n",
    "Cada ejemplo del Dataset contiene tanto la representaci√≥n num√©rica del texto como su etiqueta correspondiente. Esto asegura que cada batch extra√≠do durante el entrenamiento tenga la informaci√≥n completa necesaria para que el modelo aprenda la relaci√≥n entre entrada y salida.\n",
    "\n",
    "2. Facilitar el acceso por √≠ndice:\n",
    "El modelo y el Trainer necesitan poder acceder r√°pidamente a ejemplos individuales o batches completos. El Dataset permite extraer datos por √≠ndice, lo que habilita operaciones como el entrenamiento en lotes (mini-batches) y el submuestreo de los datos para pruebas r√°pidas o validaciones parciales.\n",
    "\n",
    "3. Optimizar el manejo de memoria y procesamiento:\n",
    "Los conjuntos de datos pueden ser muy grandes, y no siempre es posible cargarlos completamente en memoria. Al estructurar los datos como Dataset, es posible trabajar con subconjuntos y cargar solo lo necesario para cada batch \n",
    "\n",
    "4. Compatibilidad con herramientas de PyTorch y Transformers:\n",
    "La creaci√≥n de Datasets permite integrarse de manera fluida con los DataLoaders y el Trainer de Hugging Face. Esto automatiza la creaci√≥n de batches, la gesti√≥n de tensores, el c√°lculo de m√©tricas y la evaluaci√≥n del modelo, haciendo el flujo de entrenamiento m√°s eficiente y reproducible.\n",
    "\n",
    "5. Flexibilidad para experimentaci√≥n:\n",
    "Al usar Datasets, se puede f√°cilmente crear subconjuntos de entrenamiento, validaci√≥n o test, controlar la cantidad de ejemplos procesados, y ajustar r√°pidamente los experimentos sin modificar los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfa9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Para cada ejemplo devolvemos los tensores de input_ids, attention_mask y la etiqueta\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Crear datasets de entrenamiento, validaci√≥n y test\n",
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset   = NewsDataset(val_encodings, val_labels)\n",
    "test_dataset  = NewsDataset(test_encodings, test_labels)\n",
    "\n",
    "\n",
    "train_subset = Subset(train_dataset, range(min(3000, len(train_dataset))))\n",
    "val_subset   = Subset(val_dataset,   range(min(1000, len(val_dataset))))\n",
    "test_subset  = Subset(test_dataset,  range(min(1000, len(test_dataset))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a198c85",
   "metadata": {},
   "source": [
    "####  4. Enternamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ff1b3",
   "metadata": {},
   "source": [
    "El entrenamiento del modelo DistilBERT consiste en ajustar los pesos del modelo preentrenado para que pueda clasificar correctamente los textos de noticias seg√∫n su sesgo. Partimos de un modelo ya preentrenado, lo que significa que DistilBERT ya ha aprendido representaciones ling√º√≠sticas generales a partir de grandes corpus de texto. Durante esta fase, estas representaciones se refinan espec√≠ficamente para nuestro problema de clasificaci√≥n de sesgo. Esto permite que el modelo aprenda patrones sem√°nticos y contextuales relevantes para distinguir entre las tres clases de sesgo definidas en nuestro dataset.\n",
    "\n",
    "Para hacer el entrenamiento m√°s manejable y eficiente, se trabaja sobre un subset limitado de los datos, seleccionando solo unos miles de ejemplos de entrenamiento y validaci√≥n. Esto reduce el tiempo de entrenamiento, especialmente importante cuando se trabaja en CPU, y permite iterar m√°s r√°pidamente sobre la configuraci√≥n de los hiperpar√°metros.\n",
    "\n",
    "El proceso de entrenamiento se gu√≠a por una funci√≥n de p√©rdida que compara las predicciones del modelo con las etiquetas reales, ajustando los pesos para minimizar el error. Al mismo tiempo, se aplican t√©cnicas como la regularizaci√≥n por weight decay y un peque√±o calentamiento de los pasos iniciales (warmup) para estabilizar el aprendizaje y evitar que el modelo se desestabilice al principio.\n",
    "\n",
    "Durante cada epoch, el modelo revisa todos los ejemplos de entrenamiento, actualizando gradualmente sus par√°metros. Adem√°s, se registran m√©tricas de desempe√±o como la p√©rdida, para monitorizar c√≥mo mejora el modelo a lo largo del tiempo. Este entrenamiento permite que DistilBERT adapte sus embeddings y capas internas a nuestro conjunto de noticias, buscando extraer patrones discriminativos √∫tiles para la clasificaci√≥n de sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a7e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el modelo DistilBERT preentrenado para clasificaci√≥n de secuencias\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",  \n",
    "    num_labels=3                 # N√∫mero de clases de salida para la tarea de clasificaci√≥n\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b950c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred                # Recibe la salida del modelo y las etiquetas reales\n",
    "    preds = np.argmax(logits, axis=1)        # Convierte los logits en la clase predicha\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)      # Calcula Accuracy\n",
    "    f1  = f1_score(labels, preds, average=\"macro\")  # Calcula Macro-F1 \n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40019a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SISTERS\\miniconda3\\envs\\nlp310\\lib\\site-packages\\transformers\\training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n para el trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                   \n",
    "    num_train_epochs=3,                      # Num de epochs\n",
    "    per_device_train_batch_size=8,           # Tama√±o del batch durante el entrenamiento\n",
    "    per_device_eval_batch_size=16,           # Tama√±o del batch durante la evaluaci√≥n\n",
    "    warmup_steps=20,                         # Num de steps\n",
    "    weight_decay=0.01,                       # weight decay\n",
    "    logging_steps=100,                       # loggear\n",
    "    save_steps=500,                           # guardar modelo\n",
    "    save_total_limit=1,                       # Limita checkpoints \n",
    "    no_cuda=True,                             \n",
    "    eval_strategy=\"epoch\",                    # Evaluar el modelo al final de cada epoch\n",
    "    report_to=\"none\"                        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70810b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\3333358261.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_subset = Subset(train_dataset, range(3000))  \n",
    "val_subset   = Subset(val_dataset, range(2500))    \n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset, \n",
    "    eval_dataset=val_subset,    \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e8f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 1:36:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.944100</td>\n",
       "      <td>0.885592</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.588982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>0.865070</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.622499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368800</td>\n",
       "      <td>1.075154</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Output ===\n",
      "TrainOutput(global_step=1125, training_loss=0.6955495376586914, metrics={'train_runtime': 5817.322, 'train_samples_per_second': 1.547, 'train_steps_per_second': 0.193, 'total_flos': 298056962304000.0, 'train_loss': 0.6955495376586914, 'epoch': 3.0})\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo usando el trainer configurado previamente\n",
    "train_output = trainer.train()\n",
    "\n",
    "#informaci√≥n  del entrenamiento\n",
    "print(\"\\n=== Train Output ===\")\n",
    "print(train_output)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a51f37",
   "metadata": {},
   "source": [
    "####  5. Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f76fd",
   "metadata": {},
   "source": [
    "Tras el entrenamiento de DistilBERT con nuestro subconjunto de datos, procedimos a evaluar el rendimiento del modelo tanto en el conjunto de validaci√≥n como en el de test, utilizando m√©tricas est√°ndar de clasificaci√≥n: accuracy, macro-F1, y p√©rdida (loss). Estas m√©tricas permiten valorar no solo la proporci√≥n de aciertos totales, sino tambi√©n c√≥mo se comporta el modelo frente a clases menos representadas.\n",
    "\n",
    "En las primeras epochs, los resultados de validaci√≥n mostraban una mejora gradual del accuracy y macro-F1, mientras que la p√©rdida disminu√≠a, lo que indica que el modelo estaba aprendiendo a discriminar entre las clases. Por ejemplo, en la primera epoch el accuracy rondaba el 59% y el macro-F1 estaba en torno a 0.59. Esto refleja que aunque el modelo estaba captando ciertas relaciones entre palabras y sesgos de los textos, a√∫n no era capaz de generalizar completamente.\n",
    "\n",
    "A medida que avanz√°bamos a la segunda y tercera epoch, observamos que el accuracy y macro-F1 aumentaban ligeramente hasta valores cercanos al 62-64%, mientras que la p√©rdida mostraba un comportamiento m√°s fluctuante. Este aumento se debe a que el modelo va refinando los embeddings internos y ajustando los pesos, captando patrones m√°s complejos de lenguaje que permiten diferenciar mejor entre noticias con sesgos distintos. La ligera subida en la p√©rdida en la tercera epoch se puede explicar por el tama√±o reducido del subconjunto de validaci√≥n y la limitaci√≥n de ejemplos, lo que genera cierta variabilidad en la evaluaci√≥n.\n",
    "\n",
    "El informe de clasificaci√≥n final sobre el conjunto de test confirma esta tendencia. Las m√©tricas por clase muestran que el modelo tiene mejor desempe√±o en las clases m√°s frecuentes o con patrones ling√º√≠sticos m√°s claros, mientras que las clases m√°s heterog√©neas o menos representadas presentan valores ligeramente inferiores de recall y f1-score. Esto es consistente con los resultados de macro-F1, que pondera todas las clases de manera equitativa, mostrando que el modelo generaliza razonablemente pero todav√≠a tiene margen de mejora.\n",
    "\n",
    "Estos resultados se lograron gracias a varias decisiones clave en el proceso: la pretokenizaci√≥n y truncado de textos a una longitud m√°xima de 128 tokens permiti√≥ que el modelo procesara la informaci√≥n de forma eficiente, mientras que el uso de subconjuntos representativos para entrenamiento y validaci√≥n redujo dr√°sticamente los tiempos de ejecuci√≥n, manteniendo patrones significativos en los datos. Adem√°s, la configuraci√≥n de DistilBERT como modelo base, facilit√≥ entrenar varias epochs sin necesidad de GPU, consiguiendo que los embeddings se ajustaran a nuestro problema espec√≠fico de clasificaci√≥n de sesgo en noticias.\n",
    "\n",
    "En conjunto, los resultados indican que el modelo ha logrado capturar relaciones sem√°nticas √∫tiles para la clasificaci√≥n de bias, aunque su desempe√±o sigue limitado por el tama√±o del dataset y la reducci√≥n de ejemplos para acelerar el entrenamiento. Para mejorar, ser√≠a clave aumentar la cantidad de datos, permitir que el modelo vea m√°s ejemplos por clase y considerar t√©cnicas de fine-tuning m√°s profundo o aumento de datos, lo que podr√≠a ayudar a que las clases menos representadas obtengan mayor f1-score y que la p√©rdida de validaci√≥n se estabilice a√∫n m√°s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Results ===\n",
      "eval_loss: 1.0752\n",
      "eval_accuracy: 0.6240\n",
      "eval_macro_f1: 0.6217\n",
      "eval_runtime: 121.7037\n",
      "eval_samples_per_second: 8.2170\n",
      "eval_steps_per_second: 0.5180\n",
      "epoch: 3.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_4552\\97867553.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6254    0.6218    0.6236       349\n",
      "           1     0.6546    0.6059    0.6293       269\n",
      "           2     0.6609    0.6990    0.6794       382\n",
      "\n",
      "    accuracy                         0.6470      1000\n",
      "   macro avg     0.6470    0.6422    0.6441      1000\n",
      "weighted avg     0.6468    0.6470    0.6464      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/distilbert_debug\\\\tokenizer_config.json',\n",
       " 'models/distilbert_debug\\\\special_tokens_map.json',\n",
       " 'models/distilbert_debug\\\\vocab.txt',\n",
       " 'models/distilbert_debug\\\\added_tokens.json',\n",
       " 'models/distilbert_debug\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluamos el modelo usando los datos de validaci√≥n\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "# Mostramos los resultados de validaci√≥n\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")  # 4 decimales \n",
    "\n",
    "# Realizamos predicciones sobre el subconjunto de test\n",
    "preds_output = trainer.predict(test_subset)\n",
    "# Convertimos las predicciones del modelo a etiquetas finales tomando la clase con mayor probabilidad\n",
    "preds = np.argmax(preds_output.predictions, axis=1)\n",
    "# Obtenemos las etiquetas reales del conjunto de test para compararlas\n",
    "labels = preds_output.label_ids\n",
    "\n",
    "# Mostramos el informe de clasificaci√≥n con m√©tricas como precisi√≥n, recall y F1-score\n",
    "print(\"\\n=== Test Classification Report ===\")\n",
    "print(classification_report(labels, preds, digits=4))  \n",
    "\n",
    "# Guardamos el modelo e\n",
    "trainer.save_model(\"models/distilbert_debug\")\n",
    "# Guardamos el tokenizador \n",
    "tokenizer.save_pretrained(\"models/distilbert_debug\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac86ec",
   "metadata": {},
   "source": [
    "#### 6, Comparaci√≥n con otros modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c727a6",
   "metadata": {},
   "source": [
    "En la tarea de clasificaci√≥n de bias, los modelos basados en representaciones tradicionales de texto contin√∫an mostrando el mejor rendimiento. En particular, TF-IDF alcanza una Accuracy de 0.71 y un Macro-F1 de 0.70, lo que indica que la informaci√≥n basada en la frecuencia y relevancia de t√©rminos captura de manera muy efectiva los patrones discriminativos necesarios para esta tarea. Bag-of-Words, aunque ligeramente inferior, sigue mostrando un desempe√±o s√≥lido, con una Accuracy de 0.63 y un Macro-F1 de 0.63, reflejando la importancia de considerar la presencia de palabras clave concretas en lugar de su contexto sem√°ntico m√°s amplio.\n",
    "\n",
    "Los embeddings no contextuales, como Word2Vec y FastText, obtienen resultados intermedios, con Accuracy alrededor de 0.54 y Macro-F1 de 0.53‚Äì0.54. Estos modelos capturan relaciones sem√°nticas entre palabras, pero al aplicarlos con un clasificador lineal como Logistic Regression, no superan el rendimiento de TF-IDF. Esto sugiere que, para la clasificaci√≥n de bias, la mera presencia de t√©rminos clave es m√°s determinante que la informaci√≥n sem√°ntica distribuida que estos embeddings aportan.\n",
    "\n",
    "Los embeddings contextuales, incluyendo Sentence Transformers y BERT, muestran un comportamiento mixto. Sentence Transformers alcanza una Accuracy de 0.535 y un Macro-F1 de 0.535, similar a los embeddings no contextuales. Por otro lado, BERT sin un entrenamiento exhaustivo obtiene un desempe√±o muy bajo (Accuracy 0.366, Macro-F1 0.366). Esto refleja la dificultad de ajustar modelos de gran capacidad con datasets limitados y clases desbalanceadas, donde el modelo puede sobreajustarse o no generalizar correctamente.\n",
    "\n",
    "Al comparar estos resultados con DistilBERT fine-tuneado, se observa un patr√≥n coherente. DistilBERT, aunque ya se ha ajustado a la tarea de bias, sigue sin superar a TF-IDF en Accuracy o Macro-F1. La explicaci√≥n principal es que DistilBERT est√° dise√±ado para capturar relaciones contextuales complejas y matices sem√°nticos, pero la clasificaci√≥n de bias depende en gran medida de palabras y expresiones espec√≠ficas. En este caso, la riqueza contextual de DistilBERT no se traduce autom√°ticamente en mejor rendimiento, especialmente cuando el conjunto de datos es peque√±o y el fine-tuning no es suficiente para extraer patrones fiables. Por el contrario, TF-IDF, al enfatizar la frecuencia relativa de t√©rminos clave, resulta m√°s robusto y eficaz para esta tarea concreta.\n",
    "\n",
    "En conclusi√≥n, aunque DistilBERT fine-tuneado aporta capacidades contextuales avanzadas, los modelos tradicionales como TF-IDF siguen siendo m√°s efectivos para la clasificaci√≥n de bias en datasets limitados. Los embeddings no contextuales y contextuales muestran un desempe√±o moderado, pero no logran superar a las t√©cnicas basadas en frecuencia de t√©rminos debido a la naturaleza de la tarea y la dependencia de palabras clave espec√≠ficas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56c4c2",
   "metadata": {},
   "source": [
    "### **Topic and Source**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec4bc7",
   "metadata": {},
   "source": [
    "#### 1. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97333015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_clean</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>besides his most recent trip to quetta mr raha...</td>\n",
       "      <td>terrorism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poll prestigious colleges wo nt make you happi...</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>house speaker paul ryan at a private dinner ea...</td>\n",
       "      <td>us_house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn president donald trump has reason to hope ...</td>\n",
       "      <td>white_house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the controversial immigrationreform bill that ...</td>\n",
       "      <td>immigration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       content_clean        topic\n",
       "0  besides his most recent trip to quetta mr raha...    terrorism\n",
       "1  poll prestigious colleges wo nt make you happi...    education\n",
       "2  house speaker paul ryan at a private dinner ea...     us_house\n",
       "3  cnn president donald trump has reason to hope ...  white_house\n",
       "4  the controversial immigrationreform bill that ...  immigration"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# Cargamos el dataset limpio desde CSV\n",
    "df = pd.read_csv(\"data/data_clean/train_clean.csv\")\n",
    "\n",
    "# Nos quedamos solo con las columnas relevantes para la tarea (source y topic)\n",
    "# Ejemplo: para clasificar la fuente\n",
    "df_source = df[[\"content_clean\", \"source\"]].dropna()  # Eliminamos filas con valores faltantes\n",
    "# Ejemplo: para clasificar el tema\n",
    "df_topic = df[[\"content_clean\", \"topic\"]].dropna()    # Eliminamos filas con valores faltantes\n",
    "\n",
    "# Mostramos las primeras filas para verificar la carga\n",
    "df_source.head()\n",
    "df_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38fe2e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source - Train: 20106, Validation: 2234, Test: 5586\n",
      "Topic - Train: 20143, Validation: 2239, Test: 5596\n"
     ]
    }
   ],
   "source": [
    "# Filtrar clases con al menos 2 ejemplos para evitar errores con stratify\n",
    "counts_source = df_source['source'].value_counts()\n",
    "df_source_filtered = df_source[df_source['source'].isin(counts_source[counts_source >= 2].index)]\n",
    "\n",
    "# Dividir en train/test\n",
    "train_texts_source, test_texts_source, train_labels_source, test_labels_source = train_test_split(\n",
    "    df_source_filtered[\"content_clean\"].tolist(),\n",
    "    df_source_filtered[\"source\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_source_filtered[\"source\"]\n",
    ")\n",
    "\n",
    "# Dividir train/val\n",
    "train_texts_source, val_texts_source, train_labels_source, val_labels_source = train_test_split(\n",
    "    train_texts_source,\n",
    "    train_labels_source,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_labels_source\n",
    ")\n",
    "\n",
    "# Crear diccionario para mapear etiquetas a enteros\n",
    "source_classes = list(set(train_labels_source + val_labels_source + test_labels_source))\n",
    "source2id = {cls: i for i, cls in enumerate(source_classes)}\n",
    "\n",
    "# Mapear etiquetas\n",
    "train_labels_source = [source2id[label] for label in train_labels_source]\n",
    "val_labels_source   = [source2id[label] for label in val_labels_source]\n",
    "test_labels_source  = [source2id[label] for label in test_labels_source]\n",
    "\n",
    "print(f\"Source - Train: {len(train_texts_source)}, Validation: {len(val_texts_source)}, Test: {len(test_texts_source)}\")\n",
    "\n",
    "\n",
    "# --- TOPIC ---\n",
    "\n",
    "# Filtrar clases con al menos 2 ejemplos\n",
    "counts_topic = df_topic['topic'].value_counts()\n",
    "df_topic_filtered = df_topic[df_topic['topic'].isin(counts_topic[counts_topic >= 2].index)]\n",
    "\n",
    "# Dividir en train/test\n",
    "train_texts_topic, test_texts_topic, train_labels_topic, test_labels_topic = train_test_split(\n",
    "    df_topic_filtered[\"content_clean\"].tolist(),\n",
    "    df_topic_filtered[\"topic\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_topic_filtered[\"topic\"]\n",
    ")\n",
    "\n",
    "# Dividir train/val\n",
    "train_texts_topic, val_texts_topic, train_labels_topic, val_labels_topic = train_test_split(\n",
    "    train_texts_topic,\n",
    "    train_labels_topic,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_labels_topic\n",
    ")\n",
    "\n",
    "# Crear diccionario para mapear etiquetas a enteros\n",
    "topic_classes = list(set(train_labels_topic + val_labels_topic + test_labels_topic))\n",
    "topic2id = {cls: i for i, cls in enumerate(topic_classes)}\n",
    "\n",
    "# Mapear etiquetas\n",
    "train_labels_topic = [topic2id[label] for label in train_labels_topic]\n",
    "val_labels_topic   = [topic2id[label] for label in val_labels_topic]\n",
    "test_labels_topic  = [topic2id[label] for label in test_labels_topic]\n",
    "\n",
    "print(f\"Topic - Train: {len(train_texts_topic)}, Validation: {len(val_texts_topic)}, Test: {len(test_texts_topic)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a198d6",
   "metadata": {},
   "source": [
    "#### 2. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6612d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "max_train = 3000\n",
    "max_val   = 1000\n",
    "max_test  = 1000\n",
    "\n",
    "# Source\n",
    "train_encodings_source = tokenize(train_texts_source[:max_train])\n",
    "val_encodings_source   = tokenize(val_texts_source[:max_val])\n",
    "test_encodings_source  = tokenize(test_texts_source[:max_test])\n",
    "\n",
    "# Topic\n",
    "train_encodings_topic = tokenize(train_texts_topic[:max_train])\n",
    "val_encodings_topic   = tokenize(val_texts_topic[:max_val])\n",
    "test_encodings_topic  = tokenize(test_texts_topic[:max_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98205e53",
   "metadata": {},
   "source": [
    "#### 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5860614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Source\n",
    "train_dataset_source = NewsDataset(train_encodings_source, train_labels_source)\n",
    "val_dataset_source   = NewsDataset(val_encodings_source, val_labels_source)\n",
    "test_dataset_source  = NewsDataset(test_encodings_source, test_labels_source)\n",
    "\n",
    "# Topic\n",
    "train_dataset_topic = NewsDataset(train_encodings_topic, train_labels_topic)\n",
    "val_dataset_topic   = NewsDataset(val_encodings_topic, val_labels_topic)\n",
    "test_dataset_topic  = NewsDataset(test_encodings_topic, test_labels_topic)\n",
    "\n",
    "\n",
    "# --- SUBCONJUNTOS LIMITADOS ---\n",
    "\n",
    "train_subset_source = Subset(train_dataset_source, range(min(2000, len(train_dataset_source))))\n",
    "val_subset_source   = Subset(val_dataset_source, range(min(800, len(val_dataset_source))))\n",
    "test_subset_source  = Subset(test_dataset_source, range(min(800, len(test_dataset_source))))\n",
    "\n",
    "train_subset_topic = Subset(train_dataset_topic, range(min(2000, len(train_dataset_topic))))\n",
    "val_subset_topic   = Subset(val_dataset_topic, range(min(800, len(val_dataset_topic))))\n",
    "test_subset_topic  = Subset(test_dataset_topic, range(min(800, len(test_dataset_topic))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db0a95",
   "metadata": {},
   "source": [
    "#### 4. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cc12818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# N√∫mero de clases calculado din√°micamente\n",
    "num_labels_source = len(df_source[\"source\"].unique())\n",
    "num_labels_topic  = len(df_topic[\"topic\"].unique())\n",
    "\n",
    "# --- Source ---\n",
    "model_source = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels_source\n",
    ")\n",
    "\n",
    "# --- Topic ---\n",
    "model_topic = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels_topic\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7af2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1  = f1_score(labels, preds, average=\"macro\")\n",
    "    \n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c0bf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    no_cuda=True,              # Forzar CPU\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a10d440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_3480\\2952275382.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_source = Trainer(\n",
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_3480\\2952275382.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_topic = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Source ---\n",
    "trainer_source = Trainer(\n",
    "    model=model_source,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_source,\n",
    "    eval_dataset=val_subset_source,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# --- Topic ---\n",
    "trainer_topic = Trainer(\n",
    "    model=model_topic,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_topic,\n",
    "    eval_dataset=val_subset_topic,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SISTERS\\AppData\\Local\\Temp\\ipykernel_3480\\1017584255.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='124' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [124/750 08:15 < 42:20, 0.25 it/s, Epoch 0.49/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_output_source = trainer_source.train()\n",
    "print(\"\\n=== Train Output Source ===\")\n",
    "print(train_output_source)\n",
    "\n",
    "# Topic\n",
    "train_output_topic = trainer_topic.train()\n",
    "print(\"\\n=== Train Output Topic ===\")\n",
    "print(train_output_topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687ecc2",
   "metadata": {},
   "source": [
    "#### 5. Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "eval_results_source = trainer_source.evaluate()\n",
    "print(\"\\n=== Validation Results Source ===\")\n",
    "for k, v in eval_results_source.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "preds_output_source = trainer_source.predict(test_subset_source)\n",
    "preds_source = np.argmax(preds_output_source.predictions, axis=1)\n",
    "labels_source = preds_output_source.label_ids\n",
    "print(\"\\n=== Test Classification Report Source ===\")\n",
    "print(classification_report(labels_source, preds_source, digits=4))\n",
    "\n",
    "# Topic\n",
    "eval_results_topic = trainer_topic.evaluate()\n",
    "print(\"\\n=== Validation Results Topic ===\")\n",
    "for k, v in eval_results_topic.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "preds_output_topic = trainer_topic.predict(test_subset_topic)\n",
    "preds_topic = np.argmax(preds_output_topic.predictions, axis=1)\n",
    "labels_topic = preds_output_topic.label_ids\n",
    "print(\"\\n=== Test Classification Report Topic ===\")\n",
    "print(classification_report(labels_topic, preds_topic, digits=4))\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# GUARDAR MODELOS\n",
    "# ===========================\n",
    "\n",
    "trainer_source.save_model(\"models/distilbert_source\")\n",
    "trainer_topic.save_model(\"models/distilbert_topic\")\n",
    "\n",
    "tokenizer.save_pretrained(\"models/distilbert_source\")  # el mismo tokenizador sirve para ambos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33c21b",
   "metadata": {},
   "source": [
    "#### 6. Comparaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87114893",
   "metadata": {},
   "source": [
    "## **2. Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761d3aa",
   "metadata": {},
   "source": [
    "Adem√°s de la tarea principal de clasificaci√≥n de sesgo, el dataset de noticias utilizado permite abordar de forma natural otras tareas relevantes de Procesamiento de Lenguaje Natural. Una de las m√°s adecuadas en este contexto es el text summarization, ya que las noticias suelen ser documentos relativamente largos que contienen informaci√≥n redundante o secundaria. La capacidad de generar res√∫menes concisos y coherentes resulta especialmente √∫til en escenarios reales, como la lectura r√°pida de informaci√≥n, la indexaci√≥n de contenidos o la visualizaci√≥n eficiente de grandes vol√∫menes de noticias.\n",
    "\n",
    "El text summarization consiste en generar una versi√≥n m√°s corta de un texto original conservando sus ideas principales y su significado global. En este proyecto se aborda como una tarea abstractive summarization, en la que el modelo no se limita a extraer frases del texto original, sino que es capaz de reformular el contenido y generar nuevas frases que sinteticen la informaci√≥n m√°s relevante. Este enfoque es especialmente adecuado para textos period√≠sticos, donde la informaci√≥n clave puede estar distribuida a lo largo del documento y no concentrada en frases concretas.\n",
    "\n",
    "Para llevar a cabo esta tarea se ha seleccionado un modelo basado en Transformers disponible en Hugging Face, concretamente BART (Bidirectional and Auto-Regressive Transformers). BART es una arquitectura encoder‚Äìdecoder dise√±ada espec√≠ficamente para tareas de generaci√≥n de texto, como el resumen, la traducci√≥n o la generaci√≥n de respuestas. En particular, se utiliza una versi√≥n de BART preentrenada para el resumen de noticias, entrenada previamente sobre grandes corpus period√≠sticos, lo que la hace especialmente adecuada para el tipo de textos presentes en nuestro dataset.\n",
    "\n",
    "El uso de un modelo preentrenado permite aplicar el resumen sin necesidad de realizar fine-tuning adicional, lo cual es una ventaja importante dadas las limitaciones de recursos computacionales. De este modo, el modelo puede aplicarse directamente sobre un subconjunto de noticias para generar res√∫menes y analizar cualitativamente sus resultados. Esta estrategia cumple con el objetivo de la entrega, que es familiarizarse con otras tareas y arquitecturas de NLP, sin requerir un entrenamiento costoso o evaluaciones complejas.\n",
    "\n",
    "Los objetivos principales de esta parte del trabajo son, en primer lugar, explorar una tarea de NLP diferente a la clasificaci√≥n, ampliando el alcance del proyecto m√°s all√° de la predicci√≥n de etiquetas. En segundo lugar, se busca comprender c√≥mo los modelos generativos basados en Transformers procesan y condensan informaci√≥n textual, y qu√© tipo de salidas producen cuando se aplican a noticias reales. Finalmente, esta tarea permite reflexionar sobre el potencial uso pr√°ctico de el text summarization como complemento a la clasificaci√≥n de sesgo, por ejemplo, para ofrecer res√∫menes r√°pidos de noticias ya categorizadas.\n",
    "\n",
    "No obstante, tambi√©n se esperan varias limitaciones e inconvenientes. En primer lugar, al trabajar sin fine-tuning espec√≠fico sobre nuestro dataset, los res√∫menes generados pueden no capturar siempre los matices m√°s relevantes del texto original o pueden omitir detalles importantes relacionados con el sesgo. Adem√°s, los modelos de resumizaci√≥n suelen tener restricciones en la longitud m√°xima de entrada, lo que obliga a truncar textos largos y puede provocar p√©rdida de informaci√≥n. Por √∫ltimo, el uso de estos modelos en CPU implica tiempos de ejecuci√≥n relativamente altos, lo que limita la cantidad de ejemplos que pueden procesarse durante los experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe4d47",
   "metadata": {},
   "source": [
    "### 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944d18f",
   "metadata": {},
   "source": [
    "En esta etapa del proyecto se centra en obtener y preparar la informaci√≥n que se utilizar√° para generar los res√∫menes autom√°ticos. Primero, se accede al conjunto de noticias previamente limpiado y procesado, que contiene toda la informaci√≥n textual organizada en columnas. DEspues se selecciona √∫nicamente la columna que contiene el contenido principal de cada noticia, descartando cualquier informaci√≥n que no sea relevante para la tarea de resumen.\n",
    "\n",
    "Se eliminan los valores vac√≠os o nulos para asegurar que todos los textos que se procesen tengan contenido real y significativo. Para facilitar las pruebas y la visualizaci√≥n de resultados, se trabaja inicialmente con un subconjunto representativo de los textos, evitando as√≠ procesar el conjunto completo en las etapas iniciales. Adem√°s, se descartan los textos demasiado cortos, que no aportar√≠an suficiente informaci√≥n para generar res√∫menes coherentes.\n",
    "\n",
    "El objetivo de esta fase es garantizar que el modelo reciba √∫nicamente informaci√≥n √∫til y consistente, evitando errores o resultados irrelevantes. Se espera que, al final de este bloque, se tenga una lista de noticias limpias y consistentes, cada una con suficiente contenido para que el modelo pueda producir res√∫menes coherentes y significativos, y que facilite la comparaci√≥n posterior entre los textos originales y los res√∫menes generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b43ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import textwrap\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358fd682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero total de noticias: 27978\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv(\"data/data_clean/train_clean.csv\")\n",
    "\n",
    "# Nos quedamos solo con la columna de texto\n",
    "texts = df[\"content_clean\"].dropna().tolist()\n",
    "\n",
    "print(f\"N√∫mero total de noticias: {len(texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_subset = texts[:20]  \n",
    "\n",
    "# para ignorar textos cortos\n",
    "texts_subset = [t for t in texts_subset if len(t.strip()) > 20] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c1d75",
   "metadata": {},
   "source": [
    "### 2. Selecci√≥n de Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c78ff9",
   "metadata": {},
   "source": [
    "Para esta tarea se ha escogido BART (Bidirectional and Auto-Regressive Transformers) en su versi√≥n ‚Äúlarge‚Äù entrenada para resumir textos, conocido como facebook/bart-large-cnn. Este modelo combina una arquitectura de encoder-decoder con un entrenamiento espec√≠fico en tareas de resumen de textos largos, lo que le permite identificar la informaci√≥n m√°s importante y generar res√∫menes coherentes y comprensibles. BART es especialmente potente para textos de noticias, ya que puede manejar secuencias extensas y capturar las relaciones contextuales entre diferentes partes del texto, asegurando que los res√∫menes mantengan la esencia de la informaci√≥n original.\n",
    "\n",
    "La elecci√≥n del modelo tambi√©n se ha hecho considerando la capacidad computacional disponible. Aunque BART large es un modelo pesado y exigente en recursos, la configuraci√≥n se ajustaa la CPU cuando la GPU no est√© disponible. Esto asegura que los res√∫menes se generen de manera eficiente sin comprometer la calidad, adapt√°ndose a las limitaciones del entorno de trabajo.\n",
    "\n",
    "El objetivo de este bloque es garantizar que el modelo seleccionado pueda resumir las noticias de manera precisa y eficiente, extrayendo la informaci√≥n m√°s relevante y produciendo resultados que sean f√°cilmente comparables con los textos originales. Se espera que, gracias a la elecci√≥n de BART large, los res√∫menes sean coherentes, informativos y representativos del contenido de cada noticia, permitiendo evaluar de manera confiable el rendimiento del sistema de resumen autom√°tico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d6f1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Definimos el nombre del modelo de resumen que vamos a usar\n",
    "model_name = \"facebook/bart-large-cnn\"  # BART large entrenado para res√∫menes de noticias\n",
    "\n",
    "# Creamos el pipeline \n",
    "summarizer = pipeline(\n",
    "    \"summarization\",        \n",
    "    model=model_name,       \n",
    "    tokenizer=model_name,      \n",
    "    device=0 if torch.cuda.is_available() else -1 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25736ec",
   "metadata": {},
   "source": [
    "### 3. Generaci√≥n de Resumenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6414e0b",
   "metadata": {},
   "source": [
    "Este bloque constituye el n√∫cleo de la tarea de resumen, ya que es el encargado de procesar cada noticia y producir un resumen coherente y completo. La estrategia central consiste en tomar un texto, analizarlo en profundidad y extraer su informaci√≥n m√°s relevante, asegurando que el resultado sea comprensible y representativo de todo el contenido original.\n",
    "\n",
    "Dado que los textos pueden ser muy largos, se utiliza un enfoque de procesamiento por fragmentos. Cada noticia se divide en segmentos de tama√±o limitado, adaptados a la capacidad del modelo, para evitar que se supere el l√≠mite de entrada que los transformadores pueden procesar de una sola vez. Esto permite que incluso los textos extensos puedan ser resumidos sin perder informaci√≥n cr√≠tica. Cada fragmento se analiza de manera independiente y se genera un resumen parcial para ese segmento.\n",
    "\n",
    "Para mejorar la adaptabilidad del modelo, se ajustan los par√°metros de longitud m√°xima del resumen en funci√≥n del tama√±o del fragmento, de manera que los fragmentos muy cortos no generen res√∫menes desproporcionados o incompletos. Esto asegura que los res√∫menes parciales sean coherentes y mantengan un equilibrio adecuado entre concisi√≥n y exhaustividad.\n",
    "\n",
    "Una vez que se han generado los res√∫menes de todos los fragmentos, estos se unen para formar el resumen final de la noticia completa. De esta forma, se preserva la informaci√≥n esencial de cada parte del texto original, garantizando que el resultado global refleje de manera fiel el contenido total.\n",
    "\n",
    "Durante todo el proceso se contempla la posibilidad de errores o excepciones, como fragmentos que puedan exceder la capacidad del modelo o problemas imprevistos en la generaci√≥n. En esos casos, se maneja el error de manera controlada, registrando el fragmento problem√°tico y continuando con los dem√°s, lo que asegura la robustez del sistema frente a textos complejos o con irregularidades.\n",
    "\n",
    "Finalmente, este bloque se aplica iterativamente a cada texto del conjunto seleccionado, generando un resumen individual para cada noticia. El resultado esperado es que cada resumen sea claro, coherente y representativo, facilitando la comparaci√≥n con el texto original y cumpliendo con el objetivo principal del proyecto: automatizar la s√≠ntesis de informaci√≥n relevante de manera eficiente y confiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume un texto dividi√©ndolo en fragmentos si es demasiado largo para el modelo.\n",
    "\n",
    "def summarize_text(text, max_length=130, min_length=40, chunk_size=1024):\n",
    "\n",
    "    summaries = []\n",
    "    \n",
    "    # dividir el texto en fragmentos\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        #se ajusta por si es muy corto el texto original\n",
    "        adjusted_max = min(max_length, len(chunk)//2)\n",
    "        try:\n",
    "            summary = summarizer(\n",
    "                chunk,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            # por si falla\n",
    "            summaries.append(\"\")\n",
    "            print(f\"Error en el fragmento {i//chunk_size + 1}: {e}\")\n",
    "    \n",
    "    # unir los res√∫menes parciales\n",
    "    return \" \".join(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac211e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 79. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 1 generado, longitud texto: 2471\n",
      "Resumen 2 generado, longitud texto: 4081\n",
      "Resumen 3 generado, longitud texto: 4807\n",
      "Resumen 4 generado, longitud texto: 9124\n",
      "Resumen 5 generado, longitud texto: 4000\n",
      "Resumen 6 generado, longitud texto: 3933\n",
      "Resumen 7 generado, longitud texto: 8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 79. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 8 generado, longitud texto: 5473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 40. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 9 generado, longitud texto: 5279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 10 generado, longitud texto: 2212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 11 generado, longitud texto: 6453\n",
      "Resumen 12 generado, longitud texto: 6820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 13 generado, longitud texto: 4306\n",
      "Resumen 14 generado, longitud texto: 1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 15 generado, longitud texto: 3264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 16 generado, longitud texto: 6290\n",
      "Resumen 17 generado, longitud texto: 3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 18 generado, longitud texto: 7260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 19 generado, longitud texto: 8276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen 20 generado, longitud texto: 10292\n"
     ]
    }
   ],
   "source": [
    "summaries = []  # Lista donde almacenaremos los res√∫menes generados\n",
    "\n",
    "# Iteramos sobre cada texto del subconjunto de noticias\n",
    "for i, text in enumerate(texts_subset):\n",
    "    try:\n",
    "        # Generamos el resumen del texto usando la funci√≥n principal\n",
    "        summary = summarize_text(text)\n",
    "        summaries.append(summary)\n",
    "        # mensaje informativo \n",
    "        print(f\"Resumen {i+1} generado, longitud texto: {len(text)}\")\n",
    "    except Exception as e:\n",
    "        # En caso de que falle la generaci√≥n del resumen, a√±adimos un string vac√≠o\n",
    "        summaries.append(\"\")\n",
    "        # mensaje de error \n",
    "        print(f\"Error en el texto {i+1} (longitud {len(text)}): {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983851f6",
   "metadata": {},
   "source": [
    "### 4. Visualizaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbff859",
   "metadata": {},
   "source": [
    "En este proyecto, hemos conseguido extraer autom√°ticamente la informaci√≥n m√°s relevante de un conjunto de noticias, generando res√∫menes coherentes y comprensibles a partir de textos originales extensos. Para ello hemos seleccionado  el modelo BART large, entrenado espec√≠ficamente para tareas de resumen. Esta elecci√≥n nos ha permitido equilibrar la calidad de los res√∫menes con la capacidad computacional disponible, adapt√°ndonos a la CPU . De esta manera, aseguramos que nuestro sistema pueda funcionar de manera eficiente incluso con limitaciones de hardware, lo cual es fundamental para aplicaciones pr√°cticas.\n",
    "\n",
    "Uno de los logros m√°s importantes ha sido implementar un m√©todo que divide los textos largos en fragmentos manejables para el modelo. Gracias a esta estrategia hemos podido superar la limitaci√≥n de tama√±o de entrada de los transformadores y generar res√∫menes parciales que, al unirse, mantienen la coherencia y la informaci√≥n esencial de cada noticia. Este enfoque nos ha permitido procesar textos extensos sin perder contenido cr√≠tico, aunque observamos que algunos res√∫menes parciales pueden repetir informaci√≥n o generar transiciones no muy buenas entre fragmentos.\n",
    "\n",
    "Durante el desarrollo del proyecto tambi√©n identificamos algunas dificultades y limitaciones. Los textos muy cortos o con estructuras poco convencionales a veces produjeron res√∫menes incompletos o vac√≠os, lo que nos ense√±√≥ la importancia de filtrar adecuadamente los datos antes de procesarlos. Adem√°s, aunque BART large genera res√∫menes detallados, en ciertos casos los resultados fueron m√°s extensos de lo esperado, ya que el modelo intenta conservar muchos detalles de la noticia. Esto nos indica que en futuras implementaciones podr√≠a ser √∫til ajustar con mayor precisi√≥n los par√°metros de longitud m√°xima y m√≠nima o tambien considerar modelos alternativos seg√∫n la extensi√≥n y el tipo de noticia.\n",
    "\n",
    "Otro aspecto clave ha sido la visualizaci√≥n y presentaci√≥n de los resultados. Al mostrar los textos originales y sus res√∫menes en p√°rrafos legibles, con un ancho de l√≠nea limitado, hemos podido comparar de manera r√°pida y clara la fidelidad de los res√∫menes. Este enfoque nos permiti√≥ identificar f√°cilmente que res√∫menes capturan correctamente la informaci√≥n principal y cuales requieren ajustes, proporcionando un feedback inmediato sobre el desempe√±o del modelo.\n",
    "\n",
    "Con estos resultados dedujimos varias cosas. Primero, es posible automatizar de manera eficiente la s√≠ntesis de noticias largas, reduciendo significativamente el tiempo que dedicar√≠amos a revisar grandes vol√∫menes de informaci√≥n. Segundo, la calidad de los res√∫menes depende tanto del modelo elegido como de la preparaci√≥n y filtrado de los datos, lo que demuestra la importancia de contar con un dataset limpio y bien seleccionado. Tercero, aunque la automatizaci√≥n es efectiva, los modelos actuales a√∫n pueden generar repeticiones o incluir detalles irrelevantes, por lo que consideramos recomendable realizar revisiones humanas cuando se busque alta precisi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Generated_Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>besides his most recent trip to quetta mr raha...</td>\n",
       "      <td>mr rahami visited karachi pakistan in 2005 an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poll prestigious colleges wo nt make you happi...</td>\n",
       "      <td>Gallup poll finds prestigious colleges don't m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>house speaker paul ryan at a private dinner ea...</td>\n",
       "      <td>House speaker paul ryan at a private dinner ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn president donald trump has reason to hope ...</td>\n",
       "      <td>Donald trump has reason to hope his luck is ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the controversial immigrationreform bill that ...</td>\n",
       "      <td>The controversial immigrationreform bill that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Original_Text  \\\n",
       "0  besides his most recent trip to quetta mr raha...   \n",
       "1  poll prestigious colleges wo nt make you happi...   \n",
       "2  house speaker paul ryan at a private dinner ea...   \n",
       "3  cnn president donald trump has reason to hope ...   \n",
       "4  the controversial immigrationreform bill that ...   \n",
       "\n",
       "                                   Generated_Summary  \n",
       "0   mr rahami visited karachi pakistan in 2005 an...  \n",
       "1  Gallup poll finds prestigious colleges don't m...  \n",
       "2  House speaker paul ryan at a private dinner ea...  \n",
       "3  Donald trump has reason to hope his luck is ch...  \n",
       "4  The controversial immigrationreform bill that ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creamos un DataFrame que combina los textos originales con los res√∫menes generados\n",
    "df_summaries = pd.DataFrame({\n",
    "    \"Original_Text\": texts_subset,\n",
    "    \"Generated_Summary\": summaries\n",
    "})\n",
    "\n",
    "# Mostramos las primeras filas\n",
    "df_summaries.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57761288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos resumenes\n",
    "df_summaries.to_csv(\"news_summaries_bart.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae492b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NOTICIA ORIGINAL ---\n",
      "\n",
      "besides his most recent trip to quetta mr rahami visited karachi pakistan in 2005 both of those\n",
      "cities reputations have become entwined with the militant groups who have sheltered there karachi as\n",
      "a haven for the pakistani taliban and al qaeda and quetta as the headquarters of the exiled afghan\n",
      "taliban leadership but both cities are also home to generations of afghans who have fled violence in\n",
      "their home country much about his new jersey life did seem unremarkable amarjit singh a limousine\n",
      "drive...\n",
      "\n",
      "--- RESUMEN GENERADO ---\n",
      "\n",
      " mr rahami visited karachi pakistan in 2005 and quetta in 2005 both of those cities reputations have\n",
      "become entwined with the militant groups who have sheltered there. Both cities are also home to\n",
      "generations of afghans who have fled violence in their home country much about his new jersey life\n",
      "did seem unremarkable. Mr rahami had a daughter with a high school girlfriend according to friends\n",
      "reached at her home on monday night she declined to comment my heart is just broken said the woman\n",
      "who the new york times is not identifying i don t even know what to think after high school mr singh\n",
      "said there seemed to be a lot of tension between the two. A grand jury however declined to indict mr\n",
      "rahami. He spent over three months in jail on the charges according to a highranking law enforcement\n",
      "official. He also spent a day in jail in february 2012 for allegedly violating a restraining order.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "--- NOTICIA ORIGINAL ---\n",
      "\n",
      "poll prestigious colleges wo nt make you happier in life or work there s plenty of anxiety in the us\n",
      "over getting into a top college but a new gallup poll suggests that later in life it does nt matter\n",
      "nearly as much as we think in fact when you ask college graduates whether they re engaged with their\n",
      "work or thriving in all aspects of their lives their responses do nt vary one bit whether they went\n",
      "to a prestigious college or not the surprising findings come in a survey of 29650 college\n",
      "graduate...\n",
      "\n",
      "--- RESUMEN GENERADO ---\n",
      "\n",
      "Gallup poll finds prestigious colleges don't make you happier in life or work. 39 percent of college\n",
      "grads overall say they re engaged at work which is 10 points higher than the population at large.\n",
      "Almost 5 in 6 selfreport doing great in at least one sphere whether sense of purpose financial\n",
      "security ph Only 11 percent are thriving in all five areas of wellbeing. Percentage did not vary\n",
      "based on whether the grads went to a fancy namebrand school or a regional state college. A slight\n",
      "edge did go to those who attended campuses with more than 10000 students. Survey finds no broad\n",
      "influence whatsoever whether a person s diploma cost 25000 or 250000. Factors that should be guiding\n",
      "college decisions are not selectivity or prestige but cost of attendance great teaching and deep\n",
      "learning. Only 2 percent of those with 20000 to 40000 in undergraduate loans reported they were\n",
      "thriving. 7 in 10 students who borrow gallup and purdue hope to use these surveys to help colleges\n",
      "better focus on outcomes.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "--- NOTICIA ORIGINAL ---\n",
      "\n",
      "house speaker paul ryan at a private dinner earlier this year said he thought only one member wanted\n",
      "to build a wall across the entire usmexico border has learned from multiple sources with direct\n",
      "knowledge of the comments including former rep tom tancredo rco the dinner sources said took place\n",
      "on the eve of the house s passage of two relatively minor immigration bills at the end of june kate\n",
      "s law and sanctuary city reforms the far wider reaching davisoliver act was tabled at the same time\n",
      "ryan...\n",
      "\n",
      "--- RESUMEN GENERADO ---\n",
      "\n",
      "House speaker paul ryan at a private dinner earlier this year said he thought only one member wanted\n",
      "to build a wall across the entire usmexico border. The dinner sources said took place on the eve of\n",
      "the house s passage of two relatively minor immigration bills. The chamber of commerce doesn't want\n",
      "a wall. The pressure is greater from the chamber of Commerce than it is from the members tancredo.\n",
      "The speaker was attempting to summarize the disparate voices in the house gop caucus. The southern\n",
      "border wall is president donald trump s signature campaign promise. The ability of the\n",
      "republicancontrolled congress to deliver the authorization and funding will likely define the\n",
      "success or failure of the administration s immigration agenda. No funding has been forthcoming with\n",
      "any mention of the wall dropped from the spring s gop budget proposals the white house itself was\n",
      "unclear as to when funding might be expected. President trump cut a temporary deal with the\n",
      "democrats to avert a government shutdown and raise the debt.  tancredo was pessimistic the\n",
      "congressional republican leadership can be easily brought on board with the immigration agenda of\n",
      "the president and the party s conservative wing. He was caught on tape after the emergence of the\n",
      "nowinfamous access hollywood recording of the President saying he was not going to defend donald\n",
      "trump.\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "width = 100\n",
    "\n",
    "#MOstramos ejemplos de lso resumenes hechos\n",
    "for i in range(3):\n",
    "    original_text = df_summaries.loc[i, \"Original_Text\"][:500] + \"...\"\n",
    "    generated_summary = df_summaries.loc[i, \"Generated_Summary\"]\n",
    "\n",
    "    print(\"\\n--- NOTICIA ORIGINAL ---\\n\")\n",
    "    print(\"\\n\".join(textwrap.wrap(original_text, width=width)))\n",
    "    \n",
    "    print(\"\\n--- RESUMEN GENERADO ---\\n\")\n",
    "    print(\"\\n\".join(textwrap.wrap(generated_summary, width=width)))\n",
    "    print(\"\\n\" + \"=\"*width + \"\\n\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
