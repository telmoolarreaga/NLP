{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfT1kYc_EOn"
      },
      "source": [
        "# **1. Shallow Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Librerías estándar\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sklearn: Preprocesamiento, modelos y métricas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Gensim para embeddings no contextuales\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# Sentence Transformers para embeddings contextuales\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# TensorFlow / Keras para Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para representar los textos, hemos elegido TF-IDF para capturar la importancia relativa de cada palabra en un documento frente al corpus completo, reduciendo el peso de palabras muy frecuentes que no aportan información discriminativa, como artículos y preposiciones en inglés. Hemos establecido un  límite de 3,000 palabras más importantes para reducir la dimensionalidad. Ademas hemos añadido y unigramas y bigramas para capturar algo de contexto local sin sobrecargar el modelo. Por otro lado, hemos eliminado las palabras vacías en inglés para centrar el análisis en palabras significativas. Esta representación genera vectores dispersos que son ideales para los modelos clásicos que usamos.\n",
        "\n",
        "Hemos seleccionado cuatro modelos para evaluar el desempeño: Logistic Regression, LinearSVC, Random Forest y XGBoost. Esta combinación nos permite cubrir tanto modelos lineales como no lineales y comparar rapidez, precisión y estabilidad.\n",
        "\n",
        "Antes de entrenar, hemos convertido los textos a números mediante label encoding, y  una división de train/validation del 80/20 para medir el rendimiento real y evitar overfitting. Además, hemos filtrado las clases con menos de dos registros, ya que la validación estratificada requiere al menos dos ejemplos por clase. La funcion recibe como parametro la variable objetivo. En este caso, recibe las variables \"bias\", \"topic\" y \"source\", que son nuestras variables a clasificar.\n",
        "\n",
        "Finalmente, todos los modelos y el vectorizador TF-IDF han sido guardados para su reutilización. Este pipeline de Shallow Learning funciona como un baseline sólido que nos permite medir la mejora que aportan las representaciones densas y contextuales de texto que se utilizarán en las fases posteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shallow_pipeline(df, target_col):\n",
        "    # Preparamos el texto\n",
        "    if \"text_joined\" not in df.columns:\n",
        "        df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "    texts = df[\"text_joined\"].astype(str).tolist()\n",
        "    labels = df[target_col].tolist()\n",
        "\n",
        "    # Filtramos las clases con menos de 2 registros\n",
        "    counts = Counter(labels)\n",
        "    valid_classes = [c for c, cnt in counts.items() if cnt > 1]\n",
        "    mask = [lbl in valid_classes for lbl in labels]\n",
        "    texts = [t for t, m in zip(texts, mask) if m]\n",
        "    labels = [l for l, m in zip(labels, mask) if m]\n",
        "\n",
        "    # Codificamos las etiquetas\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "\n",
        "    # Hacemos el train/validation split\n",
        "    X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "        texts, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Aplicamos TF-IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))\n",
        "    X_train = vectorizer.fit_transform(X_train_text)\n",
        "    X_val = vectorizer.transform(X_val_text)\n",
        "\n",
        "    # Definimos los modelos\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
        "        \"LinearSVC\": LinearSVC(),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=150, n_jobs=-1),\n",
        "       # \"XGBoost\": XGBClassifier(n_estimators=75, eval_metric=\"mlogloss\", tree_method=\"hist\", n_jobs=-1)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Entrenamos, evaluamos y guardamos los modelos\n",
        "    os.makedirs(\"data/models\", exist_ok=True)\n",
        "    for name, model in models.items():\n",
        "        print(f\"Entrenando {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy_score(y_val, y_pred),\n",
        "            \"Macro-F1\": f1_score(y_val, y_pred, average=\"macro\")\n",
        "        }\n",
        "        # Guardamos el modelo\n",
        "        pickle.dump(model, open(f\"data/models/{name.replace(' ', '_').lower()}.pkl\", \"wb\"))\n",
        "\n",
        "    # Guardamos el vectorizador\n",
        "    os.makedirs(\"data/features\", exist_ok=True)\n",
        "    pickle.dump(vectorizer, open(\"data/features/tfidf_vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados se analizarán en la sección 4 de este noteebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n",
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "Entrenando XGBoost...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.702109  0.700091\n",
            "LinearSVC            0.698713  0.696551\n",
            "Random Forest        0.689778  0.684783\n",
            "XGBoost              0.734632  0.733918\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con bias\n",
        "results_bias = shallow_pipeline(df, \"bias\")\n",
        "print(pd.DataFrame(results_bias).T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Llamamos a la función con topic\n",
        "results_topic = shallow_pipeline(df, \"topic\")\n",
        "print(pd.DataFrame(results_topic).T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n",
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "Entrenando XGBoost...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.503401  0.103117\n",
            "LinearSVC            0.559076  0.219572\n",
            "Random Forest        0.500000  0.120979\n",
            "XGBoost              0.532581  0.223251\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con source\n",
        "results_source = shallow_pipeline(df, \"source\")\n",
        "print(pd.DataFrame(results_source).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztbnwVJVWSgX"
      },
      "source": [
        "# **2. Modelos Deep**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la parte de Deep Learning, hemos optado por utilizar redes neuronales recurrentes, específicamente LSTM y GRU, debido a su capacidad para capturar dependencias secuenciales en el texto. A diferencia de los modelos de Shallow Learning, que tratan cada palabra o n-grama de manera independiente, las RNNs permiten que la red recuerde información contextual de palabras anteriores en la secuencia, lo cual es crucial para nuestras tareas de clasificación de texto donde el significado puede depender del orden de las palabras.\n",
        "\n",
        "Para la representación de los textos, hemos empleado embeddings densos, utilizando tres enfoques distintos con Word2Vec: congelado, fine-tuneado y desde cero. En el caso de los embeddings congelados, utilizamos un modelo preentrenado de Word2Vec y lo fijamos durante el entrenamiento de la red, de manera que solo la LSTM o GRU aprenda a combinar los vectores preexistentes. Esto permite evaluar cuánto conocimiento semántico ya capturado en Word2Vec puede ayudar a la tarea sin modificarlo. En el enfoque de fine-tune, los embeddings inicializados con Word2Vec se ajustan durante el entrenamiento, permitiendo que la red adapte los vectores a las particularidades del dataset específico. Finalmente, la opción de embeddings entrenados desde cero crea vectores aleatorios que se aprenden completamente durante el entrenamiento, lo que permite que la red descubra representaciones óptimas para la tarea, aunque requiere más datos y tiempo de entrenamiento.\n",
        "\n",
        "Hemos elegido LSTM y GRU ya que cumple nuestra necesidad de comparar dos variantes de redes recurrentes: las LSTM tienen una mayor capacidad para capturar dependencias de largo plazo mediante su mecanismo de puertas, mientras que las GRU son más simples y computacionalmente eficientes, lo que puede acelerar el entrenamiento sin perder demasiado rendimiento. \n",
        "\n",
        "Los textos se transforman primero en secuencias de índices según el vocabulario de Word2Vec o un tokenizer entrenado sobre el dataset, y se aplica padding para unificar la longitud de las secuencias. Esto asegura que las redes puedan procesar lotes de datos de manera eficiente. Finalmente, la capa de salida utiliza softmax para producir probabilidades sobre las clases, y la red se entrena con categorical crossentropy, optimizando la accuracy y el macro-F1 como métricas de desempeño, lo cual es consistente con la evaluación utilizada en la parte de Shallow Learning.\n",
        "\n",
        "En conclusion, en este apartado permite que nuestra red aprenda tanto representaciones densas de palabras como patrones secuenciales de las oraciones, ofreciendo una ventaja sobre los modelos lineales y de ensamble de Shallow Learning que solo utilizan información superficial y dispersa de los textos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings finetuneados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 322ms/step - accuracy: 0.4257 - loss: 1.0562 - val_accuracy: 0.4750 - val_loss: 1.0236\n",
            "Epoch 2/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 311ms/step - accuracy: 0.5227 - loss: 0.9537 - val_accuracy: 0.4959 - val_loss: 0.9828\n",
            "Epoch 3/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 340ms/step - accuracy: 0.5904 - loss: 0.8587 - val_accuracy: 0.5234 - val_loss: 0.9534\n",
            "Epoch 4/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 428ms/step - accuracy: 0.6674 - loss: 0.7404 - val_accuracy: 0.5173 - val_loss: 1.0088\n",
            "Epoch 5/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 373ms/step - accuracy: 0.7374 - loss: 0.6141 - val_accuracy: 0.5116 - val_loss: 1.0896\n",
            "Epoch 6/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 521ms/step - accuracy: 0.8024 - loss: 0.4791 - val_accuracy: 0.5046 - val_loss: 1.2833\n",
            "Epoch 7/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 1s/step - accuracy: 0.8611 - loss: 0.3566 - val_accuracy: 0.5095 - val_loss: 1.3578\n",
            "Epoch 8/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 1s/step - accuracy: 0.8989 - loss: 0.2583 - val_accuracy: 0.5114 - val_loss: 1.6459\n",
            "Epoch 9/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 533ms/step - accuracy: 0.9331 - loss: 0.1715 - val_accuracy: 0.5161 - val_loss: 1.8227\n",
            "Epoch 10/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 380ms/step - accuracy: 0.9541 - loss: 0.1253 - val_accuracy: 0.5134 - val_loss: 2.1535\n",
            "Epoch 1/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 290ms/step - accuracy: 0.4016 - loss: 1.0760 - val_accuracy: 0.4307 - val_loss: 1.0498\n",
            "Epoch 2/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 367ms/step - accuracy: 0.4854 - loss: 1.0039 - val_accuracy: 0.4805 - val_loss: 0.9938\n",
            "Epoch 3/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 331ms/step - accuracy: 0.5812 - loss: 0.8742 - val_accuracy: 0.5088 - val_loss: 0.9586\n",
            "Epoch 4/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 320ms/step - accuracy: 0.6687 - loss: 0.7393 - val_accuracy: 0.5079 - val_loss: 0.9995\n",
            "Epoch 5/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 318ms/step - accuracy: 0.7584 - loss: 0.5768 - val_accuracy: 0.5250 - val_loss: 1.0911\n",
            "Epoch 6/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 323ms/step - accuracy: 0.8357 - loss: 0.4116 - val_accuracy: 0.5366 - val_loss: 1.2710\n",
            "Epoch 7/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 474ms/step - accuracy: 0.8903 - loss: 0.2854 - val_accuracy: 0.5359 - val_loss: 1.4438\n",
            "Epoch 8/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 618ms/step - accuracy: 0.9309 - loss: 0.1858 - val_accuracy: 0.5229 - val_loss: 1.7583\n",
            "Epoch 9/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 414ms/step - accuracy: 0.9538 - loss: 0.1276 - val_accuracy: 0.5304 - val_loss: 1.9515\n",
            "Epoch 10/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 350ms/step - accuracy: 0.9701 - loss: 0.0880 - val_accuracy: 0.5264 - val_loss: 2.2296\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step\n",
            "Resultados:\n",
            "      Accuracy  Macro-F1\n",
            "LSTM  0.513402  0.514574\n",
            "GRU   0.526447  0.526548\n"
          ]
        }
      ],
      "source": [
        "# Cargamos el dataset tokenizado\n",
        "df_train = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "# Codificamos los labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_cat = to_categorical(y_encoded)\n",
        "\n",
        "# Split train/validation\n",
        "X_tr_text, X_val_text, y_tr, y_val = train_test_split(\n",
        "    df_train[\"tokens\"], y_cat, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Cargamos el Word2Vec preentrenado de la anterior entrega\n",
        "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
        "embedding_dim = w2v_model.vector_size\n",
        "\n",
        "# Creamos el vocabulario e índices\n",
        "word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1  # +1 para padding\n",
        "\n",
        "# Convertimos los tokens a índices\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_tr_idx = [tokens_to_indices(t, word_index) for t in X_tr_text]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
        "\n",
        "# Aplicamos padding\n",
        "max_seq_len = 200\n",
        "X_tr_pad = pad_sequences(X_tr_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "# Creamos la matriz de embedding \n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Definimos y entrenamos los modelos \n",
        "\n",
        "def build_rnn(model_type='LSTM'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_seq_len,\n",
        "                        trainable=True))  # Fine-tune embeddings\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# LSTM\n",
        "lstm_model = build_rnn('LSTM')\n",
        "lstm_history = lstm_model.fit(X_tr_pad, y_tr,\n",
        "                              validation_data=(X_val_pad, y_val),\n",
        "                              epochs=10,\n",
        "                              batch_size=64)\n",
        "\n",
        "# GRU\n",
        "gru_model = build_rnn('GRU')\n",
        "gru_history = gru_model.fit(X_tr_pad, y_tr,\n",
        "                            validation_data=(X_val_pad, y_val),\n",
        "                            epochs=10,\n",
        "                            batch_size=64)\n",
        "\n",
        "# Evaluamos los modelos\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lstm = lstm_model.predict(X_val_pad, batch_size=64)\n",
        "y_pred_gru = gru_model.predict(X_val_pad, batch_size=64)\n",
        "\n",
        "y_pred_lstm_labels = np.argmax(y_pred_lstm, axis=1)\n",
        "y_pred_gru_labels = np.argmax(y_pred_gru, axis=1)\n",
        "y_val_labels = np.argmax(y_val, axis=1)\n",
        "\n",
        "# Métricas\n",
        "results = {\n",
        "    'LSTM': {\n",
        "        'Accuracy': accuracy_score(y_val_labels, y_pred_lstm_labels),\n",
        "        'Macro-F1': f1_score(y_val_labels, y_pred_lstm_labels, average='macro')\n",
        "    },\n",
        "    'GRU': {\n",
        "        'Accuracy': accuracy_score(y_val_labels, y_pred_gru_labels),\n",
        "        'Macro-F1': f1_score(y_val_labels, y_pred_gru_labels, average='macro')\n",
        "    }\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"Resultados:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-Rendimiento general:\n",
        "   -Ambos modelos muestran resultados muy similares, con valores en torno al 53–54%, tanto en Accuracy como en Macro-F1. Esto indica que:\n",
        "       -Los dos modelos capturan de forma parecida los patrones secuenciales del sesgo ideológico.\n",
        "       -No existe una ventaja clara de ninguno de los dos modelos neuronales en este dataset.\n",
        "-Interpretación:\n",
        "    -El rendimiento indica que el sesgo ideológico es una tarea difícil incluso para modelos neuronales. \n",
        "    -Puede que los textos no tengan suficiente señal secuencial para que LSTM/GRU destaquen claramente.\n",
        "-Conclusión:\n",
        "    -Ambos modelos presentan un rendimiento equivalente, pero al ser  ligeramente superior, hemos decidido usar LSTM  como baseline de deep learning. Sin embargo, estas arquitecturas probablemente no capturan matices ideológicos complejos, por lo que se es necesario explorar  modelos más potentes como BERT o RoBERTa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings no finetuneados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 90ms/step - accuracy: 0.3635 - loss: 1.0929 - val_accuracy: 0.3672 - val_loss: 1.0934\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.3830 - loss: 1.0869 - val_accuracy: 0.3801 - val_loss: 1.0908\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 93ms/step - accuracy: 0.4076 - loss: 1.0751 - val_accuracy: 0.4006 - val_loss: 1.0844\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4246 - loss: 1.0634 - val_accuracy: 0.3890 - val_loss: 1.0850\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4400 - loss: 1.0456 - val_accuracy: 0.3987 - val_loss: 1.0971\n",
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 91ms/step - accuracy: 0.3635 - loss: 1.0930 - val_accuracy: 0.3692 - val_loss: 1.0923\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 97ms/step - accuracy: 0.3759 - loss: 1.0885 - val_accuracy: 0.3924 - val_loss: 1.0874\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 121ms/step - accuracy: 0.3967 - loss: 1.0815 - val_accuracy: 0.3813 - val_loss: 1.0860\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.4070 - loss: 1.0738 - val_accuracy: 0.3830 - val_loss: 1.0834\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.4128 - loss: 1.0735 - val_accuracy: 0.3858 - val_loss: 1.0822\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step\n",
            "\n",
            "Resultados Embedding Random (NO fine-tuneado)\n",
            "LSTM → Accuracy: 0.39867762687634023 Macro-F1: 0.33385069350207125\n",
            "GRU  → Accuracy: 0.3858112937812723 Macro-F1: 0.3545762899287412\n"
          ]
        }
      ],
      "source": [
        "#Cargamos el dataset tokenizado\n",
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "\n",
        "# Creamos la  columna text_joined a partir de tokens\n",
        "df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "texts = df[\"text_joined\"].astype(str).tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Tokenización y secuencias\n",
        "vocab_size = 20000\n",
        "maxlen = 100\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X_seq = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X_seq, maxlen=maxlen)\n",
        "\n",
        "# Codificamos las etiquetas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Hacemos el train/validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Aplicamos LSTM y GRU-\n",
        "def build_lstm_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "        LSTM(128, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(np.unique(y)), activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_gru_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "        GRU(128, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(np.unique(y)), activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Entrenamos LSTM\n",
        "lstm_model = build_lstm_model()\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Entrenamos GRU\n",
        "gru_model = build_gru_model()\n",
        "history_gru = gru_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Evaluamos los modelos\n",
        "def evaluate(model):\n",
        "    preds = np.argmax(model.predict(X_val), axis=1)\n",
        "    return accuracy_score(y_val, preds), f1_score(y_val, preds, average=\"macro\")\n",
        "\n",
        "acc_lstm, f1_lstm = evaluate(lstm_model)\n",
        "acc_gru, f1_gru = evaluate(gru_model)\n",
        "\n",
        "print(\"\\nResultados Embedding Random (NO fine-tuneado)\")\n",
        "print(\"LSTM → Accuracy:\", acc_lstm, \"Macro-F1:\", f1_lstm)\n",
        "print(\"GRU  → Accuracy:\", acc_gru, \"Macro-F1:\", f1_gru)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados obtenidos muestran que, sin información semántica previa ni capacidad de ajuste, el rendimiento de los modelos secuenciales es limitado. El modelo LSTM alcanza un accuracy cercano al 0.40 y un Macro-F1 alrededor de 0.33, mientras que GRU ofrece cifras similares, ligeramente inferiores en accuracy pero algo superiores en Macro-F1. Estas métricas evidencian que, al no permitir el fine-tuning y partir de embeddings aleatorios, los modelos no son capaces de capturar adecuadamente las relaciones lingüísticas del texto y, en consecuencia, su desempeño queda claramente por debajo de los modelos con embeddings preentrenados o ajustables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word2Vec congelado vs Word2Vec fine-tuneado vs Word2Vec from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 737ms/step - accuracy: 0.4120 - loss: 1.0719 - val_accuracy: 0.4355 - val_loss: 1.0549\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 1s/step - accuracy: 0.4564 - loss: 1.0241 - val_accuracy: 0.4694 - val_loss: 1.0021\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 575ms/step - accuracy: 0.4739 - loss: 0.9988 - val_accuracy: 0.4682 - val_loss: 0.9979\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 575ms/step - accuracy: 0.4852 - loss: 0.9866 - val_accuracy: 0.4973 - val_loss: 0.9732\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 433ms/step - accuracy: 0.4917 - loss: 0.9776 - val_accuracy: 0.4993 - val_loss: 0.9686\n",
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 398ms/step - accuracy: 0.4306 - loss: 1.0563 - val_accuracy: 0.4725 - val_loss: 1.0097\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 510ms/step - accuracy: 0.5197 - loss: 0.9549 - val_accuracy: 0.4961 - val_loss: 0.9732\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 349ms/step - accuracy: 0.5951 - loss: 0.8587 - val_accuracy: 0.5075 - val_loss: 0.9620\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 489ms/step - accuracy: 0.6678 - loss: 0.7425 - val_accuracy: 0.5048 - val_loss: 1.0228\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 510ms/step - accuracy: 0.7385 - loss: 0.6043 - val_accuracy: 0.5023 - val_loss: 1.1309\n",
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 372ms/step - accuracy: 0.3844 - loss: 1.0861 - val_accuracy: 0.4466 - val_loss: 1.0464\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 416ms/step - accuracy: 0.4685 - loss: 1.0323 - val_accuracy: 0.4500 - val_loss: 1.0501\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 348ms/step - accuracy: 0.5128 - loss: 0.9844 - val_accuracy: 0.4698 - val_loss: 1.0237\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 400ms/step - accuracy: 0.5701 - loss: 0.9108 - val_accuracy: 0.4727 - val_loss: 1.0337\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 345ms/step - accuracy: 0.6291 - loss: 0.8218 - val_accuracy: 0.4921 - val_loss: 1.0518\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step\n",
            "                    Accuracy  Macro-F1\n",
            "Word2Vec Frozen     0.499285  0.487054\n",
            "Word2Vec Fine-tune  0.502323  0.502254\n",
            "Word2Vec Scratch    0.492137  0.487125\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cragamos el dataset tokenizado\n",
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "texts = df[\"tokens\"].tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Codificamos las etiquetas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Hacemos el train/val split\n",
        "X_tr_text, X_val_text, y_tr, y_val = train_test_split(\n",
        "    texts, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Cargamos el Word2Vec preentrenado\n",
        "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
        "embedding_dim = w2v_model.vector_size\n",
        "\n",
        "word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_tr_idx = [tokens_to_indices(t, word_index) for t in X_tr_text]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
        "\n",
        "max_seq_len = 200\n",
        "X_tr_pad = pad_sequences(X_tr_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Funcion para crear LSTM\n",
        "def build_lstm_model(embedding_matrix, trainable=True):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                        output_dim=embedding_matrix.shape[1],\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_seq_len,\n",
        "                        trainable=trainable))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(1e-3),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Word2Vec Frozen\n",
        "lstm_frozen = build_lstm_model(embedding_matrix, trainable=False)\n",
        "history_frozen = lstm_frozen.fit(X_tr_pad, y_tr,\n",
        "                                 validation_data=(X_val_pad, y_val),\n",
        "                                 epochs=5,\n",
        "                                 batch_size=64)\n",
        "\n",
        "# Word2Vec Fine-tune\n",
        "lstm_finetune = build_lstm_model(embedding_matrix, trainable=True)\n",
        "history_finetune = lstm_finetune.fit(X_tr_pad, y_tr,\n",
        "                                     validation_data=(X_val_pad, y_val),\n",
        "                                     epochs=5,\n",
        "                                     batch_size=64)\n",
        "\n",
        "# Word2Vec Scratch\n",
        "embedding_matrix_random = np.random.normal(size=(vocab_size, embedding_dim))\n",
        "lstm_scratch = build_lstm_model(embedding_matrix_random, trainable=True)\n",
        "history_scratch = lstm_scratch.fit(X_tr_pad, y_tr,\n",
        "                                   validation_data=(X_val_pad, y_val),\n",
        "                                   epochs=5,\n",
        "                                   batch_size=64)\n",
        "\n",
        "# Evaluamos los modelos\n",
        "def evaluate(model, X_val, y_val):\n",
        "    preds = np.argmax(model.predict(X_val), axis=1)\n",
        "    acc = accuracy_score(y_val, preds)\n",
        "    f1 = f1_score(y_val, preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "results = {}\n",
        "results['Word2Vec Frozen'] = evaluate(lstm_frozen, X_val_pad, y_val)\n",
        "results['Word2Vec Fine-tune'] = evaluate(lstm_finetune, X_val_pad, y_val)\n",
        "results['Word2Vec Scratch'] = evaluate(lstm_scratch, X_val_pad, y_val)\n",
        "\n",
        "# Mostramos los resultados\n",
        "results_df = pd.DataFrame(results, index=['Accuracy','Macro-F1']).T\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados muestran que el enfoque más eficaz es Word2Vec fine-tuneado, que obtiene el mejor rendimiento tanto en accuracy como en Macro-F1, indicando que ajustar los embeddings al dominio específico de las noticias mejora la capacidad del modelo para capturar señales lingüísticas relevantes. El modelo  congelado alcanza métricas ligeramente inferiores, lo que sugiere que aunque los vectores preentrenados aportan una buena base semántica, limitar su actualización reduce su adaptabilidad a la tarea. Por último, el modelo entrenado desde cero obtiene el peor desempeño, reflejando que aprender embeddings sin preentrenamiento requiere muchos más datos y épocas para alcanzar niveles competitivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Comparación de embeddings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings tradicionales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'text_joined'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'text_joined'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 1. Load dataset\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m     15\u001b[39m df = pd.read_pickle(\u001b[33m\"\u001b[39m\u001b[33mdata/data_clean/train_tokenized.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m texts = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_joined\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m     18\u001b[39m labels = df[\u001b[33m\"\u001b[39m\u001b[33mbias\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Encode labels\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'text_joined'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ============================\n",
        "# 1. Cargar dataset tokenizado\n",
        "# ============================\n",
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "\n",
        "# Crear columna text_joined a partir de tokens si no existe\n",
        "if \"text_joined\" not in df.columns:\n",
        "    df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "texts = df[\"text_joined\"].astype(str).tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Codificar etiquetas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Train/validation split\n",
        "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "    texts, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Helper: train + evaluate\n",
        "# ============================\n",
        "def train_eval(model, X_train, X_val, name=\"\"):\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, preds)\n",
        "    f1 = f1_score(y_val, preds, average=\"macro\")\n",
        "    print(f\"{name} →  Accuracy: {acc:.4f} | Macro-F1: {f1:.4f}\")\n",
        "    return acc, f1\n",
        "\n",
        "results = {}\n",
        "\n",
        "# ============================\n",
        "# 2. Bag of Words\n",
        "# ============================\n",
        "vectorizer_bow = CountVectorizer(max_features=20000)\n",
        "X_train_bow = vectorizer_bow.fit_transform(X_train_text)\n",
        "X_val_bow   = vectorizer_bow.transform(X_val_text)\n",
        "\n",
        "results[\"BoW_LogReg\"] = train_eval(\n",
        "    LogisticRegression(max_iter=3000),\n",
        "    X_train_bow, X_val_bow,\n",
        "    \"BoW + Logistic Regression\"\n",
        ")\n",
        "\n",
        "results[\"BoW_SVM\"] = train_eval(\n",
        "    LinearSVC(),\n",
        "    X_train_bow, X_val_bow,\n",
        "    \"BoW + LinearSVM\"\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 3. TF-IDF (Unigram)\n",
        "# ============================\n",
        "vectorizer_tfidf_uni = TfidfVectorizer(max_features=20000, ngram_range=(1,1))\n",
        "X_train_tfidf_uni = vectorizer_tfidf_uni.fit_transform(X_train_text)\n",
        "X_val_tfidf_uni   = vectorizer_tfidf_uni.transform(X_val_text)\n",
        "\n",
        "results[\"TFIDF_uni_LogReg\"] = train_eval(\n",
        "    LogisticRegression(max_iter=3000),\n",
        "    X_train_tfidf_uni, X_val_tfidf_uni,\n",
        "    \"TF-IDF Unigram + Logistic Regression\"\n",
        ")\n",
        "\n",
        "results[\"TFIDF_uni_SVM\"] = train_eval(\n",
        "    LinearSVC(),\n",
        "    X_train_tfidf_uni, X_val_tfidf_uni,\n",
        "    \"TF-IDF Unigram + LinearSVM\"\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 4. TF-IDF (N-grams 1–2)\n",
        "# ============================\n",
        "vectorizer_tfidf_ngram = TfidfVectorizer(max_features=40000, ngram_range=(1,2))\n",
        "X_train_tfidf_ngram = vectorizer_tfidf_ngram.fit_transform(X_train_text)\n",
        "X_val_tfidf_ngram   = vectorizer_tfidf_ngram.transform(X_val_text)\n",
        "\n",
        "results[\"TFIDF_ngram_LogReg\"] = train_eval(\n",
        "    LogisticRegression(max_iter=3000),\n",
        "    X_train_tfidf_ngram, X_val_tfidf_ngram,\n",
        "    \"TF-IDF N-gram (1–2) + Logistic Regression\"\n",
        ")\n",
        "\n",
        "results[\"TFIDF_ngram_SVM\"] = train_eval(\n",
        "    LinearSVC(),\n",
        "    X_train_tfidf_ngram, X_val_tfidf_ngram,\n",
        "    \"TF-IDF N-gram (1–2) + LinearSVM\"\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Resultados finales\n",
        "# ============================\n",
        "print(\"\\n==============================\")\n",
        "print(\"Resultados embeddings tradicionales\")\n",
        "print(\"==============================\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: Acc={v[0]:.4f} | F1={v[1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings no contextuales(usamos embeddings entrenados con nuestro dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/embeddings/glove.6B.100d.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m glove_file = \u001b[33m\"\u001b[39m\u001b[33mdata/embeddings/glove.6B.100d.txt\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Ajustar path\u001b[39;00m\n\u001b[32m     33\u001b[39m embeddings_index = {}\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglove_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     36\u001b[39m         values = line.split()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/embeddings/glove.6B.100d.txt'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors, FastText\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# -----------------------------\n",
        "# Cargar dataset\n",
        "# -----------------------------\n",
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "texts = df[\"tokens\"].tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Codificar etiquetas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "X_tr_text, X_val_text, y_tr, y_val = train_test_split(\n",
        "    texts, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Cargar embeddings preentrenados (GloVe ejemplo)\n",
        "# -----------------------------\n",
        "# Opción 1: GloVe\n",
        "glove_file = \"data/embeddings/glove.6B.100d.txt\"  # Ajustar path\n",
        "embeddings_index = {}\n",
        "with open(glove_file, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = vector\n",
        "\n",
        "embedding_dim = 100\n",
        "word_index = {word: i+1 for i, word in enumerate(set([t for sublist in texts for t in sublist]))}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_tr_idx = [tokens_to_indices(t, word_index) for t in X_tr_text]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
        "\n",
        "max_seq_len = 200\n",
        "X_tr_pad = pad_sequences(X_tr_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "# Crear matriz de embedding\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "# -----------------------------\n",
        "# Definir modelo LSTM\n",
        "# -----------------------------\n",
        "def build_lstm_model(embedding_matrix, trainable=False):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                        output_dim=embedding_matrix.shape[1],\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_seq_len,\n",
        "                        trainable=trainable))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
        "    model.compile(optimizer=Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Entrenar\n",
        "lstm_model = build_lstm_model(embedding_matrix, trainable=True)\n",
        "lstm_model.fit(X_tr_pad, y_tr, validation_data=(X_val_pad, y_val), epochs=5, batch_size=64)\n",
        "\n",
        "# Evaluar\n",
        "preds = np.argmax(lstm_model.predict(X_val_pad), axis=1)\n",
        "acc = accuracy_score(y_val, preds)\n",
        "f1 = f1_score(y_val, preds, average='macro')\n",
        "print(\"GloVe LSTM → Accuracy:\", acc, \"Macro-F1:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings contextuales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de ejemplos: 27978\n",
            "Clases encontradas: {0, 1, 2}\n",
            "Device: cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceed711108af4700949764cf91378f84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01cc05198076450bb114198ca94b342c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa171e2fbb674b9f964561974cfb046a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2a3d29e241d4f219fc42e17e8f69a5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57df976174634208ac8116d4e8c9b4d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrayendo embeddings BERT (esto puede tardar)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c88ecae6a794345bbd738cfdeca9c39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extrayendo BERT embeddings:   0%|          | 0/875 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (27978, 768)\n",
            "Labels shape: (27978,)\n",
            "Distribución labels: {np.int64(0): np.int64(9750), np.int64(1): np.int64(7988), np.int64(2): np.int64(10240)}\n",
            "Entrenando Logistic Regression sobre embeddings BERT (frozen)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Resultados BERT frozen + LogisticRegression:\n",
            "Accuracy: 0.5709\n",
            "Macro-F1: 0.5669\n",
            "\n",
            "Classification report:\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'numpy.int64' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 127\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMacro-F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClassification report:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Guardar clasificador\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    132\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mdata/models\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3000\u001b[39m     longest_last_line_heading = \u001b[33m\"\u001b[39m\u001b[33mweighted avg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3001\u001b[39m     name_width = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3002\u001b[39m     width = \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[32m   3003\u001b[39m     head_fmt = \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[33ms} \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[33m\"\u001b[39m * \u001b[38;5;28mlen\u001b[39m(headers)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3000\u001b[39m     longest_last_line_heading = \u001b[33m\"\u001b[39m\u001b[33mweighted avg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3001\u001b[39m     name_width = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m target_names)\n\u001b[32m   3002\u001b[39m     width = \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[32m   3003\u001b[39m     head_fmt = \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[33ms} \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[33m\"\u001b[39m * \u001b[38;5;28mlen\u001b[39m(headers)\n",
            "\u001b[31mTypeError\u001b[39m: object of type 'numpy.int64' has no len()"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "#  1) EMBEDDINGS CONTEXTUALES (freeze) + CLASSIFIER\n",
        "# ===============================\n",
        "# Requisitos: transformers, torch, scikit-learn\n",
        "# pip install transformers torch scikit-learn --upgrade\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import joblib\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ===============================\n",
        "# Cargar dataset tokenizado / limpio\n",
        "# ===============================\n",
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "\n",
        "# Crear la columna text_joined si no existe\n",
        "if \"text_joined\" not in df.columns:\n",
        "    df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Textos y etiquetas\n",
        "texts = df[\"text_joined\"].astype(str).tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "print(f\"Número de ejemplos: {len(texts)}\")\n",
        "print(f\"Clases encontradas: {set(labels)}\")\n",
        "\n",
        "# ===============================\n",
        "# Configuración del modelo\n",
        "# ===============================\n",
        "MODEL_NAME = \"distilbert-base-uncased\"   # ligero; puedes cambiarlo\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 128\n",
        "EMB_SAVE_PATH = \"data/features/bert_cls_embeddings.npy\"\n",
        "LABEL_SAVE_PATH = \"data/features/bert_labels.npy\"\n",
        "TOKENIZER_SAVE = \"data/features/bert_tokenizer\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ===============================\n",
        "# Cargar tokenizer y modelo (PyTorch)\n",
        "# ===============================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "model.to(DEVICE)\n",
        "model.eval()  # freeze embeddings\n",
        "\n",
        "# ===============================\n",
        "# Función para extraer embeddings CLS por batches\n",
        "# ===============================\n",
        "def extract_cls_embeddings(texts, batch_size=BATCH_SIZE, max_len=MAX_LEN):\n",
        "    all_embs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extrayendo BERT embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        enc = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            out = model(**enc)\n",
        "            cls_embs = out.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            all_embs.append(cls_embs)\n",
        "    all_embs = np.vstack(all_embs)\n",
        "    return all_embs\n",
        "\n",
        "# ===============================\n",
        "# Extraer embeddings (si ya existen, cargarlos)\n",
        "# ===============================\n",
        "os.makedirs(\"data/features\", exist_ok=True)\n",
        "\n",
        "if os.path.exists(EMB_SAVE_PATH) and os.path.exists(LABEL_SAVE_PATH):\n",
        "    print(\"Cargando embeddings guardados...\")\n",
        "    X = np.load(EMB_SAVE_PATH)\n",
        "    y = np.load(LABEL_SAVE_PATH)\n",
        "    le = joblib.load(\"data/features/label_encoder.joblib\")\n",
        "else:\n",
        "    print(\"Extrayendo embeddings BERT (esto puede tardar)...\")\n",
        "    X = extract_cls_embeddings(texts)\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "    np.save(EMB_SAVE_PATH, X)\n",
        "    np.save(LABEL_SAVE_PATH, y)\n",
        "    joblib.dump(le, \"data/features/label_encoder.joblib\")\n",
        "    tokenizer.save_pretrained(TOKENIZER_SAVE)\n",
        "\n",
        "print(\"Embeddings shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(\"Distribución labels:\", dict(zip(unique, counts)))\n",
        "\n",
        "# ===============================\n",
        "# Split train/validation\n",
        "# ===============================\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# Entrenar clasificador LogisticRegression\n",
        "# ===============================\n",
        "clf = LogisticRegression(max_iter=2000, multi_class=\"multinomial\", n_jobs=-1, C=1.0)\n",
        "print(\"Entrenando Logistic Regression sobre embeddings BERT (frozen)...\")\n",
        "clf.fit(X_tr, y_tr)\n",
        "\n",
        "# ===============================\n",
        "# Evaluación\n",
        "# ===============================\n",
        "y_pred = clf.predict(X_val)\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "macro_f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"\\nResultados BERT frozen + LogisticRegression:\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Macro-F1: {macro_f1:.4f}\\n\")\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_val, y_pred, target_names=le.classes_))\n",
        "\n",
        "# ===============================\n",
        "# Guardar clasificador\n",
        "# ===============================\n",
        "os.makedirs(\"data/models\", exist_ok=True)\n",
        "joblib.dump(clf, \"data/models/bert_frozen_logreg.pkl\")\n",
        "print(\"Modelo guardado en data/models/bert_frozen_logreg.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvy29jCJWbqI"
      },
      "source": [
        "# **4. Tabla Comaprativa de Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entender los resultados, hay que aclarar los parámetros utilizados para cada variable en shallow learning.\n",
        "\n",
        "En el caso de la variable source, hemos tenido que reducir el número de estimadores del Random Forest de 300 a 150. Además, el número de estimadores del XGBoost también ha sido reducido de 200 a 75. Sin esta reducción, no habriamos sido capaces de terminar la ejecución de la celda. Ha llegado a estar más de una hora y seguía sin terminar de ejecutarse. \n",
        "\n",
        "En cuanto a la variable topic, además de las rebajas aplicadas al caso de la variable source, hemos decidido quitar el modelo XGBoost, ya que no termina de ejecutarse. Disponemos de equipos con capacidades técnicas muy limitadas, por lo que, con mejores ordenadores, no se tendrían que reducir los valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_bias = {\n",
        "    \"Logistic Regression\": {\"Accuracy\":  0.702109  , \"Macro-F1\":0.700091},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.698713, \"Macro-F1\":0.696551},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.683881, \"Macro-F1\": 0.679829},\n",
        "    \"XGBoost\": {\"Accuracy\":0.734632   , \"Macro-F1\": 0.733918}\n",
        "}\n",
        "\n",
        "results_topic = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.566655  , \"Macro-F1\":0.312507},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.566476, \"Macro-F1\": 0.374703},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.507863, \"Macro-F1\": 0.230810},\n",
        "    \"XGBoost\": {\"Accuracy\": 0.499643, \"Macro-F1\": 0.325067}\n",
        "}\n",
        "\n",
        "results_source = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.503401, \"Macro-F1\":0.103117},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.559076, \"Macro-F1\":0.219572},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.500000, \"Macro-F1\": 0.120979},\n",
        "    \"XGBoost\": {\"Accuracy\": 0.532581, \"Macro-F1\": 0.223251}\n",
        "}\n",
        "\n",
        "# Crear DataFrame comparativo manualmente\n",
        "rows = []\n",
        "for model in [\"Logistic Regression\", \"LinearSVC\", \"Random Forest\", \"XGBoost\"]:\n",
        "    row = {\n",
        "        \"Model\": model,\n",
        "        \"Bias Accuracy\": results_bias[model][\"Accuracy\"],\n",
        "        \"Bias Macro-F1\": results_bias[model][\"Macro-F1\"],\n",
        "        \"Topic Accuracy\": results_topic[model][\"Accuracy\"],\n",
        "        \"Topic Macro-F1\": results_topic[model][\"Macro-F1\"],\n",
        "        \"Source Accuracy\": results_source[model][\"Accuracy\"],\n",
        "        \"Source Macro-F1\": results_source[model][\"Macro-F1\"]\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "df_comparison = pd.DataFrame(rows)\n",
        "print(df_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7iaL0qQWhoh"
      },
      "source": [
        "# **5. Interpretabilidad**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
