{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de empezar con la explicacion de esta entrega debemos mencionar la correcion de dos errores de la entrega anterior. Conseguimos aplicar stemming y lematizacion solo para TD-IDF y no para los embeddings, como hicimos anteriormente. Ademas,  adaptamos el Word2Vec para que no tuviese mas de 30 epochs.\n",
        "\n",
        "Por otro lado, hemos sido capaces de aplicar shallow learning a las tres tareas de clasificacion que teniamos previstas. Sin embargo, solo hemos podido aplicar deep learning y comparacion de embeddings a la clasificacion de sesgo. Las dos tareas restantes estaran completadas para la siguiente entrega."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfT1kYc_EOn"
      },
      "source": [
        "# **1. Shallow Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from collections import Counter\n",
        "from gensim.models import FastText, Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Embedding, Input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para representar los textos, hemos elegido TF-IDF para capturar la importancia relativa de cada palabra en un documento frente al corpus completo, reduciendo el peso de palabras muy frecuentes que no aportan información discriminativa, como artículos y preposiciones en inglés. Hemos establecido un  límite de 3,000 palabras más importantes para reducir la dimensionalidad. Ademas hemos añadido y unigramas y bigramas para capturar algo de contexto local sin sobrecargar el modelo. Por otro lado, hemos eliminado las palabras vacías en inglés para centrar el análisis en palabras significativas. Esta representación genera vectores dispersos que son ideales para los modelos clásicos que usamos.\n",
        "\n",
        "Hemos seleccionado cuatro modelos para evaluar el desempeño: Logistic Regression, LinearSVC, Random Forest y XGBoost. Esta combinación nos permite cubrir tanto modelos lineales como no lineales y comparar rapidez, precisión y estabilidad.\n",
        "\n",
        "Antes de entrenar, hemos convertido los textos a números mediante label encoding, y  una división de train/validation del 80/20 para medir el rendimiento real y evitar overfitting. Además, hemos filtrado las clases con menos de dos registros, ya que la validación estratificada requiere al menos dos ejemplos por clase. La funcion recibe como parametro la variable objetivo. En este caso, recibe las variables \"bias\", \"topic\" y \"source\", que son nuestras variables a clasificar.\n",
        "\n",
        "Finalmente, todos los modelos y el vectorizador TF-IDF han sido guardados para su reutilización. Este pipeline de Shallow Learning funciona como un baseline sólido que nos permite medir la mejora que aportan las representaciones densas y contextuales de texto que se utilizarán en las fases posteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shallow_pipeline(df, target_col):\n",
        "    # Preparamos el texto\n",
        "    if \"text_joined\" not in df.columns:\n",
        "        df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "    texts = df[\"text_joined\"].astype(str).tolist()\n",
        "    labels = df[target_col].tolist()\n",
        "\n",
        "    # Filtramos las clases con menos de 2 registros\n",
        "    counts = Counter(labels)\n",
        "    valid_classes = [c for c, cnt in counts.items() if cnt > 1]\n",
        "    mask = [lbl in valid_classes for lbl in labels]\n",
        "    texts = [t for t, m in zip(texts, mask) if m]\n",
        "    labels = [l for l, m in zip(labels, mask) if m]\n",
        "\n",
        "    # Codificamos las etiquetas\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "\n",
        "    # Hacemos el train/validation split\n",
        "    X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "        texts, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Aplicamos TF-IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))\n",
        "    X_train = vectorizer.fit_transform(X_train_text)\n",
        "    X_val = vectorizer.transform(X_val_text)\n",
        "\n",
        "    # Definimos los modelos\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
        "        \"LinearSVC\": LinearSVC(),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=150, n_jobs=-1),\n",
        "       # \"XGBoost\": XGBClassifier(n_estimators=75, eval_metric=\"mlogloss\", tree_method=\"hist\", n_jobs=-1)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Entrenamos, evaluamos y guardamos los modelos\n",
        "    os.makedirs(\"data/models\", exist_ok=True)\n",
        "    for name, model in models.items():\n",
        "        print(f\"Entrenando {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy_score(y_val, y_pred),\n",
        "            \"Macro-F1\": f1_score(y_val, y_pred, average=\"macro\")\n",
        "        }\n",
        "        # Guardamos el modelo\n",
        "        pickle.dump(model, open(f\"data/models/{name.replace(' ', '_').lower()}.pkl\", \"wb\"))\n",
        "\n",
        "    # Guardamos el vectorizador\n",
        "    os.makedirs(\"data/features\", exist_ok=True)\n",
        "    pickle.dump(vectorizer, open(\"data/features/tfidf_vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos tenido que incluir un filtro para eliminar las clases que tenían menos de dos registros antes de hacer el train_test_split. Esto se debe a que tuvimos un error al utilizar el parámetro stratify=y, que requiere al menos dos ejemplos por clase para poder crear correctamente los conjuntos de entrenamiento y validación de manera estratificada. Sin este filtro, cualquier clase con un único ejemplo provocaría que la ejecución se detuviera, como ocurría previamente con la variable source. Al aplicar este filtrado, nos aseguramos de que solo se utilicen clases con suficiente cantidad de datos, garantizando que la partición estratificada funcione y evitando que el pipeline falle durante el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados se analizarán en la sección 4 de este noteebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n",
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "Entrenando XGBoost...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.702109  0.700091\n",
            "LinearSVC            0.698713  0.696551\n",
            "Random Forest        0.689778  0.684783\n",
            "XGBoost              0.734632  0.733918\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con bias\n",
        "results_bias = shallow_pipeline(df, \"bias\")\n",
        "print(pd.DataFrame(results_bias).T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n",
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.581129  0.338987\n",
            "LinearSVC            0.589171  0.410156\n",
            "Random Forest        0.519657  0.250003\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con topic\n",
        "results_topic = shallow_pipeline(df, \"topic\")\n",
        "print(pd.DataFrame(results_topic).T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n",
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "Entrenando XGBoost...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.503401  0.103117\n",
            "LinearSVC            0.559076  0.219572\n",
            "Random Forest        0.500000  0.120979\n",
            "XGBoost              0.532581  0.223251\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con source\n",
        "results_source = shallow_pipeline(df, \"source\")\n",
        "print(pd.DataFrame(results_source).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztbnwVJVWSgX"
      },
      "source": [
        "# **2. Modelos Deep**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la parte de Deep Learning, hemos optado por utilizar redes neuronales recurrentes, específicamente LSTM y GRU, debido a su capacidad para capturar dependencias secuenciales en el texto. A diferencia de los modelos de Shallow Learning, que tratan cada palabra o n-grama de manera independiente, las RNNs permiten que la red recuerde información contextual de palabras anteriores en la secuencia, lo cual es crucial para nuestras tareas de clasificación de texto donde el significado puede depender del orden de las palabras.\n",
        "\n",
        "Para la representación de los textos, hemos empleado embeddings densos, utilizando tres enfoques distintos con Word2Vec: congelado, fine-tuneado y desde cero. En el caso de los embeddings congelados, utilizamos un modelo preentrenado de Word2Vec y lo fijamos durante el entrenamiento de la red, de manera que solo la LSTM o GRU aprenda a combinar los vectores preexistentes. Esto permite evaluar cuánto conocimiento semántico ya capturado en Word2Vec puede ayudar a la tarea sin modificarlo. En el enfoque de fine-tune, los embeddings inicializados con Word2Vec se ajustan durante el entrenamiento, permitiendo que la red adapte los vectores a las particularidades del dataset específico. Finalmente, la opción de embeddings entrenados desde cero crea vectores aleatorios que se aprenden completamente durante el entrenamiento, lo que permite que la red descubra representaciones óptimas para la tarea, aunque requiere más datos y tiempo de entrenamiento.\n",
        "\n",
        "Hemos elegido LSTM y GRU ya que cumple nuestra necesidad de comparar dos variantes de redes recurrentes: las LSTM tienen una mayor capacidad para capturar dependencias de largo plazo mediante su mecanismo de puertas, mientras que las GRU son más simples y computacionalmente eficientes, lo que puede acelerar el entrenamiento sin perder demasiado rendimiento. \n",
        "\n",
        "Los textos se transforman primero en secuencias de índices según el vocabulario de Word2Vec o un tokenizer entrenado sobre el dataset, y se aplica padding para unificar la longitud de las secuencias. Esto asegura que las redes puedan procesar lotes de datos de manera eficiente. Finalmente, la capa de salida utiliza softmax para producir probabilidades sobre las clases, y la red se entrena con categorical crossentropy, optimizando la accuracy y el macro-F1 como métricas de desempeño, lo cual es consistente con la evaluación utilizada en la parte de Shallow Learning.\n",
        "\n",
        "En conclusion, en este apartado permite que nuestra red aprenda tanto representaciones densas de palabras como patrones secuenciales de las oraciones, ofreciendo una ventaja sobre los modelos lineales y de ensamble de Shallow Learning que solo utilizan información superficial y dispersa de los textos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings finetuneados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 322ms/step - accuracy: 0.4257 - loss: 1.0562 - val_accuracy: 0.4750 - val_loss: 1.0236\n",
            "Epoch 2/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 311ms/step - accuracy: 0.5227 - loss: 0.9537 - val_accuracy: 0.4959 - val_loss: 0.9828\n",
            "Epoch 3/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 340ms/step - accuracy: 0.5904 - loss: 0.8587 - val_accuracy: 0.5234 - val_loss: 0.9534\n",
            "Epoch 4/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 428ms/step - accuracy: 0.6674 - loss: 0.7404 - val_accuracy: 0.5173 - val_loss: 1.0088\n",
            "Epoch 5/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 373ms/step - accuracy: 0.7374 - loss: 0.6141 - val_accuracy: 0.5116 - val_loss: 1.0896\n",
            "Epoch 6/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 521ms/step - accuracy: 0.8024 - loss: 0.4791 - val_accuracy: 0.5046 - val_loss: 1.2833\n",
            "Epoch 7/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 1s/step - accuracy: 0.8611 - loss: 0.3566 - val_accuracy: 0.5095 - val_loss: 1.3578\n",
            "Epoch 8/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 1s/step - accuracy: 0.8989 - loss: 0.2583 - val_accuracy: 0.5114 - val_loss: 1.6459\n",
            "Epoch 9/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 533ms/step - accuracy: 0.9331 - loss: 0.1715 - val_accuracy: 0.5161 - val_loss: 1.8227\n",
            "Epoch 10/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 380ms/step - accuracy: 0.9541 - loss: 0.1253 - val_accuracy: 0.5134 - val_loss: 2.1535\n",
            "Epoch 1/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 290ms/step - accuracy: 0.4016 - loss: 1.0760 - val_accuracy: 0.4307 - val_loss: 1.0498\n",
            "Epoch 2/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 367ms/step - accuracy: 0.4854 - loss: 1.0039 - val_accuracy: 0.4805 - val_loss: 0.9938\n",
            "Epoch 3/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 331ms/step - accuracy: 0.5812 - loss: 0.8742 - val_accuracy: 0.5088 - val_loss: 0.9586\n",
            "Epoch 4/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 320ms/step - accuracy: 0.6687 - loss: 0.7393 - val_accuracy: 0.5079 - val_loss: 0.9995\n",
            "Epoch 5/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 318ms/step - accuracy: 0.7584 - loss: 0.5768 - val_accuracy: 0.5250 - val_loss: 1.0911\n",
            "Epoch 6/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 323ms/step - accuracy: 0.8357 - loss: 0.4116 - val_accuracy: 0.5366 - val_loss: 1.2710\n",
            "Epoch 7/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 474ms/step - accuracy: 0.8903 - loss: 0.2854 - val_accuracy: 0.5359 - val_loss: 1.4438\n",
            "Epoch 8/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 618ms/step - accuracy: 0.9309 - loss: 0.1858 - val_accuracy: 0.5229 - val_loss: 1.7583\n",
            "Epoch 9/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 414ms/step - accuracy: 0.9538 - loss: 0.1276 - val_accuracy: 0.5304 - val_loss: 1.9515\n",
            "Epoch 10/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 350ms/step - accuracy: 0.9701 - loss: 0.0880 - val_accuracy: 0.5264 - val_loss: 2.2296\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step\n",
            "Resultados:\n",
            "      Accuracy  Macro-F1\n",
            "LSTM  0.513402  0.514574\n",
            "GRU   0.526447  0.526548\n"
          ]
        }
      ],
      "source": [
        "# Cargamos el dataset tokenizado\n",
        "df_train = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "# Codificamos los labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_cat = to_categorical(y_encoded)\n",
        "\n",
        "# Split train/validation\n",
        "X_tr_text, X_val_text, y_tr, y_val = train_test_split(\n",
        "    df_train[\"tokens\"], y_cat, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Cargamos el Word2Vec preentrenado de la anterior entrega\n",
        "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
        "embedding_dim = w2v_model.vector_size\n",
        "\n",
        "# Creamos el vocabulario e índices\n",
        "word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1  # +1 para padding\n",
        "\n",
        "# Convertimos los tokens a índices\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_tr_idx = [tokens_to_indices(t, word_index) for t in X_tr_text]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
        "\n",
        "# Aplicamos padding\n",
        "max_seq_len = 200\n",
        "X_tr_pad = pad_sequences(X_tr_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "# Creamos la matriz de embedding \n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Definimos y entrenamos los modelos \n",
        "\n",
        "def build_rnn(model_type='LSTM'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_seq_len,\n",
        "                        trainable=True))  # Fine-tune embeddings\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# LSTM\n",
        "lstm_model = build_rnn('LSTM')\n",
        "lstm_history = lstm_model.fit(X_tr_pad, y_tr,\n",
        "                              validation_data=(X_val_pad, y_val),\n",
        "                              epochs=10,\n",
        "                              batch_size=64)\n",
        "\n",
        "# GRU\n",
        "gru_model = build_rnn('GRU')\n",
        "gru_history = gru_model.fit(X_tr_pad, y_tr,\n",
        "                            validation_data=(X_val_pad, y_val),\n",
        "                            epochs=10,\n",
        "                            batch_size=64)\n",
        "\n",
        "# Evaluamos los modelos\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lstm = lstm_model.predict(X_val_pad, batch_size=64)\n",
        "y_pred_gru = gru_model.predict(X_val_pad, batch_size=64)\n",
        "\n",
        "y_pred_lstm_labels = np.argmax(y_pred_lstm, axis=1)\n",
        "y_pred_gru_labels = np.argmax(y_pred_gru, axis=1)\n",
        "y_val_labels = np.argmax(y_val, axis=1)\n",
        "\n",
        "# Métricas\n",
        "results = {\n",
        "    'LSTM': {\n",
        "        'Accuracy': accuracy_score(y_val_labels, y_pred_lstm_labels),\n",
        "        'Macro-F1': f1_score(y_val_labels, y_pred_lstm_labels, average='macro')\n",
        "    },\n",
        "    'GRU': {\n",
        "        'Accuracy': accuracy_score(y_val_labels, y_pred_gru_labels),\n",
        "        'Macro-F1': f1_score(y_val_labels, y_pred_gru_labels, average='macro')\n",
        "    }\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"Resultados:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings no finetuneados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 90ms/step - accuracy: 0.3635 - loss: 1.0929 - val_accuracy: 0.3672 - val_loss: 1.0934\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.3830 - loss: 1.0869 - val_accuracy: 0.3801 - val_loss: 1.0908\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 93ms/step - accuracy: 0.4076 - loss: 1.0751 - val_accuracy: 0.4006 - val_loss: 1.0844\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4246 - loss: 1.0634 - val_accuracy: 0.3890 - val_loss: 1.0850\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4400 - loss: 1.0456 - val_accuracy: 0.3987 - val_loss: 1.0971\n",
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 91ms/step - accuracy: 0.3635 - loss: 1.0930 - val_accuracy: 0.3692 - val_loss: 1.0923\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 97ms/step - accuracy: 0.3759 - loss: 1.0885 - val_accuracy: 0.3924 - val_loss: 1.0874\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 121ms/step - accuracy: 0.3967 - loss: 1.0815 - val_accuracy: 0.3813 - val_loss: 1.0860\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.4070 - loss: 1.0738 - val_accuracy: 0.3830 - val_loss: 1.0834\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.4128 - loss: 1.0735 - val_accuracy: 0.3858 - val_loss: 1.0822\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step\n",
            "\n",
            "Resultados Embedding Random (NO fine-tuneado)\n",
            "LSTM → Accuracy: 0.39867762687634023 Macro-F1: 0.33385069350207125\n",
            "GRU  → Accuracy: 0.3858112937812723 Macro-F1: 0.3545762899287412\n"
          ]
        }
      ],
      "source": [
        "# Creamos la  columna text_joined a partir de tokens\n",
        "df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "texts = df[\"text_joined\"].astype(str).tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Tokenización y secuencias\n",
        "vocab_size = 20000\n",
        "maxlen = 100\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X_seq = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X_seq, maxlen=maxlen)\n",
        "\n",
        "# Codificamos las etiquetas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Hacemos el train/validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Aplicamos LSTM y GRU-\n",
        "def build_lstm_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "        LSTM(128, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(np.unique(y)), activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_gru_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "        GRU(128, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(np.unique(y)), activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Entrenamos LSTM\n",
        "lstm_model = build_lstm_model()\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Entrenamos GRU\n",
        "gru_model = build_gru_model()\n",
        "history_gru = gru_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# Evaluamos los modelos\n",
        "def evaluate(model):\n",
        "    preds = np.argmax(model.predict(X_val), axis=1)\n",
        "    return accuracy_score(y_val, preds), f1_score(y_val, preds, average=\"macro\")\n",
        "\n",
        "acc_lstm, f1_lstm = evaluate(lstm_model)\n",
        "acc_gru, f1_gru = evaluate(gru_model)\n",
        "\n",
        "print(\"\\nResultados Embedding Random (NO fine-tuneado)\")\n",
        "print(\"LSTM → Accuracy:\", acc_lstm, \"Macro-F1:\", f1_lstm)\n",
        "print(\"GRU  → Accuracy:\", acc_gru, \"Macro-F1:\", f1_gru)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word2Vec congelado vs Word2Vec fine-tuneado vs Word2Vec from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 737ms/step - accuracy: 0.4120 - loss: 1.0719 - val_accuracy: 0.4355 - val_loss: 1.0549\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 1s/step - accuracy: 0.4564 - loss: 1.0241 - val_accuracy: 0.4694 - val_loss: 1.0021\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 575ms/step - accuracy: 0.4739 - loss: 0.9988 - val_accuracy: 0.4682 - val_loss: 0.9979\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 575ms/step - accuracy: 0.4852 - loss: 0.9866 - val_accuracy: 0.4973 - val_loss: 0.9732\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 433ms/step - accuracy: 0.4917 - loss: 0.9776 - val_accuracy: 0.4993 - val_loss: 0.9686\n",
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 398ms/step - accuracy: 0.4306 - loss: 1.0563 - val_accuracy: 0.4725 - val_loss: 1.0097\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 510ms/step - accuracy: 0.5197 - loss: 0.9549 - val_accuracy: 0.4961 - val_loss: 0.9732\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 349ms/step - accuracy: 0.5951 - loss: 0.8587 - val_accuracy: 0.5075 - val_loss: 0.9620\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 489ms/step - accuracy: 0.6678 - loss: 0.7425 - val_accuracy: 0.5048 - val_loss: 1.0228\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 510ms/step - accuracy: 0.7385 - loss: 0.6043 - val_accuracy: 0.5023 - val_loss: 1.1309\n",
            "Epoch 1/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 372ms/step - accuracy: 0.3844 - loss: 1.0861 - val_accuracy: 0.4466 - val_loss: 1.0464\n",
            "Epoch 2/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 416ms/step - accuracy: 0.4685 - loss: 1.0323 - val_accuracy: 0.4500 - val_loss: 1.0501\n",
            "Epoch 3/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 348ms/step - accuracy: 0.5128 - loss: 0.9844 - val_accuracy: 0.4698 - val_loss: 1.0237\n",
            "Epoch 4/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 400ms/step - accuracy: 0.5701 - loss: 0.9108 - val_accuracy: 0.4727 - val_loss: 1.0337\n",
            "Epoch 5/5\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 345ms/step - accuracy: 0.6291 - loss: 0.8218 - val_accuracy: 0.4921 - val_loss: 1.0518\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 59ms/step\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step\n",
            "                    Accuracy  Macro-F1\n",
            "Word2Vec Frozen     0.499285  0.487054\n",
            "Word2Vec Fine-tune  0.502323  0.502254\n",
            "Word2Vec Scratch    0.492137  0.487125\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cargamos el dataset tokenizado\n",
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "texts = df[\"tokens\"].tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Codificamos las etiquetas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Hacemos el train/val split\n",
        "X_tr_text, X_val_text, y_tr, y_val = train_test_split(\n",
        "    texts, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Cargamos el Word2Vec preentrenado\n",
        "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
        "embedding_dim = w2v_model.vector_size\n",
        "\n",
        "word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_tr_idx = [tokens_to_indices(t, word_index) for t in X_tr_text]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
        "\n",
        "max_seq_len = 200\n",
        "X_tr_pad = pad_sequences(X_tr_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Funcion para crear LSTM\n",
        "def build_lstm_model(embedding_matrix, trainable=True):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                        output_dim=embedding_matrix.shape[1],\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_seq_len,\n",
        "                        trainable=trainable))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(1e-3),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Word2Vec Frozen\n",
        "lstm_frozen = build_lstm_model(embedding_matrix, trainable=False)\n",
        "history_frozen = lstm_frozen.fit(X_tr_pad, y_tr,\n",
        "                                 validation_data=(X_val_pad, y_val),\n",
        "                                 epochs=5,\n",
        "                                 batch_size=64)\n",
        "\n",
        "# Word2Vec Fine-tune\n",
        "lstm_finetune = build_lstm_model(embedding_matrix, trainable=True)\n",
        "history_finetune = lstm_finetune.fit(X_tr_pad, y_tr,\n",
        "                                     validation_data=(X_val_pad, y_val),\n",
        "                                     epochs=5,\n",
        "                                     batch_size=64)\n",
        "\n",
        "# Word2Vec Scratch\n",
        "embedding_matrix_random = np.random.normal(size=(vocab_size, embedding_dim))\n",
        "lstm_scratch = build_lstm_model(embedding_matrix_random, trainable=True)\n",
        "history_scratch = lstm_scratch.fit(X_tr_pad, y_tr,\n",
        "                                   validation_data=(X_val_pad, y_val),\n",
        "                                   epochs=5,\n",
        "                                   batch_size=64)\n",
        "\n",
        "# Evaluamos los modelos\n",
        "def evaluate(model, X_val, y_val):\n",
        "    preds = np.argmax(model.predict(X_val), axis=1)\n",
        "    acc = accuracy_score(y_val, preds)\n",
        "    f1 = f1_score(y_val, preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "results = {}\n",
        "results['Word2Vec Frozen'] = evaluate(lstm_frozen, X_val_pad, y_val)\n",
        "results['Word2Vec Fine-tune'] = evaluate(lstm_finetune, X_val_pad, y_val)\n",
        "results['Word2Vec Scratch'] = evaluate(lstm_scratch, X_val_pad, y_val)\n",
        "\n",
        "# Mostramos los resultados\n",
        "results_df = pd.DataFrame(results, index=['Accuracy','Macro-F1']).T\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Comparación de embeddings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings tradicionales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TF-IDF</th>\n",
              "      <td>0.703002</td>\n",
              "      <td>0.702330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bag-of-Words</th>\n",
              "      <td>0.646712</td>\n",
              "      <td>0.646754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Accuracy  F1-score\n",
              "TF-IDF        0.703002  0.702330\n",
              "Bag-of-Words  0.646712  0.646754"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =======================\n",
        "# 1. EMBEDDINGS TRADICIONALES\n",
        "# TF-IDF + Bag-of-Words\n",
        "# =======================\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Preparamos texto\n",
        "df_train = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "df_train[\"text_joined\"] = df_train[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "texts = df_train[\"text_joined\"].tolist()\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "results = {}\n",
        "\n",
        "# ---------- TF-IDF ----------\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "X_tfidf = tfidf.fit_transform(texts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "results[\"TF-IDF\"] = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"F1-score\": f1_score(y_test, y_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "# ---------- Bag-of-Words ----------\n",
        "bow = CountVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "X_bow = bow.fit_transform(texts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "results[\"Bag-of-Words\"] = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"F1-score\": f1_score(y_test, y_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "# ---------- RESULTADOS ----------\n",
        "pd.DataFrame(results).T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings no contextuales(usamos embeddings entrenados con nuestro dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downstream (clustering) Word2Vec: 183562.484375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# 2. EMBEDDINGS NO-CONTEXTUALES\n",
        "# Word2Vec + FastText\n",
        "# =======================\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Preparamos tokens\n",
        "sentences = df_train[\"tokens\"].tolist()\n",
        "\n",
        "# ---------- Word2Vec ----------\n",
        "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=3, workers=4, sg=1)\n",
        "w2v_model.save(\"data/embeddings/word2vec.model\")\n",
        "\n",
        "# Métrica de clustering simple\n",
        "def cluster_quality(model, n_clusters=5):\n",
        "    X = model.wv.vectors\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    return kmeans.inertia_\n",
        "\n",
        "print(\"Downstream (clustering) Word2Vec:\", cluster_quality(w2v_model))\n",
        "\n",
        "# ---------- FastText ----------\n",
        "fasttext_model = FastText(sentences=sentences, vector_size=100, window=5, min_count=3, workers=4, sg=1)\n",
        "fasttext_model.save(\"data/embeddings/fasttext.model\")\n",
        "\n",
        "print(\"Downstream (clustering) FastText:\", cluster_quality(fasttext_model))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings contextuales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de ejemplos: 27978\n",
            "Clases encontradas: {0, 1, 2}\n",
            "Device: cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceed711108af4700949764cf91378f84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01cc05198076450bb114198ca94b342c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa171e2fbb674b9f964561974cfb046a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2a3d29e241d4f219fc42e17e8f69a5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57df976174634208ac8116d4e8c9b4d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrayendo embeddings BERT (esto puede tardar)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c88ecae6a794345bbd738cfdeca9c39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extrayendo BERT embeddings:   0%|          | 0/875 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (27978, 768)\n",
            "Labels shape: (27978,)\n",
            "Distribución labels: {np.int64(0): np.int64(9750), np.int64(1): np.int64(7988), np.int64(2): np.int64(10240)}\n",
            "Entrenando Logistic Regression sobre embeddings BERT (frozen)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Resultados BERT frozen + LogisticRegression:\n",
            "Accuracy: 0.5709\n",
            "Macro-F1: 0.5669\n",
            "\n",
            "Classification report:\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'numpy.int64' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 127\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMacro-F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClassification report:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Guardar clasificador\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    132\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mdata/models\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3000\u001b[39m     longest_last_line_heading = \u001b[33m\"\u001b[39m\u001b[33mweighted avg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3001\u001b[39m     name_width = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3002\u001b[39m     width = \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[32m   3003\u001b[39m     head_fmt = \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[33ms} \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[33m\"\u001b[39m * \u001b[38;5;28mlen\u001b[39m(headers)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3001\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3000\u001b[39m     longest_last_line_heading = \u001b[33m\"\u001b[39m\u001b[33mweighted avg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3001\u001b[39m     name_width = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cn \u001b[38;5;129;01min\u001b[39;00m target_names)\n\u001b[32m   3002\u001b[39m     width = \u001b[38;5;28mmax\u001b[39m(name_width, \u001b[38;5;28mlen\u001b[39m(longest_last_line_heading), digits)\n\u001b[32m   3003\u001b[39m     head_fmt = \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m:>\u001b[39m\u001b[38;5;132;01m{width}\u001b[39;00m\u001b[33ms} \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{:>9}\u001b[39;00m\u001b[33m\"\u001b[39m * \u001b[38;5;28mlen\u001b[39m(headers)\n",
            "\u001b[31mTypeError\u001b[39m: object of type 'numpy.int64' has no len()"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# 2. EMBEDDINGS NO-CONTEXTUALES\n",
        "# Word2Vec + FastText\n",
        "# =======================\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Preparamos tokens\n",
        "sentences = df_train[\"tokens\"].tolist()\n",
        "\n",
        "# ---------- Word2Vec ----------\n",
        "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=3, workers=4, sg=1)\n",
        "w2v_model.save(\"data/embeddings/word2vec.model\")\n",
        "\n",
        "# Métrica de clustering simple\n",
        "def cluster_quality(model, n_clusters=5):\n",
        "    X = model.wv.vectors\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    return kmeans.inertia_\n",
        "\n",
        "print(\"Downstream (clustering) Word2Vec:\", cluster_quality(w2v_model))\n",
        "\n",
        "# ---------- FastText ----------\n",
        "fasttext_model = FastText(sentences=sentences, vector_size=100, window=5, min_count=3, workers=4, sg=1)\n",
        "fasttext_model.save(\"data/embeddings/fasttext.model\")\n",
        "\n",
        "print(\"Downstream (clustering) FastText:\", cluster_quality(fasttext_model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvy29jCJWbqI"
      },
      "source": [
        "# **4. Tabla Comaprativa de Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4.1 Shallow Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entender los resultados, hay que aclarar los parámetros utilizados para cada variable en shallow learning.\n",
        "\n",
        "En el caso de la variable source, hemos tenido que reducir el número de estimadores del Random Forest de 300 a 150. Además, el número de estimadores del XGBoost también ha sido reducido de 200 a 75. Sin esta reducción, no habriamos sido capaces de terminar la ejecución de la celda. Ha llegado a estar más de una hora y seguía sin terminar de ejecutarse. \n",
        "\n",
        "En cuanto a la variable topic, además de las rebajas aplicadas al caso de la variable source, hemos decidido quitar el modelo XGBoost, ya que no termina de ejecutarse. Disponemos de equipos con capacidades técnicas muy limitadas, por lo que, con mejores ordenadores, no se tendrían que reducir los valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Resultados de Clasificación Shallow Learning (TF-IDF)\n",
            "\n",
            "           Métricas     Bias             Topic            Source         \n",
            "              Model Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1\n",
            "Logistic Regression   0.7021   0.7001   0.5811   0.3390   0.5034   0.1031\n",
            "          LinearSVC   0.6987   0.6966   0.5892   0.4102   0.5591   0.2196\n",
            "      Random Forest   0.6839   0.6798   0.5197   0.2500   0.5000   0.1210\n",
            "            XGBoost   0.7346   0.7339   0.0000   0.0000   0.5326   0.2233\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Resultados de clasificación por variable objetivo\n",
        "results_bias = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.702109, \"Macro-F1\": 0.700091},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.698713, \"Macro-F1\": 0.696551},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.683881, \"Macro-F1\": 0.679829},\n",
        "    \"XGBoost\": {\"Accuracy\": 0.734632, \"Macro-F1\": 0.733918}\n",
        "}\n",
        "\n",
        "results_topic = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.581129, \"Macro-F1\": 0.338987},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.589171, \"Macro-F1\": 0.410156},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.519657, \"Macro-F1\": 0.250003},\n",
        "    \"XGBoost\": {\"Accuracy\": 0.0, \"Macro-F1\": 0.0}\n",
        "}\n",
        "\n",
        "results_source = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.503401, \"Macro-F1\": 0.103117},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.559076, \"Macro-F1\": 0.219572},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.500000, \"Macro-F1\": 0.120979},\n",
        "    \"XGBoost\": {\"Accuracy\": 0.532581, \"Macro-F1\": 0.223251}\n",
        "}\n",
        "\n",
        "# --- Creación del DataFrame de Comparación ---\n",
        "rows = []\n",
        "models = [\"Logistic Regression\", \"LinearSVC\", \"Random Forest\", \"XGBoost\"]\n",
        "\n",
        "for model in models:\n",
        "    row = {\n",
        "        \"Model\": model,\n",
        "        \"Bias Accuracy\": results_bias[model][\"Accuracy\"],\n",
        "        \"Bias Macro-F1\": results_bias[model][\"Macro-F1\"],\n",
        "        \"Topic Accuracy\": results_topic[model][\"Accuracy\"],\n",
        "        \"Topic Macro-F1\": results_topic[model][\"Macro-F1\"],\n",
        "        \"Source Accuracy\": results_source[model][\"Accuracy\"],\n",
        "        \"Source Macro-F1\": results_source[model][\"Macro-F1\"]\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "df_comparison = pd.DataFrame(rows)\n",
        "\n",
        "# --- Formateo para la presentación ---\n",
        "\n",
        "# Redondear todas las columnas de métricas a 4 decimales\n",
        "df_display = df_comparison.round(4)\n",
        "\n",
        "# Crear un multi-índice para los nombres de las columnas para agrupar las métricas\n",
        "cols = [('Métricas', 'Model'), \n",
        "        ('Bias', 'Accuracy'), ('Bias', 'Macro-F1'),\n",
        "        ('Topic', 'Accuracy'), ('Topic', 'Macro-F1'),\n",
        "        ('Source', 'Accuracy'), ('Source', 'Macro-F1')]\n",
        "\n",
        "df_display.columns = pd.MultiIndex.from_tuples(cols)\n",
        "\n",
        "# Imprimir la tabla\n",
        "print(\" Resultados de Clasificación Shallow Learning (TF-IDF)\\n\")\n",
        "# Usar to_markdown o to_string para una salida limpia en consola\n",
        "print(df_display.to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados de clasificación usando Shallow Learning con TF-IDF muestran un comportamiento variable según la tarea y el modelo. Para la tarea de Bias, todos los modelos presentan un desempeño sólido, con XGBoost liderando (accuracy 0.7346, macro-F1 0.7339) seguido de Logistic Regression y LinearSVC alrededor de 0.70 en ambas métricas. Esto indica que las diferencias de sesgo en los textos son relativamente fáciles de capturar con TF-IDF.\n",
        "\n",
        "En la tarea de Topic, los modelos lineales, especialmente LinearSVC, obtienen los mejores resultados (accuracy 0.5892, macro-F1 0.4102). La celda de XGBoost muestra 0, al no haberse aplicado este algoritmo a la tarea por los problemas de tiempo de ejecucion explicados anteriormente. Esto refuerza que, para la clasificación por tópicos, los modelos lineales son adecuados con TF-IDF, que captura bien los términos distintivos de cada tema.\n",
        "\n",
        "En la tarea de Source, el desempeño general es más bajo. LinearSVC alcanza un accuracy de 0.5591 y un macro-F1 de 0.2196, mientras que Logistic Regression y otros modelos muestran valores menores. Esto refleja la dificultad de diferenciar la fuente del texto únicamente con TF-IDF, ya que las diferencias estilísticas son menos evidentes en la representación basada en frecuencias de palabras.\n",
        "\n",
        "En resumen, los modelos lineales ofrecen resultados consistentes para Bias y Topic, mientras que los modelos de ensamble como Random Forest o XGBoost se aplican solo a algunas tareas y muestran fortalezas específicas. TF-IDF es útil para capturar patrones globales de sesgo y tópico, pero es limitado para distinguir la fuente de los textos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4.2 Deep Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Rendimiento de Modelos de Deep Learning (LSTM & GRU)\n",
            "\n",
            "Estrategia de Embedding   Modelo  Accuracy  Macro-F1\n",
            "  Finetuneado (Aprende)     LSTM    0.5134    0.5146\n",
            "  Finetuneado (Aprende)      GRU    0.5264    0.5265\n",
            "No Finetuneado (Random)     LSTM    0.3987    0.3339\n",
            "No Finetuneado (Random)      GRU    0.3858    0.3546\n",
            "      Word2Vec (Frozen) Word2Vec    0.4993    0.4871\n",
            "   Word2Vec (Fine-tune) Word2Vec    0.5023    0.5023\n",
            "     Word2Vec (Scratch) Word2Vec    0.4921    0.4871\n"
          ]
        }
      ],
      "source": [
        "# Datos proporcionados\n",
        "data_finetune = {\n",
        "    \"Modelo\": [\"LSTM\", \"GRU\"],\n",
        "    \"Estrategia de Embedding\": [\"Finetuneado (Aprende)\", \"Finetuneado (Aprende)\"],\n",
        "    \"Accuracy\": [0.513402, 0.526447],\n",
        "    \"Macro-F1\": [0.514574, 0.526548]\n",
        "}\n",
        "\n",
        "data_random = {\n",
        "    \"Modelo\": [\"LSTM\", \"GRU\"],\n",
        "    \"Estrategia de Embedding\": [\"No Finetuneado (Random)\", \"No Finetuneado (Random)\"],\n",
        "    \"Accuracy\": [0.39867762687634023, 0.3858112937812723],\n",
        "    \"Macro-F1\": [0.33385069350207125, 0.3545762899287412]\n",
        "}\n",
        "\n",
        "data_word2vec = {\n",
        "    \"Modelo\": [\"Word2Vec\", \"Word2Vec\", \"Word2Vec\"], # Usamos Word2Vec como modelo en este caso para distinguirlo\n",
        "    \"Estrategia de Embedding\": [\"Word2Vec (Frozen)\", \"Word2Vec (Fine-tune)\", \"Word2Vec (Scratch)\"],\n",
        "    \"Accuracy\": [0.499285, 0.502323, 0.492137],\n",
        "    \"Macro-F1\": [0.487054, 0.502254, 0.487125]\n",
        "}\n",
        "\n",
        "# Crear DataFrames\n",
        "df_finetune = pd.DataFrame(data_finetune)\n",
        "df_random = pd.DataFrame(data_random)\n",
        "df_word2vec = pd.DataFrame(data_word2vec)\n",
        "\n",
        "# Unir todos los DataFrames\n",
        "df_deep_learning = pd.concat([df_finetune, df_random, df_word2vec], ignore_index=True)\n",
        "\n",
        "# Redondear las métricas a 4 decimales para la presentación\n",
        "df_deep_learning_display = df_deep_learning.round(4)\n",
        "\n",
        "# Reordenar las columnas\n",
        "column_order = [\"Estrategia de Embedding\", \"Modelo\", \"Accuracy\", \"Macro-F1\"]\n",
        "df_deep_learning_display = df_deep_learning_display[column_order]\n",
        "\n",
        "# Imprimir la tabla\n",
        "print(\" Rendimiento de Modelos de Deep Learning (LSTM & GRU)\\n\")\n",
        "# Usar to_string para una salida de tabla limpia en consola\n",
        "print(df_deep_learning_display.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los modelos de Deep Learning muestran un comportamiento muy dependiente de la estrategia de embeddings utilizada y del tipo de arquitectura. En primer lugar, los modelos finetuneados, que ajustan los embeddings durante el entrenamiento, obtienen el mejor desempeño general. Tanto LSTM como GRU alcanzan accuracy y macro-F1 superiores a 0.51, con GRU ligeramente por encima de LSTM (0.5264 vs 0.5134 en accuracy). Esto sugiere que permitir que los embeddings aprendan junto con el modelo proporciona representaciones más ajustadas a la tarea y mejora la capacidad de generalización.\n",
        "\n",
        "En contraste, los modelos no finetuneados con embeddings aleatorios presentan un desempeño mucho más bajo, con accuracy alrededor de 0.39-0.40 y macro-F1 entre 0.33 y 0.35. Esto refleja que sin preentrenamiento ni ajuste, los modelos luchan por aprender representaciones significativas desde cero a partir de datos limitados. La ligera diferencia entre LSTM y GRU indica que la arquitectura por sí sola no es suficiente para compensar embeddings iniciales pobres.\n",
        "\n",
        "Cuando se usan embeddings preentrenados tipo Word2Vec, el desempeño se estabiliza entre 0.49 y 0.50 en accuracy. Mantener los embeddings congelados ofrece resultados razonables (0.4993 accuracy), mientras que permitir un finetune ligero mejora apenas a 0.5023, lo que indica que la adaptación incremental no aporta grandes ganancias en este caso. Entrenar embeddings desde cero (scratch) también genera resultados similares (0.4921 accuracy), mostrando que el preentrenamiento ayuda, pero no de manera dramática, frente a una arquitectura LSTM o GRU básica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4.3 Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7iaL0qQWhoh"
      },
      "source": [
        "# **5. Interpretabilidad**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
