{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de empezar con la explicación de esta entrega, debemos mencionar la corrección de dos errores de la entrega anterior. Hemos corregido el fallo de  aplicar stemming y lemmatización  tanto a TF-IDF como a embeddings. Ahora, solo se aplcia a TD-IDF. Además, adaptamos el Word2Vec para que no tuviese más de 30 epochs.\n",
        "\n",
        "Por otro lado, hemos sido capaces de aplicar shallow learning a las tres tareas de clasificación que teníamos previstas. Sin embargo, solo hemos logrado implementar deep learning y la comparación de embeddings a la clasificación de sesgo. Las dos tareas restantes estarán completadas para la siguiente entrega."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfT1kYc_EOn"
      },
      "source": [
        "# **1. Shallow Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Embedding, Input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta versión se ha mejorado la sección de Shallow Learning principalmente en el análisis y la presentación de resultados. Mientras que antes el pipeline solo usaba train y validation, ahora se ha incorporado un split completo en train, evaluation y test, lo que permite aplicar técnicas como early stopping en modelos que lo soportan y evaluar finalmente el desempeño en un conjunto de test separado. Además, se ejecuta explícitamente para las tres tareas (Bias, Topic y Source) y se consolidan los resultados en una tabla comparativa con multi-índice, mostrando métricas de Accuracy y Macro-F1 por tarea. Se ha eliminado la referencia a XGBoost para evitar resultados vacíos, y se han redondeado las métricas para mejorar la legibilidad. Estos cambios permiten visualizar de manera directa el desempeño de cada modelo en todas las tareas, facilitando el análisis comparativo y la interpretación de los resultados. La preparación de los textos y el filtrado de clases se mantiene igual, indicando que las mejoras se centran en el rigor experimental, la consistencia de métricas y la presentación final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para representar los textos, hemos elegido TF-IDF para capturar la importancia relativa de cada palabra en un documento frente al corpus completo, reduciendo el peso de palabras muy frecuentes que no aportan información discriminativa, como artículos y preposiciones en inglés. Hemos establecido un límite de 3,000 palabras más importantes para reducir la dimensionalidad. Además, hemos añadido unigramas y bigramas para capturar algo de contexto local sin sobrecargar el modelo. Por otro lado, hemos eliminado las palabras vacías en inglés para centrar el análisis en palabras significativas. Esta representación genera vectores dispersos que son ideales para los modelos clásicos que usamos.\n",
        "\n",
        "Hemos seleccionado tres modelos para evaluar el desempeño: Logistic Regression, LinearSVC y Random Forest.\n",
        "\n",
        "Antes de entrenar, hemos convertido los textos a números mediante label encoding, y realizado una división de train/test/validation del 70/15/15 para medir el rendimiento real, evitar overfitting y buscar hiperparametros. Además, hemos filtrado las clases con menos de dos registros, ya que la validación estratificada requiere al menos dos ejemplos por clase. La función recibe como parámetro la variable objetivo. En este caso, recibe las variables \"bias\", \"topic\" y \"source\", que son nuestras variables a clasificar.\n",
        "\n",
        "Finalmente, todos los modelos y el vectorizador TF-IDF han sido guardados para su reutilización. Este pipeline de Shallow Learning funciona como un baseline sólido que nos permite medir la mejora que aportan las representaciones densas y contextuales de texto que se utilizarán en las fases posteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shallow_pipeline(df, target_col):\n",
        "    # Preparamos el texto\n",
        "    if \"text_joined\" not in df.columns:\n",
        "        df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "    texts = df[\"text_joined\"].astype(str).tolist()\n",
        "    labels = df[target_col].tolist()\n",
        "\n",
        "    # Filtramos las clases con menos de 2 registros\n",
        "    counts = Counter(labels)\n",
        "    valid_classes = [c for c, cnt in counts.items() if cnt > 1]\n",
        "    mask = [lbl in valid_classes for lbl in labels]\n",
        "    texts = [t for t, m in zip(texts, mask) if m]\n",
        "    labels = [l for l, m in zip(labels, mask) if m]\n",
        "\n",
        "    # Codificamos las etiquetas\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "\n",
        "    # Hacemos el train/validation split\n",
        "    X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "        texts, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Aplicamos TF-IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))\n",
        "    X_train = vectorizer.fit_transform(X_train_text)\n",
        "    X_val = vectorizer.transform(X_val_text)\n",
        "\n",
        "    # Definimos los modelos\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
        "        \"LinearSVC\": LinearSVC(),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=150, n_jobs=-1),\n",
        "       # \"XGBoost\": XGBClassifier(n_estimators=75, eval_metric=\"mlogloss\", tree_method=\"hist\", n_jobs=-1)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Entrenamos, evaluamos y guardamos los modelos\n",
        "    os.makedirs(\"data/models\", exist_ok=True)\n",
        "    for name, model in models.items():\n",
        "        print(f\"Entrenando {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy_score(y_val, y_pred),\n",
        "            \"Macro-F1\": f1_score(y_val, y_pred, average=\"macro\")\n",
        "        }\n",
        "        # Guardamos el modelo\n",
        "        pickle.dump(model, open(f\"data/models/{name.replace(' ', '_').lower()}.pkl\", \"wb\"))\n",
        "\n",
        "    # Guardamos el vectorizador\n",
        "    os.makedirs(\"data/features\", exist_ok=True)\n",
        "    pickle.dump(vectorizer, open(\"data/features/tfidf_vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos tenido que incluir un filtro para eliminar las clases que tenían menos de dos registros antes de hacer el train_test_split. Esto se debe a que tuvimos un error al utilizar el parámetro stratify=y, que requiere al menos dos ejemplos por clase para poder crear correctamente los conjuntos de entrenamiento y validación de manera estratificada. Sin este filtro, cualquier clase con un único ejemplo provocaría que la ejecución se detuviera, como ocurría previamente con la variable source. Al aplicar este filtrado, nos aseguramos de que solo se utilicen clases con suficiente cantidad de datos, garantizando que la partición estratificada funcione y evitando que el pipeline falle durante el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados se analizarán en la sección 4 de este noteebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.702109  0.700091\n",
            "LinearSVC            0.698713  0.696551\n",
            "Random Forest        0.693710  0.690578\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con bias\n",
        "results_bias = shallow_pipeline(df, \"bias\")\n",
        "print(pd.DataFrame(results_bias).T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.581129  0.338987\n",
            "LinearSVC            0.589171  0.410156\n",
            "Random Forest        0.520372  0.244504\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con topic\n",
        "results_topic = shallow_pipeline(df, \"topic\")\n",
        "print(pd.DataFrame(results_topic).T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Logistic Regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando LinearSVC...\n",
            "Entrenando Random Forest...\n",
            "                     Accuracy  Macro-F1\n",
            "Logistic Regression  0.503401  0.103117\n",
            "LinearSVC            0.559076  0.219572\n",
            "Random Forest        0.499642  0.127022\n"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función con source\n",
        "results_source = shallow_pipeline(df, \"source\")\n",
        "print(pd.DataFrame(results_source).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztbnwVJVWSgX"
      },
      "source": [
        "# **2. Modelos Deep**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la parte de Deep Learning, hemos optado por utilizar redes neuronales recurrentes, específicamente LSTM y GRU, debido a su capacidad para capturar dependencias secuenciales en el texto. A diferencia de los modelos de Shallow Learning, que tratan cada palabra o n-grama de manera independiente, las RNNs permiten que la red recuerde información contextual de palabras anteriores en la secuencia, lo cual es crucial para nuestras tareas de clasificación de texto, donde el significado puede depender del orden de las palabras.\n",
        "\n",
        "Para la representación de los textos, hemos empleado embeddings densos, utilizando tres enfoques distintos con Word2Vec: congelado, fine-tune y desde cero. En el caso de los embeddings congelados, utilizamos un modelo preentrenado de Word2Vec y lo fijamos durante el entrenamiento de la red, de manera que solo la LSTM o GRU aprenda a combinar los vectores preexistentes. Esto permite evaluar cuánto conocimiento semántico ya capturado en Word2Vec puede ayudar a la tarea sin modificarlo. En el enfoque de fine-tune, los embeddings inicializados con Word2Vec se ajustan durante el entrenamiento, permitiendo que la red adapte los vectores a las particularidades del dataset específico. Finalmente, la opción de embeddings entrenados desde cero crea vectores aleatorios que se aprenden completamente durante el entrenamiento, lo que permite que la red descubra representaciones óptimas para la tarea, aunque requiere más datos y tiempo de entrenamiento.\n",
        "\n",
        "Hemos elegido LSTM y GRU, ya que cumple nuestra necesidad de comparar dos variantes de redes recurrentes: las LSTM tienen una mayor capacidad para capturar dependencias de largo plazo mediante su mecanismo de puertas, mientras que las GRU son más simples y computacionalmente eficientes, lo que puede acelerar el entrenamiento sin perder demasiado rendimiento.\n",
        "\n",
        "Los textos se transforman primero en secuencias de índices según el vocabulario de Word2Vec o un tokenizer entrenado sobre el dataset, y se aplica padding para unificar la longitud de las secuencias. Esto asegura que las redes puedan procesar lotes de datos de manera eficiente. Finalmente, la capa de salida utiliza softmax para producir probabilidades sobre las clases, y la red se entrena con categorical crossentropy, optimizando la accuracy y el macro-F1 como métricas de desempeño, lo cual es consistente con la evaluación utilizada en la parte de Shallow Learning.\n",
        "\n",
        "En conclusión, este apartado permite que nuestra red aprenda tanto representaciones densas de palabras como patrones secuenciales de las oraciones, ofreciendo una ventaja sobre los modelos lineales y de ensamble de Shallow Learning que solo utilizan información superficial y dispersa de los textos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings fine-tuneados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 1s/step - accuracy: 0.4110 - loss: 1.0727 - val_accuracy: 0.4558 - val_loss: 1.0455\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 1s/step - accuracy: 0.5035 - loss: 0.9876 - val_accuracy: 0.4808 - val_loss: 0.9915\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 960ms/step - accuracy: 0.5876 - loss: 0.8714 - val_accuracy: 0.4961 - val_loss: 0.9985\n",
            "Epoch 4/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 2s/step - accuracy: 0.6633 - loss: 0.7473 - val_accuracy: 0.4892 - val_loss: 1.0340\n",
            "Epoch 5/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 997ms/step - accuracy: 0.7457 - loss: 0.5982 - val_accuracy: 0.5006 - val_loss: 1.1641\n",
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 917ms/step - accuracy: 0.3961 - loss: 1.0813 - val_accuracy: 0.4229 - val_loss: 1.0698\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 1s/step - accuracy: 0.4890 - loss: 1.0051 - val_accuracy: 0.4808 - val_loss: 0.9939\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 679ms/step - accuracy: 0.5864 - loss: 0.8628 - val_accuracy: 0.5130 - val_loss: 0.9604\n",
            "Epoch 4/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 643ms/step - accuracy: 0.6878 - loss: 0.7080 - val_accuracy: 0.5194 - val_loss: 0.9919\n",
            "Epoch 5/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 1s/step - accuracy: 0.7885 - loss: 0.5164 - val_accuracy: 0.5187 - val_loss: 1.1682\n",
            "Epoch 6/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 1s/step - accuracy: 0.8627 - loss: 0.3393 - val_accuracy: 0.5206 - val_loss: 1.3048\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 448ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 374ms/step\n",
            "Resultados finales sobre TEST:\n",
            "      Accuracy  Macro-F1\n",
            "LSTM  0.496783  0.494741\n",
            "GRU   0.527281  0.527386\n"
          ]
        }
      ],
      "source": [
        "#Cargamos el   dataset\n",
        "df_train = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "# Codificamos las labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_cat = to_categorical(y_encoded)\n",
        "\n",
        "# Necesitamos versión entera para stratify\n",
        "y_int = np.argmax(y_cat, axis=1)\n",
        "\n",
        "# Hacemos el Train/Val/Test split\n",
        "X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(\n",
        "    df_train[\"tokens\"], y_cat, test_size=0.3, random_state=42, stratify=y_cat\n",
        ")\n",
        "X_val_texts, X_test_texts, y_val, y_test = train_test_split(\n",
        "    X_temp_texts, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Word2Vec \n",
        "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
        "embedding_dim = w2v_model.vector_size\n",
        "word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1  # +1 para padding\n",
        "\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_train_idx = [tokens_to_indices(t, word_index) for t in X_train_texts]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_texts]\n",
        "X_test_idx = [tokens_to_indices(t, word_index) for t in X_test_texts]\n",
        "\n",
        "max_seq_len = 200\n",
        "X_train_pad = pad_sequences(X_train_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Definimos la RNN\n",
        "def build_rnn(model_type='LSTM'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_len,\n",
        "        trainable=True  \n",
        "    ))\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(y_cat.shape[1], activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hacemos early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Entrenamos y evaluamos los modelos de RNNs\n",
        "# LSTM\n",
        "lstm_model = build_rnn('LSTM')\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# GRU\n",
        "gru_model = build_rnn('GRU')\n",
        "gru_history = gru_model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lstm = np.argmax(lstm_model.predict(X_test_pad, batch_size=64), axis=1)\n",
        "y_pred_gru = np.argmax(gru_model.predict(X_test_pad, batch_size=64), axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Resultados\n",
        "results = {\n",
        "    'LSTM': {\n",
        "        'Accuracy': accuracy_score(y_test_labels, y_pred_lstm),\n",
        "        'Macro-F1': f1_score(y_test_labels, y_pred_lstm, average='macro')\n",
        "    },\n",
        "    'GRU': {\n",
        "        'Accuracy': accuracy_score(y_test_labels, y_pred_gru),\n",
        "        'Macro-F1': f1_score(y_test_labels, y_pred_gru, average='macro')\n",
        "    }\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"Resultados finales sobre TEST:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings no finetuneados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 162ms/step - accuracy: 0.3667 - loss: 1.0933 - val_accuracy: 0.3731 - val_loss: 1.0920\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 158ms/step - accuracy: 0.3812 - loss: 1.0887 - val_accuracy: 0.3695 - val_loss: 1.0884\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 177ms/step - accuracy: 0.3979 - loss: 1.0803 - val_accuracy: 0.3803 - val_loss: 1.0810\n",
            "Epoch 4/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 233ms/step - accuracy: 0.4065 - loss: 1.0729 - val_accuracy: 0.3855 - val_loss: 1.0835\n",
            "Epoch 5/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 250ms/step - accuracy: 0.4189 - loss: 1.0711 - val_accuracy: 0.3807 - val_loss: 1.0841\n",
            "Epoch 6/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 231ms/step - accuracy: 0.4214 - loss: 1.0648 - val_accuracy: 0.3886 - val_loss: 1.0830\n",
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 128ms/step - accuracy: 0.3591 - loss: 1.0938 - val_accuracy: 0.3693 - val_loss: 1.0918\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 195ms/step - accuracy: 0.3735 - loss: 1.0896 - val_accuracy: 0.3695 - val_loss: 1.0921\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 180ms/step - accuracy: 0.3943 - loss: 1.0843 - val_accuracy: 0.3800 - val_loss: 1.0890\n",
            "\n",
            "Resultados Embedding Random (NO fine-tuneado) LSTM:\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.49      0.42      1463\n",
            "           1       0.41      0.27      0.32      1198\n",
            "           2       0.43      0.40      0.41      1536\n",
            "\n",
            "    accuracy                           0.39      4197\n",
            "   macro avg       0.40      0.39      0.38      4197\n",
            "weighted avg       0.40      0.39      0.39      4197\n",
            "\n",
            "\n",
            "Resultados Embedding Random (NO fine-tuneado) GRU:\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.19      0.25      1463\n",
            "           1       0.00      0.00      0.00      1198\n",
            "           2       0.37      0.84      0.52      1536\n",
            "\n",
            "    accuracy                           0.37      4197\n",
            "   macro avg       0.25      0.34      0.26      4197\n",
            "weighted avg       0.27      0.37      0.28      4197\n",
            "\n",
            "\n",
            "Resumen de resultados Embedding Random (NO fine-tuneado):\n",
            "  Model  Accuracy  Macro-F1\n",
            "0  LSTM  0.393614  0.384725\n",
            "1   GRU  0.373600  0.256087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Cargamos el dataset\n",
        "df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "texts = df[\"text_joined\"].astype(str).tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Codificamos los labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(labels)  # entero para Keras\n",
        "\n",
        "# Hacemos el Train/Val/Test split\n",
        "X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(\n",
        "    texts, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")\n",
        "X_val_texts, X_test_texts, y_val, y_test = train_test_split(\n",
        "    X_temp_texts, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Tokenizamos\n",
        "vocab_size = 20000\n",
        "maxlen = 100\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X_train_texts)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val_texts)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "# Definimos los modelos \n",
        "def build_lstm_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "        LSTM(128, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(le.classes_), activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_gru_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "        GRU(128, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(le.classes_), activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Hacemos early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Entrenamos\n",
        "lstm_model = build_lstm_model()\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "gru_model = build_gru_model()\n",
        "history_gru = gru_model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluamos\n",
        "def evaluate(model, X_test, y_test):\n",
        "    preds = np.argmax(model.predict(X_test, batch_size=64), axis=1)\n",
        "    y_test_str = le.inverse_transform(y_test)\n",
        "    preds_str = le.inverse_transform(preds)\n",
        "    print(classification_report(y_test_str, preds_str))\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    f1 = f1_score(y_test, preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "print(\"\\nResultados Embedding Random (NO fine-tuneado) LSTM:\")\n",
        "acc_lstm, f1_lstm = evaluate(lstm_model, X_test_pad, y_test)\n",
        "\n",
        "print(\"\\nResultados Embedding Random (NO fine-tuneado) GRU:\")\n",
        "acc_gru, f1_gru = evaluate(gru_model, X_test_pad, y_test)\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['LSTM', 'GRU'],\n",
        "    'Accuracy': [acc_lstm, acc_gru],\n",
        "    'Macro-F1': [f1_lstm, f1_gru]\n",
        "})\n",
        "print(\"\\nResumen de resultados Embedding Random (NO fine-tuneado):\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word2Vec congelado vs Word2Vec fine-tuneado vs Word2Vec from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 464ms/step - accuracy: 0.4063 - loss: 1.0746 - val_accuracy: 0.4419 - val_loss: 1.0360\n",
            "Epoch 2/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 525ms/step - accuracy: 0.4442 - loss: 1.0400 - val_accuracy: 0.4618 - val_loss: 1.0181\n",
            "Epoch 3/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 489ms/step - accuracy: 0.4417 - loss: 1.0503 - val_accuracy: 0.4366 - val_loss: 1.0464\n",
            "Epoch 4/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 489ms/step - accuracy: 0.4611 - loss: 1.0217 - val_accuracy: 0.4725 - val_loss: 0.9975\n",
            "Epoch 5/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 494ms/step - accuracy: 0.4757 - loss: 1.0001 - val_accuracy: 0.4700 - val_loss: 0.9931\n",
            "Epoch 6/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 485ms/step - accuracy: 0.4924 - loss: 0.9850 - val_accuracy: 0.4748 - val_loss: 0.9890\n",
            "Epoch 7/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 520ms/step - accuracy: 0.4957 - loss: 0.9716 - val_accuracy: 0.4920 - val_loss: 0.9786\n",
            "Epoch 8/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 474ms/step - accuracy: 0.4929 - loss: 0.9800 - val_accuracy: 0.4882 - val_loss: 0.9846\n",
            "Epoch 9/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 485ms/step - accuracy: 0.4912 - loss: 0.9873 - val_accuracy: 0.4712 - val_loss: 0.9890\n",
            "Epoch 10/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 505ms/step - accuracy: 0.5122 - loss: 0.9591 - val_accuracy: 0.4898 - val_loss: 0.9764\n",
            "Epoch 11/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 506ms/step - accuracy: 0.4873 - loss: 0.9803 - val_accuracy: 0.4164 - val_loss: 1.0649\n",
            "Epoch 12/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 491ms/step - accuracy: 0.4520 - loss: 1.0254 - val_accuracy: 0.4769 - val_loss: 0.9877\n",
            "Epoch 13/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 486ms/step - accuracy: 0.5149 - loss: 0.9577 - val_accuracy: 0.5041 - val_loss: 0.9706\n",
            "Epoch 14/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 493ms/step - accuracy: 0.5351 - loss: 0.9304 - val_accuracy: 0.4945 - val_loss: 0.9728\n",
            "Epoch 15/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 494ms/step - accuracy: 0.5451 - loss: 0.9201 - val_accuracy: 0.5109 - val_loss: 0.9619\n",
            "Epoch 16/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 489ms/step - accuracy: 0.5486 - loss: 0.9086 - val_accuracy: 0.5193 - val_loss: 0.9542\n",
            "Epoch 17/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 490ms/step - accuracy: 0.5700 - loss: 0.8915 - val_accuracy: 0.5327 - val_loss: 0.9451\n",
            "Epoch 18/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 464ms/step - accuracy: 0.5763 - loss: 0.8796 - val_accuracy: 0.5388 - val_loss: 0.9342\n",
            "Epoch 19/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 458ms/step - accuracy: 0.5917 - loss: 0.8657 - val_accuracy: 0.5456 - val_loss: 0.9366\n",
            "Epoch 20/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 492ms/step - accuracy: 0.5951 - loss: 0.8493 - val_accuracy: 0.5482 - val_loss: 0.9248\n",
            "Epoch 1/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 571ms/step - accuracy: 0.4109 - loss: 1.0723 - val_accuracy: 0.4326 - val_loss: 1.0536\n",
            "Epoch 2/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 541ms/step - accuracy: 0.4756 - loss: 1.0153 - val_accuracy: 0.4350 - val_loss: 1.0517\n",
            "Epoch 3/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 567ms/step - accuracy: 0.5602 - loss: 0.9185 - val_accuracy: 0.4929 - val_loss: 0.9932\n",
            "Epoch 1/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 538ms/step - accuracy: 0.3780 - loss: 1.0910 - val_accuracy: 0.4060 - val_loss: 1.0723\n",
            "Epoch 2/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 555ms/step - accuracy: 0.4650 - loss: 1.0299 - val_accuracy: 0.4351 - val_loss: 1.0490\n",
            "Epoch 3/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 550ms/step - accuracy: 0.5282 - loss: 0.9605 - val_accuracy: 0.4684 - val_loss: 1.0406\n",
            "\n",
            "Resultados Word2Vec Frozen\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 157ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.48      0.52      1950\n",
            "           1       0.62      0.46      0.53      1598\n",
            "           2       0.51      0.67      0.58      2048\n",
            "\n",
            "    accuracy                           0.55      5596\n",
            "   macro avg       0.56      0.54      0.54      5596\n",
            "weighted avg       0.55      0.55      0.54      5596\n",
            "\n",
            "\n",
            "Resultados Word2Vec Fine-tune\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 154ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.68      0.51      1950\n",
            "           1       0.46      0.27      0.34      1598\n",
            "           2       0.49      0.34      0.40      2048\n",
            "\n",
            "    accuracy                           0.44      5596\n",
            "   macro avg       0.45      0.43      0.42      5596\n",
            "weighted avg       0.45      0.44      0.42      5596\n",
            "\n",
            "\n",
            "Resultados Word2Vec Scratch\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 153ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.35      0.37      1950\n",
            "           1       0.40      0.20      0.27      1598\n",
            "           2       0.41      0.61      0.49      2048\n",
            "\n",
            "    accuracy                           0.40      5596\n",
            "   macro avg       0.40      0.39      0.38      5596\n",
            "weighted avg       0.40      0.40      0.39      5596\n",
            "\n",
            "                    Accuracy  Macro-F1\n",
            "Word2Vec Frozen     0.545390  0.541059\n",
            "Word2Vec Fine-tune  0.438349  0.417913\n",
            "Word2Vec Scratch    0.403860  0.377377\n"
          ]
        }
      ],
      "source": [
        "# Cargamos el dataset\n",
        "texts = df[\"tokens\"].tolist()\n",
        "labels = df[\"bias\"].tolist()\n",
        "\n",
        "# Codificamos las etiquetas\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Hacemos el Train/Val/Test split\n",
        "X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(\n",
        "    df_train[\"tokens\"], y_cat, test_size=0.3, random_state=42, stratify=y_cat\n",
        ")\n",
        "X_val_texts, X_test_texts, y_val, y_test = train_test_split(\n",
        "    X_temp_texts, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Cargamos Word2Vec\n",
        "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
        "embedding_dim = w2v_model.vector_size\n",
        "word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_train_idx = [tokens_to_indices(t, word_index) for t in X_train_text]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
        "X_test_idx = [tokens_to_indices(t, word_index) for t in X_test_text]\n",
        "\n",
        "max_seq_len = 200\n",
        "X_train_pad = pad_sequences(X_train_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Función para construir el modelo LSTM\n",
        "def build_lstm_model(embedding_matrix, trainable=True):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_len,\n",
        "        trainable=trainable\n",
        "    ))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
        "    model.compile(optimizer=Adam(1e-3),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Hacemos early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Entrenamos el modelos\n",
        "# Word2Vec Frozen\n",
        "lstm_frozen = build_lstm_model(embedding_matrix, trainable=False)\n",
        "history_frozen = lstm_frozen.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Word2Vec Fine-tune\n",
        "lstm_finetune = build_lstm_model(embedding_matrix, trainable=True)\n",
        "history_finetune = lstm_finetune.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Word2Vec Scratch\n",
        "embedding_matrix_random = np.random.normal(size=(vocab_size, embedding_dim))\n",
        "lstm_scratch = build_lstm_model(embedding_matrix_random, trainable=True)\n",
        "history_scratch = lstm_scratch.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluamos\n",
        "def evaluate(model, X_test, y_test):\n",
        "    preds = np.argmax(model.predict(X_test, batch_size=64), axis=1)\n",
        "    # Convertimos enteros a nombres de clases para el classification_report\n",
        "    y_test_str = le.inverse_transform(y_test)\n",
        "    preds_str = le.inverse_transform(preds)\n",
        "    print(classification_report(y_test_str, preds_str))\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    f1 = f1_score(y_test, preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"\\nResultados Word2Vec Frozen\")\n",
        "results['Word2Vec Frozen'] = evaluate(lstm_frozen, X_test_pad, y_test)\n",
        "\n",
        "print(\"\\nResultados Word2Vec Fine-tune\")\n",
        "results['Word2Vec Fine-tune'] = evaluate(lstm_finetune, X_test_pad, y_test)\n",
        "\n",
        "print(\"\\nResultados Word2Vec Scratch\")\n",
        "results['Word2Vec Scratch'] = evaluate(lstm_scratch, X_test_pad, y_test)\n",
        "results_df = pd.DataFrame(results, index=['Accuracy','Macro-F1']).T\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Comparación de Embeddings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta sección se consideran distintas técnicas de representación de texto para tareas de clasificación orientadas a la detección de sesgos ideológicos, tema y medio en artículos periodísticos. Para ello se analizan enfoques tradicionales, embeddings no contextuales y embeddings contextuales, con el objetivo de entender cómo cada representación captura información relevante del lenguaje dentro de este dominio específico.\n",
        "\n",
        "Los métodos tradicionales, como TF-IDF y Bag-of-Words (BoW), representan el texto mediante vectores dispersos basados únicamente en la frecuencia o presencia de términos, sin tener en cuenta el word order ni el context. Se espera que estos enfoques funcionen bien cuando ciertas palabras clave o expresiones son indicadores directos de postura ideológica o del tema tratado. Sin embargo, su capacidad para capturar matices ideológicos sutiles, estructuras discursivas o patrones retóricos es limitada debido a la ausencia de información contextual.\n",
        "\n",
        "Los embeddings no contextuales, como Word2Vec y FastText, generan dense word vectors aprendidos a partir de coocurrencias en un corpus. Estos modelos son capaces de capturar similitudes semánticas entre palabras y asociaciones típicas del lenguaje periodístico, lo que ayuda a identificar vocabulario característico de ciertos medios o tendencias ideológicas. Aunque estos embeddings proporcionan una representación más rica que los métodos tradicionales, no distinguen los diferentes significados de una palabra según el contexto en el que aparece. En el caso de FastText, el uso de subword embeddings permite manejar mejor palabras raras, neologismos o términos específicos de determinados medios.\n",
        "\n",
        "Finalmente, los embeddings contextuales, como Sentence Transformers o BERT, generan representaciones que dependen del contexto completo de la oración o del documento. Esto permite que una misma palabra tenga diferentes vectores según su significado en el artículo, capturando relaciones semánticas complejas, long-range dependencies y matices ideológicos implícitos. Se espera que estos modelos sean especialmente efectivos para detectar bias más sutil, diferencias discursivas entre medios y patrones retóricos que dependen del estilo o la narrativa del artículo. No obstante, este tipo de modelos suele requerir un mayor volumen de datos y mayor capacidad computacional para ajustarse correctamente a tareas especializadas como la clasificación ideológica o la identificación del medio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3.1 Embeddings tradicionales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.71      0.71      1950\n",
            "           1       0.71      0.65      0.68      1598\n",
            "           2       0.70      0.74      0.72      2048\n",
            "\n",
            "    accuracy                           0.71      5596\n",
            "   macro avg       0.71      0.70      0.70      5596\n",
            "weighted avg       0.71      0.71      0.70      5596\n",
            "\n",
            "Bag-of-Words - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.62      0.63      1950\n",
            "           1       0.61      0.62      0.61      1598\n",
            "           2       0.65      0.65      0.65      2048\n",
            "\n",
            "    accuracy                           0.63      5596\n",
            "   macro avg       0.63      0.63      0.63      5596\n",
            "weighted avg       0.63      0.63      0.63      5596\n",
            "\n",
            "\n",
            "Comparativa Embeddings Tradicionales:\n",
            "              Val Accuracy  Test Accuracy  Val F1 (weighted)  \\\n",
            "TF-IDF            0.685490       0.705325           0.685151   \n",
            "Bag-of-Words      0.621337       0.633667           0.621458   \n",
            "\n",
            "              Test F1 (weighted)  \n",
            "TF-IDF                  0.704853  \n",
            "Bag-of-Words            0.633737  \n"
          ]
        }
      ],
      "source": [
        "#Cargamos y preparamos los datos\n",
        "texts = df_train[\"text_joined\"].tolist()\n",
        "y = df_train[\"bias\"].values\n",
        "results = {}\n",
        "\n",
        "# Hacemos el Train/Val/Test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    texts, y, test_size=0.15, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp\n",
        ")\n",
        "# TD-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.transform(X_val)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_val_pred = clf.predict(X_val_tfidf)\n",
        "y_test_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "results[\"TF-IDF\"] = {\n",
        "    \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "    \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "    \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "    \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "print(\"TF-IDF - Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Bag-of-Words\n",
        "bow = CountVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "X_train_bow = bow.fit_transform(X_train)\n",
        "X_val_bow = bow.transform(X_val)\n",
        "X_test_bow = bow.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "y_val_pred = clf.predict(X_val_bow)\n",
        "y_test_pred = clf.predict(X_test_bow)\n",
        "\n",
        "results[\"Bag-of-Words\"] = {\n",
        "    \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "    \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "    \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "    \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "print(\"Bag-of-Words - Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Resultados\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nComparativa Embeddings Tradicionales:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3.2 Embeddings no contextuales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.57      0.55      1950\n",
            "           1       0.52      0.43      0.47      1598\n",
            "           2       0.54      0.59      0.57      2048\n",
            "\n",
            "    accuracy                           0.54      5596\n",
            "   macro avg       0.54      0.53      0.53      5596\n",
            "weighted avg       0.54      0.54      0.53      5596\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastText - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.57      0.56      1950\n",
            "           1       0.53      0.44      0.48      1598\n",
            "           2       0.55      0.60      0.58      2048\n",
            "\n",
            "    accuracy                           0.54      5596\n",
            "   macro avg       0.54      0.54      0.54      5596\n",
            "weighted avg       0.54      0.54      0.54      5596\n",
            "\n",
            "\n",
            "Comparativa Embeddings No Contextuales:\n",
            "          Val Accuracy  Test Accuracy  Val F1 (weighted)  Test F1 (weighted)\n",
            "Word2Vec      0.541101       0.536812           0.539087            0.534706\n",
            "FastText      0.543781       0.543960           0.541850            0.541818\n"
          ]
        }
      ],
      "source": [
        "# Preparaamos los tokens\n",
        "sentences = df_train[\"tokens\"].tolist()\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "# Hacemos el Train/Val/Test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    texts, y, test_size=0.15, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp\n",
        ")\n",
        "results = {}\n",
        "\n",
        "# Word2Vec\n",
        "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=3, workers=1, sg=1)\n",
        "w2v_model.save(\"data/embeddings/word2vec.model\")\n",
        "\n",
        "# Weigthed average de los embeddings\n",
        "def get_avg_w2v(sentence, model):\n",
        "    vecs = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if len(vecs) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "X_train_vec = np.array([get_avg_w2v(s, w2v_model) for s in X_train])\n",
        "X_val_vec = np.array([get_avg_w2v(s, w2v_model) for s in X_val])\n",
        "X_test_vec = np.array([get_avg_w2v(s, w2v_model) for s in X_test])\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_vec, y_train)\n",
        "y_val_pred = clf.predict(X_val_vec)\n",
        "y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "results[\"Word2Vec\"] = {\n",
        "    \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "    \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "    \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "    \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "print(\"Word2Vec - Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# FastText\n",
        "fasttext_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=3, workers=1, sg=1)\n",
        "fasttext_model.save(\"data/embeddings/fasttext.model\")\n",
        "\n",
        "X_train_vec = np.array([get_avg_w2v(s, fasttext_model) for s in X_train])\n",
        "X_val_vec = np.array([get_avg_w2v(s, fasttext_model) for s in X_val])\n",
        "X_test_vec = np.array([get_avg_w2v(s, fasttext_model) for s in X_test])\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_vec, y_train)\n",
        "y_val_pred = clf.predict(X_val_vec)\n",
        "y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "results[\"FastText\"] = {\n",
        "    \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "    \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "    \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "    \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "print(\"FastText - Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Resultados\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nComparativa Embeddings No Contextuales:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3.3 Embeddings contextuales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ace6881e7a9743a2a6daaed5eae95404",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/525 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Transformers - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.54      0.53      1950\n",
            "           1       0.54      0.48      0.51      1598\n",
            "           2       0.55      0.57      0.56      2048\n",
            "\n",
            "    accuracy                           0.54      5596\n",
            "   macro avg       0.54      0.53      0.53      5596\n",
            "weighted avg       0.54      0.54      0.54      5596\n",
            "\n",
            "BERT - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.17      0.14         6\n",
            "           1       0.67      0.57      0.62         7\n",
            "           2       0.33      0.29      0.31         7\n",
            "\n",
            "    accuracy                           0.35        20\n",
            "   macro avg       0.38      0.34      0.36        20\n",
            "weighted avg       0.39      0.35      0.37        20\n",
            "\n",
            "\n",
            "Comparativa Embeddings Contextuales:\n",
            "                       Val Accuracy  Test Accuracy  Val F1 (weighted)  \\\n",
            "Sentence Transformers      0.526626        0.53574           0.525582   \n",
            "BERT                       0.350000        0.35000           0.343254   \n",
            "\n",
            "                       Test F1 (weighted)  \n",
            "Sentence Transformers            0.535128  \n",
            "BERT                             0.365934  \n"
          ]
        }
      ],
      "source": [
        "# Preparamos los textos\n",
        "texts = df_train[\"text_joined\"].tolist()\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "# Hacemos el Train/Val/Test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    texts, y, test_size=0.15, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp\n",
        ")\n",
        "results = {}\n",
        "\n",
        "# Sentence Transformers\n",
        "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "X_train_vec = st_model.encode(X_train, batch_size=32, show_progress_bar=True)\n",
        "X_val_vec = st_model.encode(X_val, batch_size=32)\n",
        "X_test_vec = st_model.encode(X_test, batch_size=32)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_vec, y_train)\n",
        "y_val_pred = clf.predict(X_val_vec)\n",
        "y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "results[\"Sentence Transformers\"] = {\n",
        "    \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "    \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "    \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "    \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "print(\"Sentence Transformers - Classification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Bert\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name)\n",
        "bert_model.eval()\n",
        "\n",
        "def bert_sentence_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "        return embeddings.mean(dim=0).numpy()\n",
        "\n",
        "X_train_subset = X_train[:100]\n",
        "X_val_subset = X_val[:20]\n",
        "X_test_subset = X_test[:20]\n",
        "y_train_subset = y_train[:100]\n",
        "y_val_subset = y_val[:20]\n",
        "y_test_subset = y_test[:20]\n",
        "\n",
        "X_train_vec = np.array([bert_sentence_embedding(t) for t in X_train_subset])\n",
        "X_val_vec = np.array([bert_sentence_embedding(t) for t in X_val_subset])\n",
        "X_test_vec = np.array([bert_sentence_embedding(t) for t in X_test_subset])\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_vec, y_train_subset)\n",
        "y_val_pred = clf.predict(X_val_vec)\n",
        "y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "results[\"BERT\"] = {\n",
        "    \"Val Accuracy\": accuracy_score(y_val_subset, y_val_pred),\n",
        "    \"Test Accuracy\": accuracy_score(y_test_subset, y_test_pred),\n",
        "    \"Val F1 (weighted)\": f1_score(y_val_subset, y_val_pred, average=\"weighted\"),\n",
        "    \"Test F1 (weighted)\": f1_score(y_test_subset, y_test_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "print(\"BERT - Classification Report (Test):\")\n",
        "print(classification_report(y_test_subset, y_test_pred))\n",
        "\n",
        "# Resultados\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nComparativa Embeddings Contextuales:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvy29jCJWbqI"
      },
      "source": [
        "# **4. Tabla Comparativa de Resultados**\n",
        "\n",
        "Para la fecha límite de esta entrega solo hemos podido hacer completamente la tarea de la variable bias. Las tareas de topic y source solo tienen presencia en la sección de Shallow Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4.1 Shallow Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entender los resultados, hay que aclarar los parámetros utilizados para cada variable en Shallow Learning.\n",
        "\n",
        "En el caso de la variable source, hemos tenido que reducir el número de estimadores del Random Forest de 300 a 150. Además, el número de estimadores del XGBoost también ha sido reducido de 200 a 75. Sin esta reducción, no habríamos sido capaces de terminar la ejecución de la celda. Ha llegado a estar más de una hora y seguía sin terminar de ejecutarse.\n",
        "\n",
        "En cuanto a la variable topic, además de las rebajas aplicadas al caso de la variable source, hemos decidido quitar el modelo XGBoost, ya que no termina de ejecutarse. Disponemos de equipos con capacidades técnicas muy limitadas, por lo que, con mejores ordenadores, no se tendrían que reducir los valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Resultados de Clasificación Shallow Learning (TF-IDF)\n",
            "\n",
            "           Métricas     Bias             Topic            Source         \n",
            "              Model Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1\n",
            "Logistic Regression   0.7021   0.7001   0.5811   0.3390   0.5034   0.1031\n",
            "          LinearSVC   0.6987   0.6966   0.5892   0.4102   0.5591   0.2196\n",
            "      Random Forest   0.6937   0.6906   0.5204   0.2445   0.4996   0.4996\n"
          ]
        }
      ],
      "source": [
        "# Resultados de clasificación por variable objetivo\n",
        "\n",
        "results_bias = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.702109, \"Macro-F1\": 0.700091},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.698713, \"Macro-F1\": 0.696551},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.693710, \"Macro-F1\": 0.690578}\n",
        "}\n",
        "\n",
        "results_topic = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.581129, \"Macro-F1\": 0.338987},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.589171, \"Macro-F1\": 0.410156},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.520372, \"Macro-F1\": 0.244504}\n",
        "    }\n",
        "\n",
        "\n",
        "results_source = {\n",
        "    \"Logistic Regression\": {\"Accuracy\": 0.503401, \"Macro-F1\": 0.103117},\n",
        "    \"LinearSVC\": {\"Accuracy\": 0.559076, \"Macro-F1\": 0.219572},\n",
        "    \"Random Forest\": {\"Accuracy\": 0.499642, \"Macro-F1\": 0.499642}\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Creación del DataFrame de Comparación ---\n",
        "rows = []\n",
        "models = [\"Logistic Regression\", \"LinearSVC\", \"Random Forest\"]\n",
        "\n",
        "for model in models:\n",
        "    row = {\n",
        "        \"Model\": model,\n",
        "        \"Bias Accuracy\": results_bias[model][\"Accuracy\"],\n",
        "        \"Bias Macro-F1\": results_bias[model][\"Macro-F1\"],\n",
        "        \"Topic Accuracy\": results_topic[model][\"Accuracy\"],\n",
        "        \"Topic Macro-F1\": results_topic[model][\"Macro-F1\"],\n",
        "        \"Source Accuracy\": results_source[model][\"Accuracy\"],\n",
        "        \"Source Macro-F1\": results_source[model][\"Macro-F1\"]\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "df_comparison = pd.DataFrame(rows)\n",
        "\n",
        "# --- Formateo para la presentación ---\n",
        "\n",
        "# Redondear todas las columnas de métricas a 4 decimales\n",
        "df_display = df_comparison.round(4)\n",
        "\n",
        "# Crear un multi-índice para los nombres de las columnas para agrupar las métricas\n",
        "cols = [('Métricas', 'Model'), \n",
        "        ('Bias', 'Accuracy'), ('Bias', 'Macro-F1'),\n",
        "        ('Topic', 'Accuracy'), ('Topic', 'Macro-F1'),\n",
        "        ('Source', 'Accuracy'), ('Source', 'Macro-F1')]\n",
        "\n",
        "df_display.columns = pd.MultiIndex.from_tuples(cols)\n",
        "\n",
        "# Imprimir la tabla\n",
        "print(\" Resultados de Clasificación Shallow Learning (TF-IDF)\\n\")\n",
        "# Usar to_markdown o to_string para una salida limpia en consola\n",
        "print(df_display.to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1- Bias:\n",
        "Todos los modelos presentan un desempeño sólido. Logistic Regression alcanza un accuracy de 0.7021 y un macro-F1 de 0.7001, seguido muy de cerca por LinearSVC, con un accuracy de 0.6987 y macro-F1 0.6966,  y Random Forest, con un accuracy de  0.6937 y  macro-F1 0.6906. Esto indica que las diferencias de sesgo en los textos son relativamente fáciles de capturar mediante TF-IDF, que identifica patrones de palabras relevantes para el sesgo.\n",
        "\n",
        "2- Topic:\n",
        "Los modelos lineales, especialmente LinearSVC, logran los mejores resultados, con un accuracy de  0.5892 y macro-F1 0.4102. Random Forest y Logistic Regression muestran un desempeño menor. Esto refuerza que, para la clasificación por tópicos, los modelos lineales son más adecuados con TF-IDF, que captura eficazmente términos distintivos de cada tema.\n",
        "\n",
        "3- Source:\n",
        "El desempeño general es más bajo. LinearSVC alcanza un accuracy de 0.5591 y un macro-F1 de 0.2196, mientras que Logistic Regression y Random Forest presentan métricas inferiores. Esto refleja la dificultad de diferenciar la fuente del texto únicamente con TF-IDF, ya que las diferencias estilísticas entre fuentes son más sutiles y menos evidentes en representaciones basadas en frecuencias de palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4.2 Deep Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendimiento de Modelos de Deep Learning (LSTM & GRU)\n",
            "\n",
            "Estrategia de Embedding   Modelo  Accuracy  Macro-F1\n",
            "            Finetuneado     LSTM    0.4968    0.4947\n",
            "            Finetuneado      GRU    0.5273    0.5274\n",
            "         No Finetuneado     LSTM    0.3936    0.3736\n",
            "         No Finetuneado      GRU    0.3847    0.2561\n",
            "      Word2Vec (Frozen) Word2Vec    0.5454    0.5411\n",
            "   Word2Vec (Fine-tune) Word2Vec    0.4383    0.4179\n",
            "     Word2Vec (Scratch) Word2Vec    0.4039    0.3774\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data_finetune = {\n",
        "    \"Modelo\": [\"LSTM\", \"GRU\"],\n",
        "    \"Estrategia de Embedding\": [\"Finetuneado\", \"Finetuneado\"],\n",
        "    \"Accuracy\": [0.496783, 0.527281],\n",
        "    \"Macro-F1\": [0.494741, 0.527386]\n",
        "}\n",
        "\n",
        "data_random = {\n",
        "    \"Modelo\": [\"LSTM\", \"GRU\"],\n",
        "    \"Estrategia de Embedding\": [\"No Finetuneado\", \"No Finetuneado\"],\n",
        "    \"Accuracy\": [0.393614, 0.384725],\n",
        "    \"Macro-F1\": [0.373600, 0.256087]\n",
        "}\n",
        "\n",
        "data_word2vec = {\n",
        "    \"Modelo\": [\"Word2Vec\", \"Word2Vec\", \"Word2Vec\"],\n",
        "    \"Estrategia de Embedding\": [\n",
        "        \"Word2Vec (Frozen)\",\n",
        "        \"Word2Vec (Fine-tune)\",\n",
        "        \"Word2Vec (Scratch)\"\n",
        "    ],\n",
        "    \"Accuracy\": [0.545390, 0.438349, 0.403860],\n",
        "    \"Macro-F1\": [0.541059, 0.417913, 0.377377]\n",
        "}\n",
        "\n",
        "# Crear DataFrames\n",
        "df_finetune = pd.DataFrame(data_finetune)\n",
        "df_random = pd.DataFrame(data_random)\n",
        "df_word2vec = pd.DataFrame(data_word2vec)\n",
        "\n",
        "# Unir todos los DataFrames\n",
        "df_deep_learning = pd.concat(\n",
        "    [df_finetune, df_random, df_word2vec],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Redondear métricas para presentación\n",
        "df_deep_learning_display = df_deep_learning.round(4)\n",
        "\n",
        "# Reordenar columnas\n",
        "column_order = [\"Estrategia de Embedding\", \"Modelo\", \"Accuracy\", \"Macro-F1\"]\n",
        "df_deep_learning_display = df_deep_learning_display[column_order]\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Rendimiento de Modelos de Deep Learning (LSTM & GRU)\\n\")\n",
        "print(df_deep_learning_display.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings finetuneados:\n",
        "\n",
        "Cuando los embeddings aprenden conjuntamente con el modelo, GRU supera ligeramente a LSTM, alcanzando un accuracy de 0.5273 y un macro-F1 de 0.5274. LSTM obtiene valores algo inferiores con un accuracy de 0.4968. Esta diferencia puede explicarse por la estructura más simple de GRU, que suele generalizar mejor en escenarios con datasets de tamaño moderado.\n",
        "\n",
        "Embeddings no finetuneado: \n",
        "\n",
        "Ambos modelos sufren una caída notable de rendimiento, especialmente GRU, cuyo macro-F1 desciende hasta 0.2561. Esto indica una incapacidad del modelo para aprender representaciones semánticas útiles a partir de embeddings no informativos.\n",
        "\n",
        "Word2Vec:\n",
        "\n",
        "El mejor rendimiento global se obtiene con Word2Vec congelado, alcanzando un accuracy de 0.5454 y un macro-F1 de 0.5411. Esto sugiere que los embeddings preentrenados capturan información semántica relevante, que el modelo puede explotar eficazmente sin necesidad de reajustarlos.\n",
        "\n",
        "\n",
        "\n",
        "En conjunto, los resultados confirman que la estrategia de embedding es el factor clave en el rendimiento. Ademas, los embeddings preentrenados y congelados Word2Vec Frozen superan a las alternativas entrenadas desde cero. Por ultimo, GRU tiende a comportarse ligeramente mejor que LSTM cuando los embeddings se aprenden.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4.3 Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados Consolidados de Modelos de Representación de Texto (Test Set)\n",
            "\n",
            "Tipo de Embedding        Modelo/Técnica  Accuracy  Macro-F1\n",
            "      Tradicional                TF-IDF    0.7100    0.7001\n",
            "      Tradicional    Bag-of-Words (BoW)    0.6300    0.6337\n",
            "    No Contextual              Word2Vec    0.5368    0.5347\n",
            "    No Contextual              FastText    0.5440    0.5418\n",
            "       Contextual Sentence Transformers    0.5351    0.5351\n",
            "       Contextual                  BERT    0.3659    0.3659\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Datos nuevos ---\n",
        "\n",
        "# Tradicionales (Shallow Learning)\n",
        "results_traditional = {\n",
        "    \"Modelo/Técnica\": [\"TF-IDF\", \"Bag-of-Words (BoW)\"],\n",
        "    \"Test Accuracy\": [0.7100, 0.6300],\n",
        "    \"Test F1 (weighted)\": [0.7001, 0.6337]  # macro-F1 ponderado\n",
        "}\n",
        "\n",
        "# No contextuales (Word2Vec / FastText)\n",
        "results_non_contextual = {\n",
        "    \"Modelo/Técnica\": [\"Word2Vec\", \"FastText\"],\n",
        "    \"Test Accuracy\": [0.5368, 0.5440],\n",
        "    \"Test F1 (weighted)\": [0.5347, 0.5418]\n",
        "}\n",
        "\n",
        "# Contextuales (Sentence Transformers / BERT)\n",
        "results_contextual = {\n",
        "    \"Modelo/Técnica\": [\"Sentence Transformers\", \"BERT\"],\n",
        "    \"Test Accuracy\": [0.5351, 0.3659],\n",
        "    \"Test F1 (weighted)\": [0.5351, 0.3659]\n",
        "}\n",
        "\n",
        "# --- Crear DataFrames ---\n",
        "df_traditional = pd.DataFrame(results_traditional).assign(Tipo_Embedding=\"Tradicional\")\n",
        "df_non_contextual = pd.DataFrame(results_non_contextual).assign(Tipo_Embedding=\"No Contextual\")\n",
        "df_contextual = pd.DataFrame(results_contextual).assign(Tipo_Embedding=\"Contextual\")\n",
        "\n",
        "# --- Concatenar todos los DataFrames ---\n",
        "df_results = pd.concat([df_traditional, df_non_contextual, df_contextual], ignore_index=True)\n",
        "\n",
        "# --- Ordenar columnas y renombrar ---\n",
        "df_results = df_results[[\"Tipo_Embedding\", \"Modelo/Técnica\", \"Test Accuracy\", \"Test F1 (weighted)\"]]\n",
        "df_results.rename(columns={\"Tipo_Embedding\": \"Tipo de Embedding\",\n",
        "                           \"Test Accuracy\": \"Accuracy\",\n",
        "                           \"Test F1 (weighted)\": \"Macro-F1\"}, inplace=True)\n",
        "\n",
        "# Redondear métricas\n",
        "df_results[[\"Accuracy\", \"Macro-F1\"]] = df_results[[\"Accuracy\", \"Macro-F1\"]].round(4)\n",
        "\n",
        "# --- Mostrar tabla ---\n",
        "print(\"Resultados Consolidados de Modelos de Representación de Texto (Test Set)\\n\")\n",
        "print(df_results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings tradicionales: \n",
        "\n",
        "Son los que obtienen el mejor rendimiento en Accuracy y Macro-F1, especialmente TF-IDF,  con0.71 y 0.70 respectivamente.Esto indica que para esta tarea, las representaciones basadas en frecuencia de términos todavía capturan bien los patrones discriminativos del texto, particularmente para la variable objetivo \"Bias\".\n",
        "\n",
        "Por otro lado, Bag-of-Words rinde un poco peor que TF-IDF, lo que es esperado ya que TF-IDF pondera la importancia de los términos, ayudando a resaltar palabras clave.\n",
        "\n",
        "Embeddings no contextuales: \n",
        "\n",
        "Este tipo de embeddings obtienen resultados moderados, con 0.54–0.544 de Accuracy y  0.53–0.54 de Macro-F1. Aunque capturan relaciones semánticas entre palabras, al usarlos con Logistic Regression sobre un downstream classification, no superan a TF-IDF.\n",
        "\n",
        "Esto sugiere que la información semántica que Word2Vec o FastText aportan no es tan crítica para la tarea como la presencia o frecuencia de términos concretos.\n",
        "\n",
        "Embeddings contextuales:\n",
        "\n",
        "Sentence Transformers se comporta similar a los embeddings no contextuales ~0.535. BERT rinde mucho peor con un 0.366 de Accuracy y 0.366 de Macro-F1. Esto probablemente sea  a causa de la reduccion en la cantidad de epochs por limitaciones de tiempo y recursos computacionales.\n",
        "\n",
        "\n",
        "En conclusion, TF-IDF sigue siendo la representación más efectiva. Los embeddings Word2Vec/FastText y Sentence Transformers son útiles pero no superan a las técnicas basadas en frecuencia de términos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Continuación de las partes pendientes de las tarea topic y source**\n",
        "\n",
        "Como indica el título, de este punto en adelante se tratan las partes de Deep Learning y Comparación de Embeddings para las tareas de topic y source. Ya que en la E3 no nos dio tiempo, lo hacemos para la entrega 4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la variable source hemos intentado replicar exactamente el mismo pipeline utilizado para bias y topic.\n",
        "\n",
        "Sin embargo, esta variable presenta una distribución extremadamente desbalanceada, con un número elevado de clases que contienen muy pocos ejemplos. Aunque se filtramos inicialmente las clases con menos de dos instancias, durante la división estratificada en conjuntos de entrenamiento, validación y test, varias clases quedaban representadas por una única muestra en los subconjuntos de validación o test.\n",
        "\n",
        "Dado que la estratificación es un requisito fundamental para garantizar evaluaciones fiables y reproducibles, y que forzar la eliminación de esta restricción introduciría sesgos importantes, hemos decidido no continuar con el entrenamiento de modelos RNN para la variable source.\n",
        "\n",
        "Este resultado pone de manifiesto una limitación estructural del dataset para esta variable y sugiere que source requeriría enfoques alternativos, asi como agrupación de clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Embeddings fine-tuneados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos una funcion para poder llamarla con la variable que queramos predecir\n",
        "def run_rnn_pipeline(\n",
        "    df,\n",
        "    target_col,\n",
        "    w2v_path=\"data/embeddings/word2vec.model\",\n",
        "    max_seq_len=200,\n",
        "    batch_size=64,\n",
        "    epochs=20\n",
        "):\n",
        "    print(f\"\\n\")\n",
        "    print(f\"  Variable objetivo: {target_col}\")\n",
        "    print(f\"\\n\")\n",
        "\n",
        "    # Filtrado de clases con al menos 2 muestras\n",
        "    counts = df[target_col].value_counts()\n",
        "    valid_classes = counts[counts >= 2].index\n",
        "    df_filtered = df[df[target_col].isin(valid_classes)].reset_index(drop=True)\n",
        "\n",
        "    print(f\"Clases originales: {df[target_col].nunique()}\")\n",
        "    print(f\"Clases tras filtrado: {df_filtered[target_col].nunique()}\")\n",
        "\n",
        "    # Codificamos las etiquetas\n",
        "    y = df_filtered[target_col].values\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_int = le.fit_transform(y)           # etiquetas 1D para stratify\n",
        "    y_cat = to_categorical(y_int)         # etiquetas one-hot para el modelo\n",
        "\n",
        "    # Hacemos el Train/Val/Test split\n",
        "    X_train_texts, X_temp_texts, y_train, y_temp, y_train_int, y_temp_int = train_test_split(\n",
        "        df_filtered[\"tokens\"],\n",
        "        y_cat,\n",
        "        y_int,\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=y_int\n",
        "    )\n",
        "\n",
        "    X_val_texts, X_test_texts, y_val, y_test, y_val_int, y_test_int = train_test_split(\n",
        "        X_temp_texts,\n",
        "        y_temp,\n",
        "        y_temp_int,\n",
        "        test_size=0.5,\n",
        "        random_state=42,\n",
        "        stratify=y_temp_int\n",
        "    )\n",
        "\n",
        "    # Word2Vec\n",
        "    w2v_model = Word2Vec.load(w2v_path)\n",
        "    embedding_dim = w2v_model.vector_size\n",
        "\n",
        "    word_index = {word: i + 1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "    vocab_size = len(word_index) + 1  \n",
        "\n",
        "    def tokens_to_indices(tokens):\n",
        "        return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "    X_train_idx = [tokens_to_indices(t) for t in X_train_texts]\n",
        "    X_val_idx   = [tokens_to_indices(t) for t in X_val_texts]\n",
        "    X_test_idx  = [tokens_to_indices(t) for t in X_test_texts]\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train_idx, maxlen=max_seq_len, padding=\"post\")\n",
        "    X_val_pad   = pad_sequences(X_val_idx,   maxlen=max_seq_len, padding=\"post\")\n",
        "    X_test_pad  = pad_sequences(X_test_idx,  maxlen=max_seq_len, padding=\"post\")\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "    # Definimos la RNN\n",
        "    def build_rnn(cell=\"LSTM\"):\n",
        "        model = Sequential()\n",
        "        model.add(\n",
        "            Embedding(\n",
        "                input_dim=vocab_size,\n",
        "                output_dim=embedding_dim,\n",
        "                weights=[embedding_matrix],\n",
        "                input_length=max_seq_len,\n",
        "                trainable=True  # fine-tuning Word2Vec\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if cell == \"LSTM\":\n",
        "            model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "        elif cell == \"GRU\":\n",
        "            model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "        model.add(Dense(y_cat.shape[1], activation=\"softmax\"))\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=1e-3),\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    # EarlyStopping\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Entrenamos los modelos\n",
        "    lstm_model = build_rnn(\"LSTM\")\n",
        "    lstm_model.fit(\n",
        "        X_train_pad,\n",
        "        y_train,\n",
        "        validation_data=(X_val_pad, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    gru_model = build_rnn(\"GRU\")\n",
        "    gru_model.fit(\n",
        "        X_train_pad,\n",
        "        y_train,\n",
        "        validation_data=(X_val_pad, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluamos\n",
        "    y_pred_lstm = np.argmax(lstm_model.predict(X_test_pad), axis=1)\n",
        "    y_pred_gru  = np.argmax(gru_model.predict(X_test_pad), axis=1)\n",
        "\n",
        "    y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    results = {\n",
        "        \"LSTM\": {\n",
        "            \"Accuracy\": accuracy_score(y_test_labels, y_pred_lstm),\n",
        "            \"Macro-F1\": f1_score(y_test_labels, y_pred_lstm, average=\"macro\"),\n",
        "        },\n",
        "        \"GRU\": {\n",
        "            \"Accuracy\": accuracy_score(y_test_labels, y_pred_gru),\n",
        "            \"Macro-F1\": f1_score(y_test_labels, y_pred_gru, average=\"macro\"),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    print(\"\\nResultados finales:\")\n",
        "    print(results_df)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Procesando columna: topic =====\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 283ms/step - accuracy: 0.1712 - loss: 3.7774 - val_accuracy: 0.2362 - val_loss: 3.4463\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 304ms/step - accuracy: 0.2408 - loss: 3.2817 - val_accuracy: 0.2924 - val_loss: 2.9537\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 479ms/step - accuracy: 0.3315 - loss: 2.7591 - val_accuracy: 0.3861 - val_loss: 2.5254\n",
            "Epoch 4/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 587ms/step - accuracy: 0.4025 - loss: 2.4214 - val_accuracy: 0.4185 - val_loss: 2.3445\n",
            "Epoch 5/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 365ms/step - accuracy: 0.4471 - loss: 2.2077 - val_accuracy: 0.4402 - val_loss: 2.2047\n",
            "Epoch 6/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 387ms/step - accuracy: 0.4844 - loss: 1.9966 - val_accuracy: 0.4664 - val_loss: 2.1131\n",
            "Epoch 7/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 267ms/step - accuracy: 0.5208 - loss: 1.8264 - val_accuracy: 0.4685 - val_loss: 2.0712\n",
            "Epoch 8/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 283ms/step - accuracy: 0.5631 - loss: 1.6484 - val_accuracy: 0.4764 - val_loss: 2.0477\n",
            "Epoch 9/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 238ms/step - accuracy: 0.6070 - loss: 1.4697 - val_accuracy: 0.4721 - val_loss: 2.0765\n",
            "Epoch 10/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 338ms/step - accuracy: 0.6531 - loss: 1.2892 - val_accuracy: 0.4774 - val_loss: 2.1200\n",
            "Epoch 11/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 324ms/step - accuracy: 0.7015 - loss: 1.1085 - val_accuracy: 0.4700 - val_loss: 2.2094\n",
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 247ms/step - accuracy: 0.1944 - loss: 3.6295 - val_accuracy: 0.3124 - val_loss: 3.0126\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 239ms/step - accuracy: 0.3711 - loss: 2.6311 - val_accuracy: 0.4295 - val_loss: 2.3352\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 248ms/step - accuracy: 0.4557 - loss: 2.1772 - val_accuracy: 0.4697 - val_loss: 2.1080\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step\n",
            "\n",
            "===== Resultados finales para topic =====\n",
            "      Accuracy  Macro-F1\n",
            "LSTM  0.486061  0.239902\n",
            "GRU   0.309030  0.040144\n"
          ]
        }
      ],
      "source": [
        "results_topic = run_rnn_pipeline(df_train, \"topic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source\n",
        "results_source = run_rnn_pipeline(df_train, \"source\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Embeddings no fine-tuneados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos una función para ejecutar con la variable objetivo que queramos\n",
        "def run_random_embedding_experiment(df, target_col, vocab_size=20000, maxlen=100,\n",
        "                                    embedding_dim=100, batch_size=64, epochs=20,\n",
        "                                    min_samples_per_class=5):\n",
        "    print(f\"\\n===== Embeddings no fine-tuneados | Target: {target_col} =====\\n\")\n",
        "    \n",
        "    # Filtramos las clases raras\n",
        "    counts = df[target_col].value_counts()\n",
        "    valid_classes = counts[counts >= min_samples_per_class].index\n",
        "    df_filtered = df[df[target_col].isin(valid_classes)].copy()\n",
        "    \n",
        "    if df_filtered.empty:\n",
        "        print(f\"No hay suficientes datos para {target_col} después de filtrar clases raras.\")\n",
        "        return None\n",
        "\n",
        "    # Preparamos los textos y etiquetas\n",
        "    df_filtered[\"text_joined\"] = df_filtered[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "    texts = df_filtered[\"text_joined\"].astype(str).tolist()\n",
        "    labels = df_filtered[target_col].tolist()\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(labels)\n",
        "\n",
        "    # Hacemos el Train/Val/Test split\n",
        "    X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(\n",
        "        texts, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "    \n",
        "    X_val_texts, X_test_texts, y_val, y_test = train_test_split(\n",
        "        X_temp_texts, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    # Tokenizamos\n",
        "    tokenizer = Tokenizer(num_words=vocab_size)\n",
        "    tokenizer.fit_on_texts(X_train_texts)\n",
        "\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val_texts)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "    X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen)\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "    # Definimos los modelos\n",
        "    def build_lstm_model():\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "            LSTM(128, return_sequences=False),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(len(le.classes_), activation='softmax')\n",
        "        ])\n",
        "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def build_gru_model():\n",
        "        model = Sequential([\n",
        "            Embedding(vocab_size, embedding_dim, input_length=maxlen, trainable=False),\n",
        "            GRU(128, return_sequences=False),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(len(le.classes_), activation='softmax')\n",
        "        ])\n",
        "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    # Hacemos early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Entrenamos los modelos\n",
        "    lstm_model = build_lstm_model()\n",
        "    lstm_model.fit(X_train_pad, y_train,\n",
        "                   validation_data=(X_val_pad, y_val),\n",
        "                   epochs=epochs,\n",
        "                   batch_size=batch_size,\n",
        "                   callbacks=[early_stopping])\n",
        "\n",
        "    gru_model = build_gru_model()\n",
        "    gru_model.fit(X_train_pad, y_train,\n",
        "                  validation_data=(X_val_pad, y_val),\n",
        "                  epochs=epochs,\n",
        "                  batch_size=batch_size,\n",
        "                  callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluamos\n",
        "    def evaluate(model, X_test, y_test):\n",
        "        preds = np.argmax(model.predict(X_test, batch_size=batch_size), axis=1)\n",
        "        y_test_str = le.inverse_transform(y_test)\n",
        "        preds_str = le.inverse_transform(preds)\n",
        "        print(classification_report(y_test_str, preds_str))\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        f1 = f1_score(y_test, preds, average='macro')\n",
        "        return acc, f1\n",
        "\n",
        "    print(\"\\nResultados LSTM:\")\n",
        "    acc_lstm, f1_lstm = evaluate(lstm_model, X_test_pad, y_test)\n",
        "\n",
        "    print(\"\\nResultados GRU:\")\n",
        "    acc_gru, f1_gru = evaluate(gru_model, X_test_pad, y_test)\n",
        "    results_df = pd.DataFrame({\n",
        "        \"Model\": [\"LSTM\", \"GRU\"],\n",
        "        \"Accuracy\": [acc_lstm, acc_gru],\n",
        "        \"Macro-F1\": [f1_lstm, f1_gru]\n",
        "    })\n",
        "\n",
        "    print(\"\\nResumen de resultados:\")\n",
        "    print(results_df)\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Embeddings no fine-tuneados | Target: topic =====\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 122ms/step - accuracy: 0.1583 - loss: 3.9671 - val_accuracy: 0.1616 - val_loss: 3.8406\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 118ms/step - accuracy: 0.1614 - loss: 3.8738 - val_accuracy: 0.1616 - val_loss: 3.8367\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.1615 - loss: 3.8530 - val_accuracy: 0.1616 - val_loss: 3.8230\n",
            "Epoch 4/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.1616 - loss: 3.8376 - val_accuracy: 0.1616 - val_loss: 3.8148\n",
            "Epoch 5/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 114ms/step - accuracy: 0.1624 - loss: 3.7929 - val_accuracy: 0.1633 - val_loss: 3.7652\n",
            "Epoch 6/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.1641 - loss: 3.7494 - val_accuracy: 0.1714 - val_loss: 3.7397\n",
            "Epoch 7/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 115ms/step - accuracy: 0.1685 - loss: 3.7146 - val_accuracy: 0.1754 - val_loss: 3.6712\n",
            "Epoch 8/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 135ms/step - accuracy: 0.1759 - loss: 3.6646 - val_accuracy: 0.1709 - val_loss: 3.6577\n",
            "Epoch 9/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 126ms/step - accuracy: 0.1778 - loss: 3.6264 - val_accuracy: 0.1873 - val_loss: 3.6568\n",
            "Epoch 10/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 128ms/step - accuracy: 0.1861 - loss: 3.5937 - val_accuracy: 0.1880 - val_loss: 3.6112\n",
            "Epoch 11/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 132ms/step - accuracy: 0.1924 - loss: 3.5582 - val_accuracy: 0.2011 - val_loss: 3.5592\n",
            "Epoch 12/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 123ms/step - accuracy: 0.2003 - loss: 3.5351 - val_accuracy: 0.1973 - val_loss: 3.5403\n",
            "Epoch 13/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 211ms/step - accuracy: 0.2026 - loss: 3.5127 - val_accuracy: 0.2164 - val_loss: 3.4873\n",
            "Epoch 14/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 223ms/step - accuracy: 0.2109 - loss: 3.4820 - val_accuracy: 0.2071 - val_loss: 3.5113\n",
            "Epoch 15/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 205ms/step - accuracy: 0.2144 - loss: 3.4689 - val_accuracy: 0.2183 - val_loss: 3.4669\n",
            "Epoch 16/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 188ms/step - accuracy: 0.2185 - loss: 3.4381 - val_accuracy: 0.2064 - val_loss: 3.5065\n",
            "Epoch 17/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 203ms/step - accuracy: 0.2201 - loss: 3.4235 - val_accuracy: 0.2295 - val_loss: 3.4115\n",
            "Epoch 18/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 189ms/step - accuracy: 0.2244 - loss: 3.3975 - val_accuracy: 0.2390 - val_loss: 3.3610\n",
            "Epoch 19/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 212ms/step - accuracy: 0.2333 - loss: 3.3551 - val_accuracy: 0.2324 - val_loss: 3.3738\n",
            "Epoch 20/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 197ms/step - accuracy: 0.2353 - loss: 3.3214 - val_accuracy: 0.2340 - val_loss: 3.3604\n",
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 197ms/step - accuracy: 0.1537 - loss: 3.9678 - val_accuracy: 0.1616 - val_loss: 3.8328\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 189ms/step - accuracy: 0.1610 - loss: 3.8637 - val_accuracy: 0.1616 - val_loss: 3.8331\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 190ms/step - accuracy: 0.1615 - loss: 3.8376 - val_accuracy: 0.1616 - val_loss: 3.7875\n",
            "\n",
            "Resultados LSTM:\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 84ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abortion       0.00      0.00      0.00        40\n",
            "                              africa       0.00      0.00      0.00         1\n",
            "                         agriculture       0.00      0.00      0.00         3\n",
            "                      animal_welfare       0.00      0.00      0.00         2\n",
            "              arts_and_entertainment       0.00      0.00      0.00         6\n",
            "                                asia       0.00      0.00      0.00         8\n",
            "                 banking_and_finance       0.00      0.00      0.00        24\n",
            "                    bridging_divides       0.00      0.00      0.00         9\n",
            "                            business       0.00      0.00      0.00        10\n",
            "                    campaign_finance       0.00      0.00      0.00        22\n",
            "                   campaign_rhetoric       0.00      0.00      0.00         5\n",
            "capital_punishment_and_death_penalty       0.00      0.00      0.00         1\n",
            "                               china       0.00      0.00      0.00        17\n",
            "                                 cia       0.00      0.00      0.00         7\n",
            "                        civil_rights       0.00      0.00      0.00        19\n",
            "                         coronavirus       0.07      0.51      0.13       120\n",
            "                    criminal_justice       0.00      0.00      0.00        13\n",
            "                             culture       0.00      0.00      0.00        35\n",
            "                       cybersecurity       0.00      0.00      0.00        11\n",
            "                       death_penalty       0.00      0.00      0.00         2\n",
            "                             defense       0.00      0.00      0.00        25\n",
            "                    democratic_party       0.00      0.00      0.00        25\n",
            "                            disaster       0.00      0.00      0.00        13\n",
            "                     domestic_policy       0.00      0.00      0.00         2\n",
            "                               ebola       0.00      0.00      0.00        10\n",
            "                     economic_policy       0.00      0.00      0.00        24\n",
            "                    economy_and_jobs       0.25      0.10      0.15        78\n",
            "                           education       0.00      0.00      0.00        48\n",
            "                           elections       0.34      0.77      0.47       678\n",
            "                              energy       0.00      0.00      0.00         9\n",
            "                         environment       0.00      0.00      0.00        66\n",
            "                                 epa       0.00      0.00      0.00         5\n",
            "                              europe       0.00      0.00      0.00        15\n",
            "             facts_and_fact_checking       0.00      0.00      0.00         9\n",
            "                           fake_news       0.00      0.00      0.00         8\n",
            "                 family_and_marriage       0.00      0.00      0.00         1\n",
            "                                 fbi       0.00      0.00      0.00        36\n",
            "                                 fda       0.00      0.00      0.00         1\n",
            "                      federal_budget       0.21      0.14      0.17        72\n",
            "                        fiscal_cliff       0.00      0.00      0.00        21\n",
            "                                food       0.00      0.00      0.00         2\n",
            "                      foreign_policy       0.00      0.00      0.00        60\n",
            "                         free_speech       0.00      0.00      0.00        30\n",
            "                        general_news       0.00      0.00      0.00        34\n",
            "                       great_britain       0.00      0.00      0.00        19\n",
            "          gun_control_and_gun_rights       0.71      0.57      0.63        97\n",
            "                          healthcare       0.35      0.58      0.44       163\n",
            "                            holidays       0.00      0.00      0.00        17\n",
            "                   homeland_security       0.00      0.00      0.00        14\n",
            "            housing_and_homelessness       0.00      0.00      0.00         3\n",
            "                         immigration       0.14      0.44      0.21       186\n",
            "                         impeachment       0.00      0.00      0.00        47\n",
            "                          inequality       0.00      0.00      0.00         4\n",
            "                                isis       0.00      0.00      0.00        19\n",
            "                              israel       0.00      0.00      0.00        14\n",
            "                             justice       0.00      0.00      0.00        35\n",
            "                  justice_department       0.00      0.00      0.00        32\n",
            "                               labor       0.00      0.00      0.00        15\n",
            "                         lgbt_rights       0.00      0.00      0.00        33\n",
            "              marijuana_legalization       0.00      0.00      0.00        12\n",
            "                          media_bias       0.14      0.11      0.13       124\n",
            "                      media_industry       0.00      0.00      0.00         4\n",
            "                              mexico       0.00      0.00      0.00        11\n",
            "                         middle_east       0.22      0.22      0.22       105\n",
            "                    national_defense       0.00      0.00      0.00        16\n",
            "                   national_security       0.00      0.00      0.00        62\n",
            "                         north_korea       0.45      0.69      0.55        36\n",
            "                                 nsa       0.00      0.00      0.00        28\n",
            "                     nuclear_weapons       0.00      0.00      0.00         4\n",
            "            obesity_and_malnutrition       0.00      0.00      0.00         1\n",
            "                       opioid_crisis       0.00      0.00      0.00         4\n",
            "                           palestine       0.00      0.00      0.00         4\n",
            "                        polarization       0.00      0.00      0.00        50\n",
            "                            politics       0.11      0.01      0.01       254\n",
            "                             privacy       0.00      0.00      0.00         6\n",
            "                       public_health       0.00      0.00      0.00         2\n",
            "                     race_and_racism       0.00      0.00      0.00        50\n",
            "                  religion_and_faith       0.00      0.00      0.00        24\n",
            "                    republican_party       0.00      0.00      0.00        63\n",
            "                  role_of_government       0.00      0.00      0.00         7\n",
            "                              russia       0.00      0.00      0.00        23\n",
            "                             science       0.00      0.00      0.00         3\n",
            "                   sexual_misconduct       0.00      0.00      0.00        16\n",
            "                     social_security       0.00      0.00      0.00         2\n",
            "                         south_korea       0.00      0.00      0.00         1\n",
            "                              sports       0.00      0.00      0.00        10\n",
            "                    state_department       0.00      0.00      0.00        24\n",
            "                       supreme_court       0.00      0.00      0.00        91\n",
            "                               taxes       0.33      0.25      0.29        44\n",
            "                           tea_party       0.00      0.00      0.00         6\n",
            "                          technology       0.00      0.00      0.00        27\n",
            "                           terrorism       0.38      0.14      0.21        56\n",
            "                               trade       0.00      0.00      0.00        44\n",
            "                      transportation       0.00      0.00      0.00         8\n",
            "                            treasury       0.00      0.00      0.00         8\n",
            "                         us_congress       0.00      0.00      0.00        59\n",
            "                     us_constitution       0.00      0.00      0.00         6\n",
            "                            us_house       0.00      0.00      0.00        75\n",
            "                         us_military       0.00      0.00      0.00        28\n",
            "                           us_senate       0.00      0.00      0.00        83\n",
            "                    veterans_affairs       0.00      0.00      0.00         8\n",
            "                 violence_in_america       0.00      0.00      0.00        75\n",
            "       voting_rights_and_voter_fraud       0.00      0.00      0.00        12\n",
            "                             welfare       0.00      0.00      0.00         2\n",
            "                         white_house       0.14      0.30      0.19       221\n",
            "                      women's_issues       0.00      0.00      0.00         8\n",
            "                               world       0.00      0.00      0.00        65\n",
            "\n",
            "                            accuracy                           0.23      4197\n",
            "                           macro avg       0.04      0.05      0.04      4197\n",
            "                        weighted avg       0.14      0.23      0.16      4197\n",
            "\n",
            "\n",
            "Resultados GRU:\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abortion       0.00      0.00      0.00        40\n",
            "                              africa       0.00      0.00      0.00         1\n",
            "                         agriculture       0.00      0.00      0.00         3\n",
            "                      animal_welfare       0.00      0.00      0.00         2\n",
            "              arts_and_entertainment       0.00      0.00      0.00         6\n",
            "                                asia       0.00      0.00      0.00         8\n",
            "                 banking_and_finance       0.00      0.00      0.00        24\n",
            "                    bridging_divides       0.00      0.00      0.00         9\n",
            "                            business       0.00      0.00      0.00        10\n",
            "                    campaign_finance       0.00      0.00      0.00        22\n",
            "                   campaign_rhetoric       0.00      0.00      0.00         5\n",
            "capital_punishment_and_death_penalty       0.00      0.00      0.00         1\n",
            "                               china       0.00      0.00      0.00        17\n",
            "                                 cia       0.00      0.00      0.00         7\n",
            "                        civil_rights       0.00      0.00      0.00        19\n",
            "                         coronavirus       0.00      0.00      0.00       120\n",
            "                    criminal_justice       0.00      0.00      0.00        13\n",
            "                             culture       0.00      0.00      0.00        35\n",
            "                       cybersecurity       0.00      0.00      0.00        11\n",
            "                       death_penalty       0.00      0.00      0.00         2\n",
            "                             defense       0.00      0.00      0.00        25\n",
            "                    democratic_party       0.00      0.00      0.00        25\n",
            "                            disaster       0.00      0.00      0.00        13\n",
            "                     domestic_policy       0.00      0.00      0.00         2\n",
            "                               ebola       0.00      0.00      0.00        10\n",
            "                     economic_policy       0.00      0.00      0.00        24\n",
            "                    economy_and_jobs       0.00      0.00      0.00        78\n",
            "                           education       0.00      0.00      0.00        48\n",
            "                           elections       0.16      1.00      0.28       678\n",
            "                              energy       0.00      0.00      0.00         9\n",
            "                         environment       0.00      0.00      0.00        66\n",
            "                                 epa       0.00      0.00      0.00         5\n",
            "                              europe       0.00      0.00      0.00        15\n",
            "             facts_and_fact_checking       0.00      0.00      0.00         9\n",
            "                           fake_news       0.00      0.00      0.00         8\n",
            "                 family_and_marriage       0.00      0.00      0.00         1\n",
            "                                 fbi       0.00      0.00      0.00        36\n",
            "                                 fda       0.00      0.00      0.00         1\n",
            "                      federal_budget       0.00      0.00      0.00        72\n",
            "                        fiscal_cliff       0.00      0.00      0.00        21\n",
            "                                food       0.00      0.00      0.00         2\n",
            "                      foreign_policy       0.00      0.00      0.00        60\n",
            "                         free_speech       0.00      0.00      0.00        30\n",
            "                        general_news       0.00      0.00      0.00        34\n",
            "                       great_britain       0.00      0.00      0.00        19\n",
            "          gun_control_and_gun_rights       0.00      0.00      0.00        97\n",
            "                          healthcare       0.00      0.00      0.00       163\n",
            "                            holidays       0.00      0.00      0.00        17\n",
            "                   homeland_security       0.00      0.00      0.00        14\n",
            "            housing_and_homelessness       0.00      0.00      0.00         3\n",
            "                         immigration       0.00      0.00      0.00       186\n",
            "                         impeachment       0.00      0.00      0.00        47\n",
            "                          inequality       0.00      0.00      0.00         4\n",
            "                                isis       0.00      0.00      0.00        19\n",
            "                              israel       0.00      0.00      0.00        14\n",
            "                             justice       0.00      0.00      0.00        35\n",
            "                  justice_department       0.00      0.00      0.00        32\n",
            "                               labor       0.00      0.00      0.00        15\n",
            "                         lgbt_rights       0.00      0.00      0.00        33\n",
            "              marijuana_legalization       0.00      0.00      0.00        12\n",
            "                          media_bias       0.00      0.00      0.00       124\n",
            "                      media_industry       0.00      0.00      0.00         4\n",
            "                              mexico       0.00      0.00      0.00        11\n",
            "                         middle_east       0.00      0.00      0.00       105\n",
            "                    national_defense       0.00      0.00      0.00        16\n",
            "                   national_security       0.00      0.00      0.00        62\n",
            "                         north_korea       0.00      0.00      0.00        36\n",
            "                                 nsa       0.00      0.00      0.00        28\n",
            "                     nuclear_weapons       0.00      0.00      0.00         4\n",
            "            obesity_and_malnutrition       0.00      0.00      0.00         1\n",
            "                       opioid_crisis       0.00      0.00      0.00         4\n",
            "                           palestine       0.00      0.00      0.00         4\n",
            "                        polarization       0.00      0.00      0.00        50\n",
            "                            politics       0.00      0.00      0.00       254\n",
            "                             privacy       0.00      0.00      0.00         6\n",
            "                       public_health       0.00      0.00      0.00         2\n",
            "                     race_and_racism       0.00      0.00      0.00        50\n",
            "                  religion_and_faith       0.00      0.00      0.00        24\n",
            "                    republican_party       0.00      0.00      0.00        63\n",
            "                  role_of_government       0.00      0.00      0.00         7\n",
            "                              russia       0.00      0.00      0.00        23\n",
            "                             science       0.00      0.00      0.00         3\n",
            "                   sexual_misconduct       0.00      0.00      0.00        16\n",
            "                     social_security       0.00      0.00      0.00         2\n",
            "                         south_korea       0.00      0.00      0.00         1\n",
            "                              sports       0.00      0.00      0.00        10\n",
            "                    state_department       0.00      0.00      0.00        24\n",
            "                       supreme_court       0.00      0.00      0.00        91\n",
            "                               taxes       0.00      0.00      0.00        44\n",
            "                           tea_party       0.00      0.00      0.00         6\n",
            "                          technology       0.00      0.00      0.00        27\n",
            "                           terrorism       0.00      0.00      0.00        56\n",
            "                               trade       0.00      0.00      0.00        44\n",
            "                      transportation       0.00      0.00      0.00         8\n",
            "                            treasury       0.00      0.00      0.00         8\n",
            "                         us_congress       0.00      0.00      0.00        59\n",
            "                     us_constitution       0.00      0.00      0.00         6\n",
            "                            us_house       0.00      0.00      0.00        75\n",
            "                         us_military       0.00      0.00      0.00        28\n",
            "                           us_senate       0.00      0.00      0.00        83\n",
            "                    veterans_affairs       0.00      0.00      0.00         8\n",
            "                 violence_in_america       0.00      0.00      0.00        75\n",
            "       voting_rights_and_voter_fraud       0.00      0.00      0.00        12\n",
            "                             welfare       0.00      0.00      0.00         2\n",
            "                         white_house       0.00      0.00      0.00       221\n",
            "                      women's_issues       0.00      0.00      0.00         8\n",
            "                               world       0.00      0.00      0.00        65\n",
            "\n",
            "                            accuracy                           0.16      4197\n",
            "                           macro avg       0.00      0.01      0.00      4197\n",
            "                        weighted avg       0.03      0.16      0.04      4197\n",
            "\n",
            "\n",
            "Resumen de resultados:\n",
            "  Model  Accuracy  Macro-F1\n",
            "0  LSTM  0.234215  0.035418\n",
            "1   GRU  0.161544  0.002600\n"
          ]
        }
      ],
      "source": [
        "# Para topic\n",
        "results_topic = run_random_embedding_experiment(df_train, \"topic\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Word2Vec congelado vs Word2Vec fine-tuneado vs Word2Vec from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hacemos una función para ejecutar con la variable objetivo que queramos\n",
        "def run_w2v_experiment(df, target_col, w2v_path=\"data/embeddings/word2vec.model\",\n",
        "                       embedding_dim=100, max_seq_len=200, batch_size=64, epochs=20,\n",
        "                       min_samples_per_class=5):\n",
        "    \n",
        "    print(f\"\\n===== Word2Vec experiment | Target: {target_col} =====\\n\")\n",
        "    \n",
        "    # Filtramos las clases raras\n",
        "    counts = df[target_col].value_counts()\n",
        "    valid_classes = counts[counts >= min_samples_per_class].index\n",
        "    df_filtered = df[df[target_col].isin(valid_classes)].copy()\n",
        "    \n",
        "    if df_filtered.empty:\n",
        "        print(f\"No hay suficientes datos para {target_col} después de filtrar clases raras.\")\n",
        "        return None\n",
        "    \n",
        "    # Preparamos los textos y etiquetas\n",
        "    texts = df_filtered[\"tokens\"].tolist()\n",
        "    labels = df_filtered[target_col].tolist()\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(labels)\n",
        "    \n",
        "    # Hacemos el Train/Val/Test split\n",
        "    X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(\n",
        "        texts, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "    \n",
        "    X_val_texts, X_test_texts, y_val, y_test = train_test_split(\n",
        "        X_temp_texts, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "    )\n",
        "    \n",
        "    # Cargamos el modelo Word2Vec\n",
        "    w2v_model = Word2Vec.load(w2v_path)\n",
        "    embedding_dim = w2v_model.vector_size\n",
        "    word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "    vocab_size = len(word_index) + 1\n",
        "    \n",
        "    def tokens_to_indices(tokens, word_index):\n",
        "        return [word_index[t] for t in tokens if t in word_index]\n",
        "    \n",
        "    X_train_idx = [tokens_to_indices(t, word_index) for t in X_train_texts]\n",
        "    X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_texts]\n",
        "    X_test_idx = [tokens_to_indices(t, word_index) for t in X_test_texts]\n",
        "    \n",
        "    X_train_pad = pad_sequences(X_train_idx, maxlen=max_seq_len, padding='post')\n",
        "    X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "    X_test_pad = pad_sequences(X_test_idx, maxlen=max_seq_len, padding='post')\n",
        "    \n",
        "    # Creamos la matriz de embedding\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "    \n",
        "    # Funcion para construir el modelo LSTM\n",
        "    def build_lstm_model(embedding_matrix, trainable=True):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(\n",
        "            input_dim=embedding_matrix.shape[0],\n",
        "            output_dim=embedding_matrix.shape[1],\n",
        "            weights=[embedding_matrix],\n",
        "            input_length=max_seq_len,\n",
        "            trainable=trainable\n",
        "        ))\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "        model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))\n",
        "        model.compile(optimizer=Adam(1e-3),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    \n",
        "    # Enrtenamos\n",
        "    # Word2Vec Frozen\n",
        "    lstm_frozen = build_lstm_model(embedding_matrix, trainable=False)\n",
        "    lstm_frozen.fit(X_train_pad, y_train,\n",
        "                    validation_data=(X_val_pad, y_val),\n",
        "                    epochs=epochs, batch_size=batch_size,\n",
        "                    callbacks=[early_stopping])\n",
        "    \n",
        "    # Word2Vec Fine-tune\n",
        "    lstm_finetune = build_lstm_model(embedding_matrix, trainable=True)\n",
        "    lstm_finetune.fit(X_train_pad, y_train,\n",
        "                      validation_data=(X_val_pad, y_val),\n",
        "                      epochs=epochs, batch_size=batch_size,\n",
        "                      callbacks=[early_stopping])\n",
        "    \n",
        "    # Word2Vec Scratch \n",
        "    embedding_matrix_random = np.random.normal(size=(vocab_size, embedding_dim))\n",
        "    lstm_scratch = build_lstm_model(embedding_matrix_random, trainable=True)\n",
        "    lstm_scratch.fit(X_train_pad, y_train,\n",
        "                      validation_data=(X_val_pad, y_val),\n",
        "                      epochs=epochs, batch_size=batch_size,\n",
        "                      callbacks=[early_stopping])\n",
        "    \n",
        "    # Evaluamos\n",
        "    def evaluate(model, X_test, y_test):\n",
        "        preds = np.argmax(model.predict(X_test, batch_size=batch_size), axis=1)\n",
        "        y_test_str = le.inverse_transform(y_test)\n",
        "        preds_str = le.inverse_transform(preds)\n",
        "        print(classification_report(y_test_str, preds_str))\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        f1 = f1_score(y_test, preds, average='macro')\n",
        "        return acc, f1\n",
        "    \n",
        "    results = {}\n",
        "    print(\"\\nResultados Word2Vec Frozen\")\n",
        "    results['Word2Vec Frozen'] = evaluate(lstm_frozen, X_test_pad, y_test)\n",
        "    \n",
        "    print(\"\\nResultados Word2Vec Fine-tune\")\n",
        "    results['Word2Vec Fine-tune'] = evaluate(lstm_finetune, X_test_pad, y_test)\n",
        "    \n",
        "    print(\"\\nResultados Word2Vec Scratch\")\n",
        "    results['Word2Vec Scratch'] = evaluate(lstm_scratch, X_test_pad, y_test)\n",
        "\n",
        "    results_df = pd.DataFrame(results, index=['Accuracy','Macro-F1']).T\n",
        "    print(\"\\nResumen de resultados:\")\n",
        "    print(results_df)\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Word2Vec experiment | Target: topic =====\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 723ms/step - accuracy: 0.1688 - loss: 3.7845 - val_accuracy: 0.2171 - val_loss: 3.6130\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 796ms/step - accuracy: 0.2002 - loss: 3.5086 - val_accuracy: 0.2398 - val_loss: 3.2151\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 816ms/step - accuracy: 0.2528 - loss: 3.1476 - val_accuracy: 0.3132 - val_loss: 2.8662\n",
            "Epoch 4/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 736ms/step - accuracy: 0.3254 - loss: 2.7796 - val_accuracy: 0.3665 - val_loss: 2.5484\n",
            "Epoch 5/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 939ms/step - accuracy: 0.3726 - loss: 2.5319 - val_accuracy: 0.4009 - val_loss: 2.3977\n",
            "Epoch 6/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 895ms/step - accuracy: 0.3971 - loss: 2.3950 - val_accuracy: 0.4254 - val_loss: 2.2938\n",
            "Epoch 7/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 856ms/step - accuracy: 0.4207 - loss: 2.2851 - val_accuracy: 0.4399 - val_loss: 2.2046\n",
            "Epoch 8/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 865ms/step - accuracy: 0.4358 - loss: 2.1993 - val_accuracy: 0.4566 - val_loss: 2.1392\n",
            "Epoch 9/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 870ms/step - accuracy: 0.4457 - loss: 2.1329 - val_accuracy: 0.4619 - val_loss: 2.0866\n",
            "Epoch 10/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 864ms/step - accuracy: 0.4483 - loss: 2.1269 - val_accuracy: 0.4707 - val_loss: 2.0641\n",
            "Epoch 11/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 867ms/step - accuracy: 0.4633 - loss: 2.0529 - val_accuracy: 0.4764 - val_loss: 2.0108\n",
            "Epoch 12/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 875ms/step - accuracy: 0.4753 - loss: 2.0002 - val_accuracy: 0.4809 - val_loss: 1.9813\n",
            "Epoch 13/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 601ms/step - accuracy: 0.4771 - loss: 1.9673 - val_accuracy: 0.4847 - val_loss: 1.9720\n",
            "Epoch 14/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 451ms/step - accuracy: 0.4783 - loss: 1.9352 - val_accuracy: 0.4816 - val_loss: 1.9648\n",
            "Epoch 15/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 446ms/step - accuracy: 0.4874 - loss: 1.9021 - val_accuracy: 0.4871 - val_loss: 1.9262\n",
            "Epoch 16/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 478ms/step - accuracy: 0.4924 - loss: 1.8809 - val_accuracy: 0.4964 - val_loss: 1.9175\n",
            "Epoch 17/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 479ms/step - accuracy: 0.4992 - loss: 1.8505 - val_accuracy: 0.4976 - val_loss: 1.9060\n",
            "Epoch 18/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 436ms/step - accuracy: 0.5056 - loss: 1.8271 - val_accuracy: 0.4955 - val_loss: 1.8987\n",
            "Epoch 19/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 478ms/step - accuracy: 0.5068 - loss: 1.8085 - val_accuracy: 0.4950 - val_loss: 1.8937\n",
            "Epoch 20/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 488ms/step - accuracy: 0.5112 - loss: 1.7820 - val_accuracy: 0.5029 - val_loss: 1.8758\n",
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 570ms/step - accuracy: 0.1692 - loss: 3.7726 - val_accuracy: 0.1809 - val_loss: 3.6357\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 748ms/step - accuracy: 0.2204 - loss: 3.4151 - val_accuracy: 0.2741 - val_loss: 3.1278\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 861ms/step - accuracy: 0.2906 - loss: 2.9847 - val_accuracy: 0.3277 - val_loss: 2.7597\n",
            "Epoch 1/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 853ms/step - accuracy: 0.1509 - loss: 3.9393 - val_accuracy: 0.1616 - val_loss: 3.8380\n",
            "Epoch 2/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 689ms/step - accuracy: 0.1744 - loss: 3.6815 - val_accuracy: 0.2083 - val_loss: 3.4951\n",
            "Epoch 3/20\n",
            "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 468ms/step - accuracy: 0.2690 - loss: 3.2001 - val_accuracy: 0.3172 - val_loss: 2.9776\n",
            "\n",
            "Resultados Word2Vec Frozen\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abortion       0.70      0.82      0.76        40\n",
            "                              africa       0.00      0.00      0.00         1\n",
            "                         agriculture       0.00      0.00      0.00         3\n",
            "                      animal_welfare       0.00      0.00      0.00         2\n",
            "              arts_and_entertainment       0.00      0.00      0.00         6\n",
            "                                asia       0.00      0.00      0.00         8\n",
            "                 banking_and_finance       0.27      0.25      0.26        24\n",
            "                    bridging_divides       0.00      0.00      0.00         9\n",
            "                            business       0.00      0.00      0.00        10\n",
            "                    campaign_finance       0.29      0.18      0.22        22\n",
            "                   campaign_rhetoric       0.00      0.00      0.00         5\n",
            "capital_punishment_and_death_penalty       0.00      0.00      0.00         1\n",
            "                               china       0.23      0.29      0.26        17\n",
            "                                 cia       0.50      0.29      0.36         7\n",
            "                        civil_rights       1.00      0.05      0.10        19\n",
            "                         coronavirus       0.61      0.79      0.69       120\n",
            "                    criminal_justice       0.40      0.46      0.43        13\n",
            "                             culture       0.10      0.11      0.11        35\n",
            "                       cybersecurity       0.14      0.09      0.11        11\n",
            "                       death_penalty       0.00      0.00      0.00         2\n",
            "                             defense       0.33      0.24      0.28        25\n",
            "                    democratic_party       0.00      0.00      0.00        25\n",
            "                            disaster       0.55      0.46      0.50        13\n",
            "                     domestic_policy       0.00      0.00      0.00         2\n",
            "                               ebola       0.57      0.80      0.67        10\n",
            "                     economic_policy       0.29      0.08      0.13        24\n",
            "                    economy_and_jobs       0.42      0.54      0.47        78\n",
            "                           education       0.69      0.75      0.72        48\n",
            "                           elections       0.61      0.84      0.71       678\n",
            "                              energy       0.80      0.44      0.57         9\n",
            "                         environment       0.61      0.83      0.71        66\n",
            "                                 epa       0.50      0.20      0.29         5\n",
            "                              europe       0.43      0.20      0.27        15\n",
            "             facts_and_fact_checking       0.00      0.00      0.00         9\n",
            "                           fake_news       0.00      0.00      0.00         8\n",
            "                 family_and_marriage       0.00      0.00      0.00         1\n",
            "                                 fbi       0.36      0.44      0.40        36\n",
            "                                 fda       0.00      0.00      0.00         1\n",
            "                      federal_budget       0.45      0.60      0.51        72\n",
            "                        fiscal_cliff       0.59      0.62      0.60        21\n",
            "                                food       0.00      0.00      0.00         2\n",
            "                      foreign_policy       0.23      0.23      0.23        60\n",
            "                         free_speech       0.47      0.23      0.31        30\n",
            "                        general_news       0.06      0.03      0.04        34\n",
            "                       great_britain       0.72      0.68      0.70        19\n",
            "          gun_control_and_gun_rights       0.72      0.84      0.78        97\n",
            "                          healthcare       0.75      0.83      0.78       163\n",
            "                            holidays       0.15      0.12      0.13        17\n",
            "                   homeland_security       0.50      0.14      0.22        14\n",
            "            housing_and_homelessness       0.00      0.00      0.00         3\n",
            "                         immigration       0.64      0.87      0.74       186\n",
            "                         impeachment       0.38      0.60      0.47        47\n",
            "                          inequality       0.00      0.00      0.00         4\n",
            "                                isis       0.22      0.11      0.14        19\n",
            "                              israel       0.44      0.57      0.50        14\n",
            "                             justice       0.09      0.06      0.07        35\n",
            "                  justice_department       0.23      0.09      0.13        32\n",
            "                               labor       0.33      0.13      0.19        15\n",
            "                         lgbt_rights       0.56      0.73      0.63        33\n",
            "              marijuana_legalization       0.53      0.83      0.65        12\n",
            "                          media_bias       0.38      0.44      0.40       124\n",
            "                      media_industry       0.00      0.00      0.00         4\n",
            "                              mexico       0.00      0.00      0.00        11\n",
            "                         middle_east       0.39      0.71      0.51       105\n",
            "                    national_defense       0.00      0.00      0.00        16\n",
            "                   national_security       0.16      0.06      0.09        62\n",
            "                         north_korea       0.56      0.89      0.69        36\n",
            "                                 nsa       0.63      0.86      0.73        28\n",
            "                     nuclear_weapons       0.00      0.00      0.00         4\n",
            "            obesity_and_malnutrition       0.00      0.00      0.00         1\n",
            "                       opioid_crisis       0.25      0.25      0.25         4\n",
            "                           palestine       0.00      0.00      0.00         4\n",
            "                        polarization       0.35      0.12      0.18        50\n",
            "                            politics       0.21      0.10      0.14       254\n",
            "                             privacy       0.00      0.00      0.00         6\n",
            "                       public_health       0.00      0.00      0.00         2\n",
            "                     race_and_racism       0.35      0.24      0.29        50\n",
            "                  religion_and_faith       0.43      0.42      0.43        24\n",
            "                    republican_party       0.14      0.05      0.07        63\n",
            "                  role_of_government       0.00      0.00      0.00         7\n",
            "                              russia       0.00      0.00      0.00        23\n",
            "                             science       0.00      0.00      0.00         3\n",
            "                   sexual_misconduct       0.30      0.44      0.36        16\n",
            "                     social_security       0.00      0.00      0.00         2\n",
            "                         south_korea       0.00      0.00      0.00         1\n",
            "                              sports       0.67      0.20      0.31        10\n",
            "                    state_department       0.28      0.21      0.24        24\n",
            "                       supreme_court       0.59      0.75      0.66        91\n",
            "                               taxes       0.61      0.50      0.55        44\n",
            "                           tea_party       0.00      0.00      0.00         6\n",
            "                          technology       0.28      0.44      0.34        27\n",
            "                           terrorism       0.47      0.62      0.54        56\n",
            "                               trade       0.61      0.70      0.65        44\n",
            "                      transportation       0.17      0.12      0.14         8\n",
            "                            treasury       0.33      0.25      0.29         8\n",
            "                         us_congress       0.16      0.05      0.08        59\n",
            "                     us_constitution       0.00      0.00      0.00         6\n",
            "                            us_house       0.35      0.23      0.28        75\n",
            "                         us_military       0.23      0.32      0.27        28\n",
            "                           us_senate       0.27      0.16      0.20        83\n",
            "                    veterans_affairs       0.55      0.75      0.63         8\n",
            "                 violence_in_america       0.47      0.59      0.52        75\n",
            "       voting_rights_and_voter_fraud       0.40      0.17      0.24        12\n",
            "                             welfare       0.00      0.00      0.00         2\n",
            "                         white_house       0.29      0.27      0.28       221\n",
            "                      women's_issues       0.00      0.00      0.00         8\n",
            "                               world       0.42      0.42      0.42        65\n",
            "\n",
            "                            accuracy                           0.49      4197\n",
            "                           macro avg       0.28      0.27      0.26      4197\n",
            "                        weighted avg       0.43      0.49      0.45      4197\n",
            "\n",
            "\n",
            "Resultados Word2Vec Fine-tune\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 171ms/step\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abortion       0.00      0.00      0.00        40\n",
            "                              africa       0.00      0.00      0.00         1\n",
            "                         agriculture       0.00      0.00      0.00         3\n",
            "                      animal_welfare       0.00      0.00      0.00         2\n",
            "              arts_and_entertainment       0.00      0.00      0.00         6\n",
            "                                asia       0.00      0.00      0.00         8\n",
            "                 banking_and_finance       0.00      0.00      0.00        24\n",
            "                    bridging_divides       0.00      0.00      0.00         9\n",
            "                            business       0.00      0.00      0.00        10\n",
            "                    campaign_finance       0.00      0.00      0.00        22\n",
            "                   campaign_rhetoric       0.00      0.00      0.00         5\n",
            "capital_punishment_and_death_penalty       0.00      0.00      0.00         1\n",
            "                               china       0.00      0.00      0.00        17\n",
            "                                 cia       0.00      0.00      0.00         7\n",
            "                        civil_rights       0.00      0.00      0.00        19\n",
            "                         coronavirus       0.00      0.00      0.00       120\n",
            "                    criminal_justice       0.00      0.00      0.00        13\n",
            "                             culture       0.00      0.00      0.00        35\n",
            "                       cybersecurity       0.00      0.00      0.00        11\n",
            "                       death_penalty       0.00      0.00      0.00         2\n",
            "                             defense       0.00      0.00      0.00        25\n",
            "                    democratic_party       0.00      0.00      0.00        25\n",
            "                            disaster       0.00      0.00      0.00        13\n",
            "                     domestic_policy       0.00      0.00      0.00         2\n",
            "                               ebola       0.00      0.00      0.00        10\n",
            "                     economic_policy       0.00      0.00      0.00        24\n",
            "                    economy_and_jobs       0.00      0.00      0.00        78\n",
            "                           education       0.00      0.00      0.00        48\n",
            "                           elections       0.18      0.99      0.30       678\n",
            "                              energy       0.00      0.00      0.00         9\n",
            "                         environment       0.00      0.00      0.00        66\n",
            "                                 epa       0.00      0.00      0.00         5\n",
            "                              europe       0.00      0.00      0.00        15\n",
            "             facts_and_fact_checking       0.00      0.00      0.00         9\n",
            "                           fake_news       0.00      0.00      0.00         8\n",
            "                 family_and_marriage       0.00      0.00      0.00         1\n",
            "                                 fbi       0.00      0.00      0.00        36\n",
            "                                 fda       0.00      0.00      0.00         1\n",
            "                      federal_budget       0.00      0.00      0.00        72\n",
            "                        fiscal_cliff       0.00      0.00      0.00        21\n",
            "                                food       0.00      0.00      0.00         2\n",
            "                      foreign_policy       0.00      0.00      0.00        60\n",
            "                         free_speech       0.00      0.00      0.00        30\n",
            "                        general_news       0.00      0.00      0.00        34\n",
            "                       great_britain       0.00      0.00      0.00        19\n",
            "          gun_control_and_gun_rights       0.00      0.00      0.00        97\n",
            "                          healthcare       0.23      0.58      0.33       163\n",
            "                            holidays       0.00      0.00      0.00        17\n",
            "                   homeland_security       0.00      0.00      0.00        14\n",
            "            housing_and_homelessness       0.00      0.00      0.00         3\n",
            "                         immigration       0.00      0.00      0.00       186\n",
            "                         impeachment       0.00      0.00      0.00        47\n",
            "                          inequality       0.00      0.00      0.00         4\n",
            "                                isis       0.00      0.00      0.00        19\n",
            "                              israel       0.00      0.00      0.00        14\n",
            "                             justice       0.00      0.00      0.00        35\n",
            "                  justice_department       0.00      0.00      0.00        32\n",
            "                               labor       0.00      0.00      0.00        15\n",
            "                         lgbt_rights       0.00      0.00      0.00        33\n",
            "              marijuana_legalization       0.00      0.00      0.00        12\n",
            "                          media_bias       0.00      0.00      0.00       124\n",
            "                      media_industry       0.00      0.00      0.00         4\n",
            "                              mexico       0.00      0.00      0.00        11\n",
            "                         middle_east       0.00      0.00      0.00       105\n",
            "                    national_defense       0.00      0.00      0.00        16\n",
            "                   national_security       0.00      0.00      0.00        62\n",
            "                         north_korea       0.00      0.00      0.00        36\n",
            "                                 nsa       0.00      0.00      0.00        28\n",
            "                     nuclear_weapons       0.00      0.00      0.00         4\n",
            "            obesity_and_malnutrition       0.00      0.00      0.00         1\n",
            "                       opioid_crisis       0.00      0.00      0.00         4\n",
            "                           palestine       0.00      0.00      0.00         4\n",
            "                        polarization       0.00      0.00      0.00        50\n",
            "                            politics       0.00      0.00      0.00       254\n",
            "                             privacy       0.00      0.00      0.00         6\n",
            "                       public_health       0.00      0.00      0.00         2\n",
            "                     race_and_racism       0.00      0.00      0.00        50\n",
            "                  religion_and_faith       0.00      0.00      0.00        24\n",
            "                    republican_party       0.00      0.00      0.00        63\n",
            "                  role_of_government       0.00      0.00      0.00         7\n",
            "                              russia       0.00      0.00      0.00        23\n",
            "                             science       0.00      0.00      0.00         3\n",
            "                   sexual_misconduct       0.00      0.00      0.00        16\n",
            "                     social_security       0.00      0.00      0.00         2\n",
            "                         south_korea       0.00      0.00      0.00         1\n",
            "                              sports       0.00      0.00      0.00        10\n",
            "                    state_department       0.00      0.00      0.00        24\n",
            "                       supreme_court       0.00      0.00      0.00        91\n",
            "                               taxes       0.00      0.00      0.00        44\n",
            "                           tea_party       0.00      0.00      0.00         6\n",
            "                          technology       0.00      0.00      0.00        27\n",
            "                           terrorism       0.00      0.00      0.00        56\n",
            "                               trade       0.00      0.00      0.00        44\n",
            "                      transportation       0.00      0.00      0.00         8\n",
            "                            treasury       0.00      0.00      0.00         8\n",
            "                         us_congress       0.00      0.00      0.00        59\n",
            "                     us_constitution       0.00      0.00      0.00         6\n",
            "                            us_house       0.00      0.00      0.00        75\n",
            "                         us_military       0.00      0.00      0.00        28\n",
            "                           us_senate       0.00      0.00      0.00        83\n",
            "                    veterans_affairs       0.00      0.00      0.00         8\n",
            "                 violence_in_america       0.00      0.00      0.00        75\n",
            "       voting_rights_and_voter_fraud       0.00      0.00      0.00        12\n",
            "                             welfare       0.00      0.00      0.00         2\n",
            "                         white_house       0.00      0.00      0.00       221\n",
            "                      women's_issues       0.00      0.00      0.00         8\n",
            "                               world       0.00      0.00      0.00        65\n",
            "\n",
            "                            accuracy                           0.18      4197\n",
            "                           macro avg       0.00      0.01      0.01      4197\n",
            "                        weighted avg       0.04      0.18      0.06      4197\n",
            "\n",
            "\n",
            "Resultados Word2Vec Scratch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2s/step\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abortion       0.00      0.00      0.00        40\n",
            "                              africa       0.00      0.00      0.00         1\n",
            "                         agriculture       0.00      0.00      0.00         3\n",
            "                      animal_welfare       0.00      0.00      0.00         2\n",
            "              arts_and_entertainment       0.00      0.00      0.00         6\n",
            "                                asia       0.00      0.00      0.00         8\n",
            "                 banking_and_finance       0.00      0.00      0.00        24\n",
            "                    bridging_divides       0.00      0.00      0.00         9\n",
            "                            business       0.00      0.00      0.00        10\n",
            "                    campaign_finance       0.00      0.00      0.00        22\n",
            "                   campaign_rhetoric       0.00      0.00      0.00         5\n",
            "capital_punishment_and_death_penalty       0.00      0.00      0.00         1\n",
            "                               china       0.00      0.00      0.00        17\n",
            "                                 cia       0.00      0.00      0.00         7\n",
            "                        civil_rights       0.00      0.00      0.00        19\n",
            "                         coronavirus       0.00      0.00      0.00       120\n",
            "                    criminal_justice       0.00      0.00      0.00        13\n",
            "                             culture       0.00      0.00      0.00        35\n",
            "                       cybersecurity       0.00      0.00      0.00        11\n",
            "                       death_penalty       0.00      0.00      0.00         2\n",
            "                             defense       0.00      0.00      0.00        25\n",
            "                    democratic_party       0.00      0.00      0.00        25\n",
            "                            disaster       0.00      0.00      0.00        13\n",
            "                     domestic_policy       0.00      0.00      0.00         2\n",
            "                               ebola       0.00      0.00      0.00        10\n",
            "                     economic_policy       0.00      0.00      0.00        24\n",
            "                    economy_and_jobs       0.00      0.00      0.00        78\n",
            "                           education       0.00      0.00      0.00        48\n",
            "                           elections       0.16      1.00      0.28       678\n",
            "                              energy       0.00      0.00      0.00         9\n",
            "                         environment       0.00      0.00      0.00        66\n",
            "                                 epa       0.00      0.00      0.00         5\n",
            "                              europe       0.00      0.00      0.00        15\n",
            "             facts_and_fact_checking       0.00      0.00      0.00         9\n",
            "                           fake_news       0.00      0.00      0.00         8\n",
            "                 family_and_marriage       0.00      0.00      0.00         1\n",
            "                                 fbi       0.00      0.00      0.00        36\n",
            "                                 fda       0.00      0.00      0.00         1\n",
            "                      federal_budget       0.00      0.00      0.00        72\n",
            "                        fiscal_cliff       0.00      0.00      0.00        21\n",
            "                                food       0.00      0.00      0.00         2\n",
            "                      foreign_policy       0.00      0.00      0.00        60\n",
            "                         free_speech       0.00      0.00      0.00        30\n",
            "                        general_news       0.00      0.00      0.00        34\n",
            "                       great_britain       0.00      0.00      0.00        19\n",
            "          gun_control_and_gun_rights       0.00      0.00      0.00        97\n",
            "                          healthcare       0.00      0.00      0.00       163\n",
            "                            holidays       0.00      0.00      0.00        17\n",
            "                   homeland_security       0.00      0.00      0.00        14\n",
            "            housing_and_homelessness       0.00      0.00      0.00         3\n",
            "                         immigration       0.00      0.00      0.00       186\n",
            "                         impeachment       0.00      0.00      0.00        47\n",
            "                          inequality       0.00      0.00      0.00         4\n",
            "                                isis       0.00      0.00      0.00        19\n",
            "                              israel       0.00      0.00      0.00        14\n",
            "                             justice       0.00      0.00      0.00        35\n",
            "                  justice_department       0.00      0.00      0.00        32\n",
            "                               labor       0.00      0.00      0.00        15\n",
            "                         lgbt_rights       0.00      0.00      0.00        33\n",
            "              marijuana_legalization       0.00      0.00      0.00        12\n",
            "                          media_bias       0.00      0.00      0.00       124\n",
            "                      media_industry       0.00      0.00      0.00         4\n",
            "                              mexico       0.00      0.00      0.00        11\n",
            "                         middle_east       0.00      0.00      0.00       105\n",
            "                    national_defense       0.00      0.00      0.00        16\n",
            "                   national_security       0.00      0.00      0.00        62\n",
            "                         north_korea       0.00      0.00      0.00        36\n",
            "                                 nsa       0.00      0.00      0.00        28\n",
            "                     nuclear_weapons       0.00      0.00      0.00         4\n",
            "            obesity_and_malnutrition       0.00      0.00      0.00         1\n",
            "                       opioid_crisis       0.00      0.00      0.00         4\n",
            "                           palestine       0.00      0.00      0.00         4\n",
            "                        polarization       0.00      0.00      0.00        50\n",
            "                            politics       0.00      0.00      0.00       254\n",
            "                             privacy       0.00      0.00      0.00         6\n",
            "                       public_health       0.00      0.00      0.00         2\n",
            "                     race_and_racism       0.00      0.00      0.00        50\n",
            "                  religion_and_faith       0.00      0.00      0.00        24\n",
            "                    republican_party       0.00      0.00      0.00        63\n",
            "                  role_of_government       0.00      0.00      0.00         7\n",
            "                              russia       0.00      0.00      0.00        23\n",
            "                             science       0.00      0.00      0.00         3\n",
            "                   sexual_misconduct       0.00      0.00      0.00        16\n",
            "                     social_security       0.00      0.00      0.00         2\n",
            "                         south_korea       0.00      0.00      0.00         1\n",
            "                              sports       0.00      0.00      0.00        10\n",
            "                    state_department       0.00      0.00      0.00        24\n",
            "                       supreme_court       0.00      0.00      0.00        91\n",
            "                               taxes       0.00      0.00      0.00        44\n",
            "                           tea_party       0.00      0.00      0.00         6\n",
            "                          technology       0.00      0.00      0.00        27\n",
            "                           terrorism       0.00      0.00      0.00        56\n",
            "                               trade       0.00      0.00      0.00        44\n",
            "                      transportation       0.00      0.00      0.00         8\n",
            "                            treasury       0.00      0.00      0.00         8\n",
            "                         us_congress       0.00      0.00      0.00        59\n",
            "                     us_constitution       0.00      0.00      0.00         6\n",
            "                            us_house       0.00      0.00      0.00        75\n",
            "                         us_military       0.00      0.00      0.00        28\n",
            "                           us_senate       0.00      0.00      0.00        83\n",
            "                    veterans_affairs       0.00      0.00      0.00         8\n",
            "                 violence_in_america       0.00      0.00      0.00        75\n",
            "       voting_rights_and_voter_fraud       0.00      0.00      0.00        12\n",
            "                             welfare       0.00      0.00      0.00         2\n",
            "                         white_house       0.00      0.00      0.00       221\n",
            "                      women's_issues       0.00      0.00      0.00         8\n",
            "                               world       0.00      0.00      0.00        65\n",
            "\n",
            "                            accuracy                           0.16      4197\n",
            "                           macro avg       0.00      0.01      0.00      4197\n",
            "                        weighted avg       0.03      0.16      0.04      4197\n",
            "\n",
            "\n",
            "Resumen de resultados:\n",
            "                    Accuracy  Macro-F1\n",
            "Word2Vec Frozen     0.493448  0.257780\n",
            "Word2Vec Fine-tune  0.181558  0.005892\n",
            "Word2Vec Scratch    0.161544  0.002600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Topic\n",
        "results_topic = run_w2v_experiment(df_train, \"topic\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Comparación de Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Embedings Tadicionales**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos una función para ejecutar con la variable objetivo que queramos\n",
        "def run_traditional_embeddings(df, target_col, min_samples_per_class=5):\n",
        "    print(f\"\\n===== Embeddings Tradicionales | Target: {target_col} =====\\n\")\n",
        "    \n",
        "    # Filtramos las clases raras\n",
        "    counts = df[target_col].value_counts()\n",
        "    valid_classes = counts[counts >= min_samples_per_class].index\n",
        "    df_filtered = df[df[target_col].isin(valid_classes)].copy()\n",
        "    \n",
        "    if df_filtered.empty:\n",
        "        print(f\"No hay suficientes datos para {target_col} después de filtrar clases raras.\")\n",
        "        return None\n",
        "    \n",
        "    # Preparamos los textos y etiquetas\n",
        "    df_filtered[\"text_joined\"] = df_filtered[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "    texts = df_filtered[\"text_joined\"].tolist()\n",
        "    y = df_filtered[target_col].values\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Hacemos el Train/Val/Test split\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        texts, y, test_size=0.15, random_state=42, stratify=y\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp\n",
        "    )\n",
        "    \n",
        "    # TF-IDF\n",
        "    tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "    X_val_tfidf = tfidf.transform(X_val)\n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    y_val_pred = clf.predict(X_val_tfidf)\n",
        "    y_test_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "    results[\"TF-IDF\"] = {\n",
        "        \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "        \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "        \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "        \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "    print(\"TF-IDF - Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "    \n",
        "    # Bag of Words\n",
        "    bow = CountVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
        "    X_train_bow = bow.fit_transform(X_train)\n",
        "    X_val_bow = bow.transform(X_val)\n",
        "    X_test_bow = bow.transform(X_test)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_bow, y_train)\n",
        "\n",
        "    y_val_pred = clf.predict(X_val_bow)\n",
        "    y_test_pred = clf.predict(X_test_bow)\n",
        "\n",
        "    results[\"Bag-of-Words\"] = {\n",
        "        \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "        \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "        \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "        \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "    print(\"Bag-of-Words - Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "    \n",
        "    # Resultado\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    print(\"\\nComparativa Embeddings Tradicionales:\")\n",
        "    print(results_df)\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Embeddings Tradicionales | Target: topic =====\n",
            "\n",
            "TF-IDF - Classification Report (Test):\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abortion       0.80      0.82      0.81        39\n",
            "                              africa       0.00      0.00      0.00         2\n",
            "                         agriculture       0.00      0.00      0.00         4\n",
            "                      animal_welfare       0.00      0.00      0.00         2\n",
            "              arts_and_entertainment       0.00      0.00      0.00         6\n",
            "                                asia       0.00      0.00      0.00         8\n",
            "                 banking_and_finance       0.54      0.54      0.54        24\n",
            "                    bridging_divides       0.00      0.00      0.00         9\n",
            "                            business       0.00      0.00      0.00        10\n",
            "                    campaign_finance       0.62      0.23      0.33        22\n",
            "                   campaign_rhetoric       0.00      0.00      0.00         5\n",
            "capital_punishment_and_death_penalty       0.00      0.00      0.00         1\n",
            "                               china       0.38      0.47      0.42        17\n",
            "                                 cia       0.50      0.29      0.36         7\n",
            "                        civil_rights       0.70      0.37      0.48        19\n",
            "                         coronavirus       0.63      0.83      0.72       120\n",
            "                    criminal_justice       0.50      0.38      0.43        13\n",
            "                             culture       0.42      0.23      0.30        35\n",
            "                       cybersecurity       0.29      0.17      0.21        12\n",
            "                       death_penalty       0.00      0.00      0.00         2\n",
            "                             defense       0.67      0.40      0.50        25\n",
            "                    democratic_party       0.00      0.00      0.00        25\n",
            "                            disaster       0.53      0.62      0.57        13\n",
            "                     domestic_policy       0.00      0.00      0.00         3\n",
            "                               ebola       0.50      0.70      0.58        10\n",
            "                     economic_policy       0.33      0.04      0.08        23\n",
            "                    economy_and_jobs       0.59      0.69      0.64        78\n",
            "                           education       0.73      0.74      0.74        47\n",
            "                           elections       0.67      0.91      0.77       678\n",
            "                              energy       0.33      0.11      0.17         9\n",
            "                         environment       0.66      0.86      0.75        66\n",
            "                                 epa       0.75      0.60      0.67         5\n",
            "                              europe       0.00      0.00      0.00        15\n",
            "             facts_and_fact_checking       0.00      0.00      0.00        10\n",
            "                           fake_news       0.33      0.12      0.18         8\n",
            "                 family_and_marriage       0.00      0.00      0.00         2\n",
            "                                 fbi       0.61      0.56      0.58        36\n",
            "                                 fda       0.00      0.00      0.00         1\n",
            "                      federal_budget       0.57      0.69      0.63        72\n",
            "                        fiscal_cliff       0.81      0.62      0.70        21\n",
            "                                food       0.00      0.00      0.00         3\n",
            "                      foreign_policy       0.35      0.20      0.26        60\n",
            "                         free_speech       0.44      0.40      0.42        30\n",
            "                        general_news       0.35      0.18      0.24        34\n",
            "                       great_britain       0.71      0.63      0.67        19\n",
            "          gun_control_and_gun_rights       0.86      0.93      0.89        97\n",
            "                          healthcare       0.80      0.88      0.84       162\n",
            "                            holidays       0.83      0.31      0.45        16\n",
            "                   homeland_security       1.00      0.07      0.13        14\n",
            "            housing_and_homelessness       0.00      0.00      0.00         3\n",
            "                         immigration       0.71      0.89      0.79       186\n",
            "                         impeachment       0.58      0.57      0.57        46\n",
            "                          inequality       0.00      0.00      0.00         5\n",
            "                                isis       0.40      0.42      0.41        19\n",
            "                              israel       0.54      0.47      0.50        15\n",
            "                             justice       0.38      0.14      0.21        35\n",
            "                  justice_department       0.50      0.41      0.45        32\n",
            "                               labor       0.67      0.13      0.22        15\n",
            "                         lgbt_rights       0.67      0.61      0.63        33\n",
            "              marijuana_legalization       1.00      0.75      0.86        12\n",
            "                          media_bias       0.52      0.59      0.55       124\n",
            "                      media_industry       0.00      0.00      0.00         4\n",
            "                              mexico       0.80      0.33      0.47        12\n",
            "                         middle_east       0.56      0.74      0.64       104\n",
            "                    national_defense       0.00      0.00      0.00        16\n",
            "                   national_security       0.42      0.27      0.33        62\n",
            "                         north_korea       0.73      0.97      0.83        36\n",
            "                                 nsa       0.69      0.74      0.71        27\n",
            "                     nuclear_weapons       0.00      0.00      0.00         4\n",
            "            obesity_and_malnutrition       0.00      0.00      0.00         1\n",
            "                       opioid_crisis       0.00      0.00      0.00         4\n",
            "                           palestine       0.00      0.00      0.00         4\n",
            "                        polarization       0.65      0.22      0.33        50\n",
            "                            politics       0.33      0.39      0.36       254\n",
            "                             privacy       0.00      0.00      0.00         6\n",
            "                       public_health       0.00      0.00      0.00         2\n",
            "                     race_and_racism       0.57      0.40      0.47        50\n",
            "                  religion_and_faith       0.76      0.54      0.63        24\n",
            "                    republican_party       0.67      0.10      0.17        63\n",
            "                  role_of_government       0.00      0.00      0.00         7\n",
            "                              russia       0.60      0.13      0.21        23\n",
            "                             science       0.00      0.00      0.00         3\n",
            "                   sexual_misconduct       0.22      0.12      0.16        16\n",
            "                     social_security       0.00      0.00      0.00         2\n",
            "                         south_korea       0.00      0.00      0.00         1\n",
            "                              sports       0.00      0.00      0.00        10\n",
            "                    state_department       0.56      0.43      0.49        23\n",
            "                       supreme_court       0.65      0.85      0.74        91\n",
            "                               taxes       0.60      0.68      0.64        44\n",
            "                           tea_party       1.00      0.33      0.50         6\n",
            "                          technology       0.31      0.41      0.35        27\n",
            "                           terrorism       0.49      0.65      0.56        55\n",
            "                               trade       0.72      0.70      0.71        44\n",
            "                      transportation       1.00      0.33      0.50         9\n",
            "                            treasury       0.25      0.12      0.17         8\n",
            "                         us_congress       0.30      0.15      0.20        59\n",
            "                     us_constitution       0.00      0.00      0.00         7\n",
            "                            us_house       0.38      0.37      0.38        75\n",
            "                         us_military       0.35      0.39      0.37        28\n",
            "                           us_senate       0.47      0.33      0.39        82\n",
            "                    veterans_affairs       0.64      0.88      0.74         8\n",
            "                 violence_in_america       0.54      0.74      0.63        74\n",
            "       voting_rights_and_voter_fraud       0.50      0.17      0.25        12\n",
            "                             welfare       0.00      0.00      0.00         2\n",
            "                         white_house       0.39      0.43      0.41       221\n",
            "                      women's_issues       0.00      0.00      0.00         8\n",
            "                               world       0.58      0.58      0.58        65\n",
            "\n",
            "                            accuracy                           0.58      4197\n",
            "                           macro avg       0.39      0.32      0.33      4197\n",
            "                        weighted avg       0.55      0.58      0.55      4197\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag-of-Words - Classification Report (Test):\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abortion       0.80      0.82      0.81        39\n",
            "                              africa       0.00      0.00      0.00         2\n",
            "                         agriculture       0.50      0.50      0.50         4\n",
            "                      animal_welfare       0.00      0.00      0.00         2\n",
            "              arts_and_entertainment       0.67      0.33      0.44         6\n",
            "                                asia       0.33      0.25      0.29         8\n",
            "                 banking_and_finance       0.48      0.46      0.47        24\n",
            "                    bridging_divides       0.00      0.00      0.00         9\n",
            "                            business       0.00      0.00      0.00        10\n",
            "                    campaign_finance       0.29      0.23      0.26        22\n",
            "                   campaign_rhetoric       0.00      0.00      0.00         5\n",
            "capital_punishment_and_death_penalty       0.00      0.00      0.00         1\n",
            "                               china       0.41      0.41      0.41        17\n",
            "                                 cia       0.67      0.57      0.62         7\n",
            "                        civil_rights       0.38      0.42      0.40        19\n",
            "                         coronavirus       0.62      0.76      0.68       120\n",
            "                    criminal_justice       0.31      0.31      0.31        13\n",
            "                             culture       0.39      0.31      0.35        35\n",
            "                       cybersecurity       0.45      0.42      0.43        12\n",
            "                       death_penalty       0.00      0.00      0.00         2\n",
            "                             defense       0.46      0.44      0.45        25\n",
            "                    democratic_party       0.25      0.20      0.22        25\n",
            "                            disaster       0.62      0.62      0.62        13\n",
            "                     domestic_policy       0.00      0.00      0.00         3\n",
            "                               ebola       0.64      0.70      0.67        10\n",
            "                     economic_policy       0.11      0.04      0.06        23\n",
            "                    economy_and_jobs       0.52      0.54      0.53        78\n",
            "                           education       0.79      0.70      0.74        47\n",
            "                           elections       0.70      0.81      0.75       678\n",
            "                              energy       0.50      0.44      0.47         9\n",
            "                         environment       0.71      0.71      0.71        66\n",
            "                                 epa       0.50      0.60      0.55         5\n",
            "                              europe       0.50      0.13      0.21        15\n",
            "             facts_and_fact_checking       0.50      0.20      0.29        10\n",
            "                           fake_news       0.33      0.25      0.29         8\n",
            "                 family_and_marriage       0.00      0.00      0.00         2\n",
            "                                 fbi       0.60      0.42      0.49        36\n",
            "                                 fda       0.00      0.00      0.00         1\n",
            "                      federal_budget       0.59      0.58      0.59        72\n",
            "                        fiscal_cliff       0.75      0.57      0.65        21\n",
            "                                food       0.40      0.67      0.50         3\n",
            "                      foreign_policy       0.29      0.28      0.29        60\n",
            "                         free_speech       0.54      0.50      0.52        30\n",
            "                        general_news       0.18      0.15      0.16        34\n",
            "                       great_britain       0.65      0.68      0.67        19\n",
            "          gun_control_and_gun_rights       0.85      0.85      0.85        97\n",
            "                          healthcare       0.78      0.80      0.79       162\n",
            "                            holidays       0.38      0.31      0.34        16\n",
            "                   homeland_security       0.50      0.21      0.30        14\n",
            "            housing_and_homelessness       0.50      0.33      0.40         3\n",
            "                         immigration       0.74      0.83      0.78       186\n",
            "                         impeachment       0.66      0.63      0.64        46\n",
            "                          inequality       0.33      0.20      0.25         5\n",
            "                                isis       0.47      0.37      0.41        19\n",
            "                              israel       0.56      0.33      0.42        15\n",
            "                             justice       0.35      0.26      0.30        35\n",
            "                  justice_department       0.36      0.38      0.37        32\n",
            "                               labor       0.33      0.13      0.19        15\n",
            "                         lgbt_rights       0.69      0.55      0.61        33\n",
            "              marijuana_legalization       1.00      0.67      0.80        12\n",
            "                          media_bias       0.43      0.48      0.46       124\n",
            "                      media_industry       0.00      0.00      0.00         4\n",
            "                              mexico       0.71      0.42      0.53        12\n",
            "                         middle_east       0.52      0.56      0.54       104\n",
            "                    national_defense       0.38      0.31      0.34        16\n",
            "                   national_security       0.35      0.27      0.31        62\n",
            "                         north_korea       0.77      0.75      0.76        36\n",
            "                                 nsa       0.65      0.81      0.72        27\n",
            "                     nuclear_weapons       0.00      0.00      0.00         4\n",
            "            obesity_and_malnutrition       0.00      0.00      0.00         1\n",
            "                       opioid_crisis       0.20      0.25      0.22         4\n",
            "                           palestine       1.00      0.50      0.67         4\n",
            "                        polarization       0.37      0.32      0.34        50\n",
            "                            politics       0.24      0.32      0.28       254\n",
            "                             privacy       1.00      0.17      0.29         6\n",
            "                       public_health       0.00      0.00      0.00         2\n",
            "                     race_and_racism       0.46      0.34      0.39        50\n",
            "                  religion_and_faith       0.69      0.46      0.55        24\n",
            "                    republican_party       0.29      0.16      0.21        63\n",
            "                  role_of_government       0.00      0.00      0.00         7\n",
            "                              russia       0.44      0.30      0.36        23\n",
            "                             science       0.50      0.67      0.57         3\n",
            "                   sexual_misconduct       0.33      0.25      0.29        16\n",
            "                     social_security       1.00      0.50      0.67         2\n",
            "                         south_korea       0.00      0.00      0.00         1\n",
            "                              sports       0.30      0.30      0.30        10\n",
            "                    state_department       0.57      0.57      0.57        23\n",
            "                       supreme_court       0.69      0.75      0.72        91\n",
            "                               taxes       0.60      0.59      0.60        44\n",
            "                           tea_party       0.50      0.33      0.40         6\n",
            "                          technology       0.38      0.48      0.43        27\n",
            "                           terrorism       0.43      0.51      0.47        55\n",
            "                               trade       0.76      0.70      0.73        44\n",
            "                      transportation       0.50      0.33      0.40         9\n",
            "                            treasury       0.50      0.25      0.33         8\n",
            "                         us_congress       0.21      0.19      0.20        59\n",
            "                     us_constitution       0.33      0.29      0.31         7\n",
            "                            us_house       0.41      0.33      0.37        75\n",
            "                         us_military       0.58      0.39      0.47        28\n",
            "                           us_senate       0.42      0.33      0.37        82\n",
            "                    veterans_affairs       0.45      0.62      0.53         8\n",
            "                 violence_in_america       0.60      0.62      0.61        74\n",
            "       voting_rights_and_voter_fraud       0.33      0.17      0.22        12\n",
            "                             welfare       0.50      0.50      0.50         2\n",
            "                         white_house       0.30      0.36      0.33       221\n",
            "                      women's_issues       0.14      0.12      0.13         8\n",
            "                               world       0.49      0.51      0.50        65\n",
            "\n",
            "                            accuracy                           0.54      4197\n",
            "                           macro avg       0.43      0.37      0.39      4197\n",
            "                        weighted avg       0.53      0.54      0.53      4197\n",
            "\n",
            "\n",
            "Comparativa Embeddings Tradicionales:\n",
            "              Val Accuracy  Test Accuracy  Val F1 (weighted)  \\\n",
            "TF-IDF            0.581844       0.583750           0.546214   \n",
            "Bag-of-Words      0.544198       0.539909           0.536001   \n",
            "\n",
            "              Test F1 (weighted)  \n",
            "TF-IDF                  0.547573  \n",
            "Bag-of-Words            0.530032  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Topic\n",
        "results_topic = run_traditional_embeddings(df_train, \"topic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Embeddings Tradicionales | Target: source =====\n",
            "\n",
            "TF-IDF - Classification Report (Test):\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abc news       0.00      0.00      0.00        28\n",
            "                   abc news (online)       0.00      0.00      0.00         1\n",
            "                          al jazeera       1.00      0.19      0.32        16\n",
            "allysia finley (wall street journal)       0.00      0.00      0.00         1\n",
            "                  american spectator       0.00      0.00      0.00        28\n",
            "                         ann coulter       0.00      0.00      0.00         1\n",
            "                       ap fact check       0.00      0.00      0.00         1\n",
            "                    associated press       0.75      0.26      0.38        35\n",
            "                               axios       0.00      0.00      0.00         9\n",
            "                            bbc news       0.76      0.55      0.64        91\n",
            "                         ben shapiro       0.00      0.00      0.00         3\n",
            "                           bloomberg       0.45      0.25      0.32        20\n",
            "                      breitbart news       1.00      0.04      0.09        45\n",
            "                    business insider       0.00      0.00      0.00         9\n",
            "                       buzzfeed news       0.00      0.00      0.00         7\n",
            "                                 cbn       0.53      0.16      0.25        50\n",
            "                            cbs news       0.00      0.00      0.00        19\n",
            "                 charles krauthammer       0.00      0.00      0.00         1\n",
            "                   chicago sun-times       0.00      0.00      0.00         9\n",
            "           christian science monitor       0.93      0.95      0.94       157\n",
            "                                cnbc       0.00      0.00      0.00        13\n",
            "                      cnn (web news)       0.69      0.80      0.74       335\n",
            "                     cnn - editorial       0.00      0.00      0.00        10\n",
            "                         daily beast       0.00      0.00      0.00        29\n",
            "                           daily kos       0.00      0.00      0.00        15\n",
            "                          daily mail       0.00      0.00      0.00         6\n",
            "                        damon linker       0.00      0.00      0.00         1\n",
            "                       democracy now       1.00      0.78      0.88         9\n",
            "                          ezra klein       0.00      0.00      0.00         1\n",
            "                     fivethirtyeight       1.00      0.09      0.17        11\n",
            "                            fox news       0.39      0.37      0.38       165\n",
            "                   fox news (online)       0.00      0.00      0.00        10\n",
            "                    fox news opinion       0.00      0.00      0.00         6\n",
            "                     fox online news       0.34      0.62      0.44       242\n",
            "                         george will       0.00      0.00      0.00         2\n",
            "                        guest writer       0.00      0.00      0.00        10\n",
            "                 guest writer - left       0.00      0.00      0.00        22\n",
            "                guest writer - right       0.00      0.00      0.00        53\n",
            "                              hotair       0.00      0.00      0.00         9\n",
            "                        howard kurtz       0.00      0.00      0.00         2\n",
            "        international business times       0.00      0.00      0.00         5\n",
            "                             jacobin       0.00      0.00      0.00         3\n",
            "                           john fund       0.00      0.00      0.00         2\n",
            "                        john stossel       0.00      0.00      0.00         4\n",
            "                      jonah goldberg       0.00      0.00      0.00         1\n",
            "                       juan williams       0.00      0.00      0.00         1\n",
            "                      julian zelizer       0.00      0.00      0.00         1\n",
            "                         marketwatch       0.67      0.15      0.25        13\n",
            "                       media matters       0.00      0.00      0.00        13\n",
            "               media research center       0.00      0.00      0.00         3\n",
            "           michael brendan dougherty       0.00      0.00      0.00         1\n",
            "                     michelle malkin       0.00      0.00      0.00         1\n",
            "                        mother jones       0.00      0.00      0.00        13\n",
            "                     national review       0.34      0.59      0.44       122\n",
            "                   nbc news (online)       0.00      0.00      0.00         5\n",
            "                         nbcnews.com       0.00      0.00      0.00         2\n",
            "                   new york magazine       0.00      0.00      0.00         6\n",
            "                       new york post       0.00      0.00      0.00        20\n",
            "             new york post (opinion)       0.00      0.00      0.00         1\n",
            "        new york times (online news)       0.00      0.00      0.00         3\n",
            "               new york times - news       0.71      0.42      0.53       167\n",
            "            new york times - opinion       0.00      0.00      0.00         8\n",
            "                         newsbusters       0.00      0.00      0.00         5\n",
            "                             newsmax       0.33      0.01      0.03        69\n",
            "                      newsmax (news)       0.00      0.00      0.00         1\n",
            "                      newsmax - news       0.00      0.00      0.00         8\n",
            "                   newsmax - opinion       0.00      0.00      0.00         4\n",
            "                       newt gingrich       0.00      0.00      0.00         1\n",
            "                       npr editorial       0.00      0.00      0.00         1\n",
            "                     npr online news       0.47      0.60      0.53       242\n",
            "                 pew research center       0.00      0.00      0.00         4\n",
            "                            politico       0.42      0.76      0.54       300\n",
            "                          propublica       0.00      0.00      0.00         8\n",
            "                           rand paul       0.00      0.00      0.00         1\n",
            "                   realclearpolitics       0.00      0.00      0.00         5\n",
            "                              reason       0.46      0.21      0.29        58\n",
            "                             reuters       0.61      0.60      0.61        98\n",
            "                          rich lowry       0.00      0.00      0.00         6\n",
            "                         ryan cooper       0.00      0.00      0.00         1\n",
            "                               salon       0.55      0.30      0.39        70\n",
            "                 scientific american       0.00      0.00      0.00         5\n",
            "                               slate       0.00      0.00      0.00        18\n",
            "                        the atlantic       0.00      0.00      0.00        22\n",
            "                    the boston globe       0.00      0.00      0.00         3\n",
            "                    the daily caller       0.00      0.00      0.00        27\n",
            "                      the daily wire       0.00      0.00      0.00        14\n",
            "                       the economist       0.00      0.00      0.00         3\n",
            "                     the epoch times       0.00      0.00      0.00         4\n",
            "                       the flip side       0.90      0.32      0.47        28\n",
            "                        the guardian       0.48      0.15      0.23        66\n",
            "                            the hill       0.73      0.77      0.75       163\n",
            "                       the intercept       0.00      0.00      0.00         5\n",
            "                the marshall project       0.00      0.00      0.00         3\n",
            "                          the nation       0.00      0.00      0.00         4\n",
            "                      the new yorker       0.00      0.00      0.00         3\n",
            "                     the week - news       0.00      0.00      0.00        15\n",
            "                  the week - opinion       0.00      0.00      0.00         3\n",
            "                        theblaze.com       0.00      0.00      0.00        24\n",
            "                       thinkprogress       0.00      0.00      0.00         4\n",
            "                       time magazine       0.00      0.00      0.00         8\n",
            "                            townhall       0.34      0.46      0.39       153\n",
            "                           usa today       0.24      0.44      0.31       216\n",
            "                         vanity fair       0.00      0.00      0.00        19\n",
            "                                vice       0.00      0.00      0.00         8\n",
            "                       victor hanson       0.00      0.00      0.00         8\n",
            "                                 vox       0.25      0.49      0.33       174\n",
            "     wall street journal - editorial       0.00      0.00      0.00         1\n",
            "          wall street journal - news       1.00      0.02      0.05        42\n",
            "              washington free beacon       0.00      0.00      0.00         4\n",
            "                     washington post       0.00      0.00      0.00        13\n",
            "                    washington times       0.66      0.85      0.74       349\n",
            "                         yahoo! news       0.00      0.00      0.00         1\n",
            "                      yahoo! the 360       0.00      0.00      0.00         9\n",
            "\n",
            "                            accuracy                           0.48      4178\n",
            "                           macro avg       0.16      0.11      0.11      4178\n",
            "                        weighted avg       0.45      0.48      0.43      4178\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag-of-Words - Classification Report (Test):\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                            abc news       0.31      0.14      0.20        28\n",
            "                   abc news (online)       0.00      0.00      0.00         1\n",
            "                          al jazeera       0.75      0.19      0.30        16\n",
            "allysia finley (wall street journal)       0.00      0.00      0.00         1\n",
            "                  american spectator       0.29      0.18      0.22        28\n",
            "                         ann coulter       0.00      0.00      0.00         1\n",
            "                       ap fact check       0.00      0.00      0.00         1\n",
            "                    associated press       0.68      0.54      0.60        35\n",
            "                               axios       0.33      0.11      0.17         9\n",
            "                            bbc news       0.70      0.68      0.69        91\n",
            "                         ben shapiro       0.00      0.00      0.00         3\n",
            "                           bloomberg       0.83      0.50      0.62        20\n",
            "                      breitbart news       0.20      0.22      0.21        45\n",
            "                    business insider       0.00      0.00      0.00         9\n",
            "                       buzzfeed news       0.33      0.14      0.20         7\n",
            "                                 cbn       0.52      0.50      0.51        50\n",
            "                            cbs news       0.25      0.05      0.09        19\n",
            "                 charles krauthammer       0.00      0.00      0.00         1\n",
            "                   chicago sun-times       1.00      0.11      0.20         9\n",
            "           christian science monitor       0.94      0.95      0.95       157\n",
            "                                cnbc       0.25      0.08      0.12        13\n",
            "                      cnn (web news)       0.80      0.76      0.78       335\n",
            "                     cnn - editorial       1.00      0.30      0.46        10\n",
            "                         daily beast       0.11      0.10      0.11        29\n",
            "                           daily kos       0.42      0.33      0.37        15\n",
            "                          daily mail       0.00      0.00      0.00         6\n",
            "                        damon linker       0.00      0.00      0.00         1\n",
            "                       democracy now       0.83      0.56      0.67         9\n",
            "                          ezra klein       0.00      0.00      0.00         1\n",
            "                     fivethirtyeight       1.00      0.36      0.53        11\n",
            "                            fox news       0.31      0.41      0.35       165\n",
            "                   fox news (online)       0.00      0.00      0.00        10\n",
            "                    fox news opinion       0.00      0.00      0.00         6\n",
            "                     fox online news       0.40      0.56      0.47       242\n",
            "                         george will       0.00      0.00      0.00         2\n",
            "                        guest writer       0.50      0.10      0.17        10\n",
            "                 guest writer - left       0.10      0.05      0.06        22\n",
            "                guest writer - right       0.12      0.11      0.12        53\n",
            "                              hotair       1.00      0.11      0.20         9\n",
            "                        howard kurtz       0.00      0.00      0.00         2\n",
            "        international business times       0.00      0.00      0.00         5\n",
            "                             jacobin       0.00      0.00      0.00         3\n",
            "                           john fund       0.00      0.00      0.00         2\n",
            "                        john stossel       0.00      0.00      0.00         4\n",
            "                      jonah goldberg       0.00      0.00      0.00         1\n",
            "                       juan williams       0.00      0.00      0.00         1\n",
            "                      julian zelizer       0.00      0.00      0.00         1\n",
            "                         marketwatch       0.33      0.15      0.21        13\n",
            "                       media matters       0.33      0.08      0.12        13\n",
            "               media research center       0.00      0.00      0.00         3\n",
            "           michael brendan dougherty       0.00      0.00      0.00         1\n",
            "                     michelle malkin       0.00      0.00      0.00         1\n",
            "                        mother jones       0.00      0.00      0.00        13\n",
            "                     national review       0.46      0.54      0.49       122\n",
            "                   nbc news (online)       0.67      0.40      0.50         5\n",
            "                         nbcnews.com       0.00      0.00      0.00         2\n",
            "                   new york magazine       0.00      0.00      0.00         6\n",
            "                       new york post       0.11      0.05      0.07        20\n",
            "             new york post (opinion)       0.00      0.00      0.00         1\n",
            "        new york times (online news)       0.00      0.00      0.00         3\n",
            "               new york times - news       0.64      0.76      0.69       167\n",
            "            new york times - opinion       0.00      0.00      0.00         8\n",
            "                         newsbusters       0.50      0.20      0.29         5\n",
            "                             newsmax       0.19      0.20      0.20        69\n",
            "                      newsmax (news)       0.00      0.00      0.00         1\n",
            "                      newsmax - news       0.00      0.00      0.00         8\n",
            "                   newsmax - opinion       1.00      0.75      0.86         4\n",
            "                       newt gingrich       0.00      0.00      0.00         1\n",
            "                       npr editorial       0.00      0.00      0.00         1\n",
            "                     npr online news       0.46      0.53      0.49       242\n",
            "                 pew research center       1.00      0.50      0.67         4\n",
            "                            politico       0.54      0.66      0.60       300\n",
            "                          propublica       0.00      0.00      0.00         8\n",
            "                           rand paul       0.00      0.00      0.00         1\n",
            "                   realclearpolitics       0.00      0.00      0.00         5\n",
            "                              reason       0.33      0.36      0.35        58\n",
            "                             reuters       0.72      0.74      0.73        98\n",
            "                          rich lowry       0.00      0.00      0.00         6\n",
            "                         ryan cooper       0.00      0.00      0.00         1\n",
            "                               salon       0.50      0.46      0.48        70\n",
            "                 scientific american       1.00      0.20      0.33         5\n",
            "                               slate       0.08      0.06      0.07        18\n",
            "                        the atlantic       0.19      0.14      0.16        22\n",
            "                    the boston globe       0.00      0.00      0.00         3\n",
            "                    the daily caller       0.20      0.19      0.19        27\n",
            "                      the daily wire       0.00      0.00      0.00        14\n",
            "                       the economist       0.00      0.00      0.00         3\n",
            "                     the epoch times       0.00      0.00      0.00         4\n",
            "                       the flip side       0.78      0.64      0.71        28\n",
            "                        the guardian       0.74      0.56      0.64        66\n",
            "                            the hill       0.78      0.76      0.77       163\n",
            "                       the intercept       0.00      0.00      0.00         5\n",
            "                the marshall project       0.00      0.00      0.00         3\n",
            "                          the nation       1.00      0.75      0.86         4\n",
            "                      the new yorker       0.00      0.00      0.00         3\n",
            "                     the week - news       0.00      0.00      0.00        15\n",
            "                  the week - opinion       0.00      0.00      0.00         3\n",
            "                        theblaze.com       0.18      0.12      0.15        24\n",
            "                       thinkprogress       0.00      0.00      0.00         4\n",
            "                       time magazine       1.00      0.38      0.55         8\n",
            "                            townhall       0.36      0.52      0.43       153\n",
            "                           usa today       0.31      0.39      0.34       216\n",
            "                         vanity fair       0.22      0.11      0.14        19\n",
            "                                vice       0.00      0.00      0.00         8\n",
            "                       victor hanson       1.00      0.50      0.67         8\n",
            "                                 vox       0.46      0.47      0.46       174\n",
            "     wall street journal - editorial       0.00      0.00      0.00         1\n",
            "          wall street journal - news       0.81      0.52      0.64        42\n",
            "              washington free beacon       0.00      0.00      0.00         4\n",
            "                     washington post       0.10      0.08      0.09        13\n",
            "                    washington times       0.73      0.78      0.75       349\n",
            "                         yahoo! news       0.00      0.00      0.00         1\n",
            "                      yahoo! the 360       1.00      0.67      0.80         9\n",
            "\n",
            "                            accuracy                           0.53      4178\n",
            "                           macro avg       0.29      0.20      0.22      4178\n",
            "                        weighted avg       0.52      0.53      0.51      4178\n",
            "\n",
            "\n",
            "Comparativa Embeddings Tradicionales:\n",
            "              Val Accuracy  Test Accuracy  Val F1 (weighted)  \\\n",
            "TF-IDF            0.483848       0.482049           0.438332   \n",
            "Bag-of-Words      0.529553       0.526807           0.515714   \n",
            "\n",
            "              Test F1 (weighted)  \n",
            "TF-IDF                  0.433240  \n",
            "Bag-of-Words            0.512062  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Source\n",
        "results_source = run_traditional_embeddings(df_train, \"source\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Embeddings contextuales**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos una función para ejecutar con la variable objetivo que queramos\n",
        "def run_contextual_embeddings(df, target_col, min_samples_per_class=5):\n",
        "    print(f\"\\n===== Embeddings Contextuales | Target: {target_col} =====\\n\")\n",
        "\n",
        "    # Filtramos las clases raras\n",
        "    counts = df[target_col].value_counts()\n",
        "    valid_classes = counts[counts >= min_samples_per_class].index\n",
        "    df_filtered = df[df[target_col].isin(valid_classes)].copy()\n",
        "    \n",
        "    if df_filtered.empty:\n",
        "        print(\"No hay suficientes datos después de filtrar clases raras.\")\n",
        "        return None\n",
        "\n",
        "    df_filtered[\"text_joined\"] = df_filtered[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "    texts = df_filtered[\"text_joined\"].tolist()\n",
        "    labels = df_filtered[target_col].tolist()\n",
        "\n",
        "    # Codificamos las etiquetas\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(labels)\n",
        "\n",
        "    # Hacemos el Train/Val/Test split\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        texts, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Sentence Transformers\n",
        "    st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    X_train_vec = st_model.encode(X_train, batch_size=32, show_progress_bar=True)\n",
        "    X_val_vec = st_model.encode(X_val, batch_size=32)\n",
        "    X_test_vec = st_model.encode(X_test, batch_size=32)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "    y_val_pred = clf.predict(X_val_vec)\n",
        "    y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "    results[\"Sentence Transformers\"] = {\n",
        "        \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "        \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "        \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "        \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "    print(\"Sentence Transformers - Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    # Bert\n",
        "    bert_model_name = \"bert-base-uncased\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "    bert_model = BertModel.from_pretrained(bert_model_name)\n",
        "    bert_model.eval()\n",
        "\n",
        "    def bert_sentence_embedding(text):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "            return embeddings.mean(dim=0).numpy()\n",
        "\n",
        "    # Limitamos el número de textos por memoria\n",
        "    X_train_subset = X_train[:100]\n",
        "    X_val_subset = X_val[:20]\n",
        "    X_test_subset = X_test[:20]\n",
        "    y_train_subset = y_train[:100]\n",
        "    y_val_subset = y_val[:20]\n",
        "    y_test_subset = y_test[:20]\n",
        "\n",
        "    X_train_vec = np.array([bert_sentence_embedding(t) for t in X_train_subset])\n",
        "    X_val_vec = np.array([bert_sentence_embedding(t) for t in X_val_subset])\n",
        "    X_test_vec = np.array([bert_sentence_embedding(t) for t in X_test_subset])\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_vec, y_train_subset)\n",
        "    y_val_pred = clf.predict(X_val_vec)\n",
        "    y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "    results[\"BERT\"] = {\n",
        "        \"Val Accuracy\": accuracy_score(y_val_subset, y_val_pred),\n",
        "        \"Test Accuracy\": accuracy_score(y_test_subset, y_test_pred),\n",
        "        \"Val F1 (weighted)\": f1_score(y_val_subset, y_val_pred, average=\"weighted\"),\n",
        "        \"Test F1 (weighted)\": f1_score(y_test_subset, y_test_pred, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "    print(\"BERT - Classification Report (Test):\")\n",
        "    print(classification_report(y_test_subset, y_test_pred))\n",
        "\n",
        "    # Resultados\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    print(\"\\nComparativa Embeddings Contextuales:\")\n",
        "    print(results_df)\n",
        "\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Embeddings Contextuales | Target: topic =====\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe063986b6764819ba5c36bf71fe114e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/612 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Transformers - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.92      0.86        39\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         6\n",
            "           5       0.50      0.12      0.20         8\n",
            "           6       0.42      0.42      0.42        24\n",
            "           7       0.00      0.00      0.00         9\n",
            "           8       0.00      0.00      0.00        10\n",
            "           9       1.00      0.18      0.31        22\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         1\n",
            "          12       0.32      0.41      0.36        17\n",
            "          13       0.75      0.43      0.55         7\n",
            "          14       0.60      0.16      0.25        19\n",
            "          15       0.62      0.81      0.70       120\n",
            "          16       0.71      0.38      0.50        13\n",
            "          17       0.20      0.14      0.17        35\n",
            "          18       0.20      0.17      0.18        12\n",
            "          19       0.00      0.00      0.00         2\n",
            "          20       0.64      0.36      0.46        25\n",
            "          21       0.00      0.00      0.00        25\n",
            "          22       0.53      0.62      0.57        13\n",
            "          23       0.00      0.00      0.00         3\n",
            "          24       0.57      0.80      0.67        10\n",
            "          25       0.33      0.04      0.08        23\n",
            "          26       0.53      0.69      0.60        78\n",
            "          27       0.64      0.72      0.68        47\n",
            "          28       0.63      0.86      0.73       678\n",
            "          29       0.00      0.00      0.00         9\n",
            "          30       0.58      0.77      0.66        66\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.67      0.13      0.22        15\n",
            "          33       0.00      0.00      0.00        10\n",
            "          34       0.00      0.00      0.00         8\n",
            "          35       0.00      0.00      0.00         2\n",
            "          36       0.53      0.44      0.48        36\n",
            "          37       0.00      0.00      0.00         1\n",
            "          38       0.52      0.71      0.60        72\n",
            "          39       0.73      0.52      0.61        21\n",
            "          40       0.00      0.00      0.00         3\n",
            "          41       0.27      0.22      0.24        60\n",
            "          42       0.44      0.27      0.33        30\n",
            "          43       0.22      0.12      0.15        34\n",
            "          44       0.67      0.74      0.70        19\n",
            "          45       0.81      0.89      0.85        97\n",
            "          46       0.72      0.90      0.80       162\n",
            "          47       0.75      0.19      0.30        16\n",
            "          48       0.50      0.07      0.12        14\n",
            "          49       0.00      0.00      0.00         3\n",
            "          50       0.67      0.85      0.75       186\n",
            "          51       0.54      0.54      0.54        46\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.40      0.21      0.28        19\n",
            "          54       0.57      0.53      0.55        15\n",
            "          55       0.12      0.03      0.05        35\n",
            "          56       0.55      0.38      0.44        32\n",
            "          57       0.50      0.13      0.21        15\n",
            "          58       0.62      0.55      0.58        33\n",
            "          59       0.91      0.83      0.87        12\n",
            "          60       0.41      0.50      0.45       124\n",
            "          61       0.00      0.00      0.00         4\n",
            "          62       0.67      0.17      0.27        12\n",
            "          63       0.44      0.63      0.52       104\n",
            "          64       0.00      0.00      0.00        16\n",
            "          65       0.31      0.18      0.22        62\n",
            "          66       0.68      0.94      0.79        36\n",
            "          67       0.64      0.85      0.73        27\n",
            "          68       0.00      0.00      0.00         4\n",
            "          69       0.00      0.00      0.00         1\n",
            "          70       1.00      0.25      0.40         4\n",
            "          71       0.00      0.00      0.00         4\n",
            "          72       0.59      0.26      0.36        50\n",
            "          73       0.26      0.23      0.24       254\n",
            "          74       0.00      0.00      0.00         6\n",
            "          75       0.00      0.00      0.00         2\n",
            "          76       0.35      0.34      0.35        50\n",
            "          77       0.52      0.46      0.49        24\n",
            "          78       0.42      0.08      0.13        63\n",
            "          79       0.00      0.00      0.00         7\n",
            "          80       0.29      0.09      0.13        23\n",
            "          81       0.00      0.00      0.00         3\n",
            "          82       0.44      0.25      0.32        16\n",
            "          83       0.00      0.00      0.00         2\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.38      0.30      0.33        10\n",
            "          86       0.56      0.22      0.31        23\n",
            "          87       0.62      0.79      0.70        91\n",
            "          88       0.60      0.61      0.61        44\n",
            "          89       0.00      0.00      0.00         6\n",
            "          90       0.44      0.41      0.42        27\n",
            "          91       0.48      0.62      0.54        55\n",
            "          92       0.67      0.68      0.67        44\n",
            "          93       0.71      0.56      0.62         9\n",
            "          94       0.00      0.00      0.00         8\n",
            "          95       0.20      0.07      0.10        59\n",
            "          96       0.00      0.00      0.00         7\n",
            "          97       0.33      0.35      0.34        75\n",
            "          98       0.39      0.43      0.41        28\n",
            "          99       0.43      0.37      0.40        82\n",
            "         100       1.00      0.12      0.22         8\n",
            "         101       0.50      0.69      0.58        74\n",
            "         102       0.00      0.00      0.00        12\n",
            "         103       0.00      0.00      0.00         2\n",
            "         104       0.34      0.40      0.37       221\n",
            "         105       0.00      0.00      0.00         8\n",
            "         106       0.46      0.46      0.46        65\n",
            "\n",
            "    accuracy                           0.54      4197\n",
            "   macro avg       0.35      0.29      0.29      4197\n",
            "weighted avg       0.49      0.54      0.50      4197\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2       0.00      0.00      0.00         1\n",
            "          15       0.00      0.00      0.00         0\n",
            "          26       0.00      0.00      0.00         1\n",
            "          28       0.83      1.00      0.91         5\n",
            "          29       0.00      0.00      0.00         1\n",
            "          38       0.00      0.00      0.00         1\n",
            "          44       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         0\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.00      0.00      0.00         1\n",
            "          59       0.00      0.00      0.00         1\n",
            "          60       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00         1\n",
            "          72       0.00      0.00      0.00         1\n",
            "          73       0.00      0.00      0.00         0\n",
            "          76       0.00      0.00      0.00         1\n",
            "          95       0.00      0.00      0.00         1\n",
            "          96       0.00      0.00      0.00         1\n",
            "          97       0.00      0.00      0.00         0\n",
            "          98       1.00      1.00      1.00         1\n",
            "          99       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.30        20\n",
            "   macro avg       0.09      0.10      0.09        20\n",
            "weighted avg       0.26      0.30      0.28        20\n",
            "\n",
            "\n",
            "Comparativa Embeddings Contextuales:\n",
            "                       Val Accuracy  Test Accuracy  Val F1 (weighted)  \\\n",
            "Sentence Transformers      0.534429       0.536812           0.493386   \n",
            "BERT                       0.400000       0.300000           0.291765   \n",
            "\n",
            "                       Test F1 (weighted)  \n",
            "Sentence Transformers            0.496460  \n",
            "BERT                             0.277273  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Topic\n",
        "results_topic = run_contextual_embeddings(df_train, \"topic\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Embeddings Contextuales | Target: source =====\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e07361e39e7e4a3a85ee7036a2fecda8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/610 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Transformers - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        28\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.57      0.25      0.35        16\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00        28\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       1.00      0.03      0.06        35\n",
            "           8       0.00      0.00      0.00         9\n",
            "           9       0.30      0.38      0.34        91\n",
            "          10       0.00      0.00      0.00         3\n",
            "          11       0.22      0.10      0.14        20\n",
            "          12       0.00      0.00      0.00        45\n",
            "          13       0.00      0.00      0.00         9\n",
            "          14       0.00      0.00      0.00         7\n",
            "          15       0.10      0.02      0.03        50\n",
            "          16       0.00      0.00      0.00        19\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.00      0.00      0.00         9\n",
            "          19       0.20      0.17      0.18       157\n",
            "          20       0.00      0.00      0.00        13\n",
            "          21       0.36      0.61      0.46       335\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.00      0.00      0.00        29\n",
            "          24       0.00      0.00      0.00        15\n",
            "          25       0.00      0.00      0.00         6\n",
            "          26       0.00      0.00      0.00         1\n",
            "          27       1.00      0.11      0.20         9\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.00      0.00      0.00        11\n",
            "          30       0.32      0.15      0.20       165\n",
            "          31       0.00      0.00      0.00        10\n",
            "          32       0.00      0.00      0.00         6\n",
            "          33       0.22      0.33      0.27       242\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.00      0.00      0.00        10\n",
            "          36       0.00      0.00      0.00        22\n",
            "          37       0.14      0.02      0.03        53\n",
            "          38       0.00      0.00      0.00         9\n",
            "          39       0.00      0.00      0.00         2\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         3\n",
            "          42       0.00      0.00      0.00         2\n",
            "          43       0.00      0.00      0.00         4\n",
            "          44       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         1\n",
            "          47       0.00      0.00      0.00        13\n",
            "          48       0.00      0.00      0.00        13\n",
            "          49       0.00      0.00      0.00         3\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.00      0.00      0.00         1\n",
            "          52       0.00      0.00      0.00        13\n",
            "          53       0.22      0.31      0.26       122\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         2\n",
            "          56       0.00      0.00      0.00         6\n",
            "          57       0.00      0.00      0.00        20\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.00      0.00      0.00         3\n",
            "          60       0.32      0.36      0.34       167\n",
            "          61       0.00      0.00      0.00         8\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00        69\n",
            "          64       0.00      0.00      0.00         1\n",
            "          65       0.00      0.00      0.00         8\n",
            "          66       0.00      0.00      0.00         4\n",
            "          67       0.00      0.00      0.00         1\n",
            "          68       0.00      0.00      0.00         1\n",
            "          69       0.14      0.13      0.14       242\n",
            "          70       0.00      0.00      0.00         4\n",
            "          71       0.27      0.53      0.35       300\n",
            "          72       0.00      0.00      0.00         8\n",
            "          73       0.00      0.00      0.00         1\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.16      0.09      0.11        58\n",
            "          76       0.38      0.34      0.36        98\n",
            "          77       0.00      0.00      0.00         6\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.21      0.14      0.17        70\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00        18\n",
            "          82       0.00      0.00      0.00        22\n",
            "          83       0.00      0.00      0.00         3\n",
            "          84       0.00      0.00      0.00        27\n",
            "          85       0.00      0.00      0.00        14\n",
            "          86       0.00      0.00      0.00         3\n",
            "          87       0.00      0.00      0.00         4\n",
            "          88       0.88      0.25      0.39        28\n",
            "          89       0.00      0.00      0.00        66\n",
            "          90       0.41      0.61      0.49       163\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         3\n",
            "          93       0.00      0.00      0.00         4\n",
            "          94       0.00      0.00      0.00         3\n",
            "          95       0.00      0.00      0.00        15\n",
            "          96       0.00      0.00      0.00         3\n",
            "          97       0.00      0.00      0.00        24\n",
            "          98       0.00      0.00      0.00         4\n",
            "          99       0.00      0.00      0.00         8\n",
            "         100       0.20      0.17      0.18       153\n",
            "         101       0.23      0.33      0.27       216\n",
            "         102       0.00      0.00      0.00        19\n",
            "         103       0.00      0.00      0.00         8\n",
            "         104       0.00      0.00      0.00         8\n",
            "         105       0.16      0.22      0.19       174\n",
            "         106       0.00      0.00      0.00         1\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.00      0.00      0.00         4\n",
            "         109       0.00      0.00      0.00        13\n",
            "         110       0.30      0.54      0.38       349\n",
            "         111       0.00      0.00      0.00         1\n",
            "         112       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.27      4178\n",
            "   macro avg       0.07      0.05      0.05      4178\n",
            "weighted avg       0.22      0.27      0.23      4178\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2       0.00      0.00      0.00         1\n",
            "           9       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         1\n",
            "          21       0.17      1.00      0.29         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          30       0.00      0.00      0.00         0\n",
            "          33       0.00      0.00      0.00         0\n",
            "          36       0.00      0.00      0.00         1\n",
            "          47       0.00      0.00      0.00         1\n",
            "          53       0.00      0.00      0.00         2\n",
            "          60       0.00      0.00      0.00         0\n",
            "          63       0.00      0.00      0.00         2\n",
            "          69       0.00      0.00      0.00         0\n",
            "          71       0.00      0.00      0.00         3\n",
            "          76       0.00      0.00      0.00         1\n",
            "          90       0.00      0.00      0.00         1\n",
            "         100       0.00      0.00      0.00         1\n",
            "         101       0.00      0.00      0.00         0\n",
            "         110       0.50      0.50      0.50         2\n",
            "\n",
            "    accuracy                           0.10        20\n",
            "   macro avg       0.03      0.07      0.04        20\n",
            "weighted avg       0.06      0.10      0.06        20\n",
            "\n",
            "\n",
            "Comparativa Embeddings Contextuales:\n",
            "                       Val Accuracy  Test Accuracy  Val F1 (weighted)  \\\n",
            "Sentence Transformers      0.264657       0.274055           0.222000   \n",
            "BERT                       0.150000       0.100000           0.120833   \n",
            "\n",
            "                       Test F1 (weighted)  \n",
            "Sentence Transformers            0.227381  \n",
            "BERT                             0.064286  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Source\n",
        "results_source = run_contextual_embeddings(df_train, \"source\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Embeddings no Contextuales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos una función para ejecutar con la variable objetivo que queramos\n",
        "def run_non_contextual_embeddings(df, target_col, min_samples_per_class=5):\n",
        "    print(f\"\\n===== Embeddings No Contextuales | Target: {target_col} =====\\n\")\n",
        "    \n",
        "    # Filtramos las clases raras\n",
        "    counts = df[target_col].value_counts()\n",
        "    valid_classes = counts[counts >= min_samples_per_class].index\n",
        "    df_filtered = df[df[target_col].isin(valid_classes)].copy()\n",
        "    \n",
        "    if df_filtered.empty:\n",
        "        print(\"No hay suficientes datos después de filtrar clases raras.\")\n",
        "        return None\n",
        "\n",
        "    df_filtered[\"text_joined\"] = df_filtered[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "    sentences = df_filtered[\"tokens\"].tolist()\n",
        "    labels = df_filtered[target_col].tolist()\n",
        "    \n",
        "    # Hacemos el Train/Val/Test split\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(labels)\n",
        "    \n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        sentences, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp\n",
        "    )\n",
        "    \n",
        "    results = {}\n",
        "\n",
        "    # Word2Vec\n",
        "    w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=3, workers=4, sg=1)\n",
        "    \n",
        "    def get_avg_w2v(sentence, model):\n",
        "        vecs = [model.wv[word] for word in sentence if word in model.wv]\n",
        "        if len(vecs) == 0:\n",
        "            return np.zeros(model.vector_size)\n",
        "        return np.mean(vecs, axis=0)\n",
        "    \n",
        "    X_train_vec = np.array([get_avg_w2v(s, w2v_model) for s in X_train])\n",
        "    X_val_vec = np.array([get_avg_w2v(s, w2v_model) for s in X_val])\n",
        "    X_test_vec = np.array([get_avg_w2v(s, w2v_model) for s in X_test])\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "    y_val_pred = clf.predict(X_val_vec)\n",
        "    y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "    results[\"Word2Vec\"] = {\n",
        "        \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "        \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "        \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "        \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "    print(\"Word2Vec - Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    # FastText\n",
        "    fasttext_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=3, workers=4, sg=1)\n",
        "\n",
        "    X_train_vec = np.array([get_avg_w2v(s, fasttext_model) for s in X_train])\n",
        "    X_val_vec = np.array([get_avg_w2v(s, fasttext_model) for s in X_val])\n",
        "    X_test_vec = np.array([get_avg_w2v(s, fasttext_model) for s in X_test])\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "    y_val_pred = clf.predict(X_val_vec)\n",
        "    y_test_pred = clf.predict(X_test_vec)\n",
        "\n",
        "    results[\"FastText\"] = {\n",
        "        \"Val Accuracy\": accuracy_score(y_val, y_val_pred),\n",
        "        \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
        "        \"Val F1 (weighted)\": f1_score(y_val, y_val_pred, average=\"weighted\"),\n",
        "        \"Test F1 (weighted)\": f1_score(y_test, y_test_pred, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "    print(\"FastText - Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    # Resultados\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    print(\"\\nComparativa Embeddings No Contextuales:\")\n",
        "    print(results_df)\n",
        "    \n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Embeddings No Contextuales | Target: topic =====\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84        39\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         6\n",
            "           5       0.00      0.00      0.00         8\n",
            "           6       0.35      0.25      0.29        24\n",
            "           7       0.00      0.00      0.00         9\n",
            "           8       0.00      0.00      0.00        10\n",
            "           9       0.50      0.09      0.15        22\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         1\n",
            "          12       0.22      0.12      0.15        17\n",
            "          13       0.00      0.00      0.00         7\n",
            "          14       1.00      0.05      0.10        19\n",
            "          15       0.54      0.81      0.65       120\n",
            "          16       0.50      0.15      0.24        13\n",
            "          17       0.19      0.09      0.12        35\n",
            "          18       0.60      0.25      0.35        12\n",
            "          19       0.00      0.00      0.00         2\n",
            "          20       0.57      0.16      0.25        25\n",
            "          21       0.00      0.00      0.00        25\n",
            "          22       0.64      0.69      0.67        13\n",
            "          23       0.00      0.00      0.00         3\n",
            "          24       0.00      0.00      0.00        10\n",
            "          25       0.00      0.00      0.00        23\n",
            "          26       0.52      0.67      0.58        78\n",
            "          27       0.71      0.72      0.72        47\n",
            "          28       0.60      0.90      0.72       678\n",
            "          29       0.00      0.00      0.00         9\n",
            "          30       0.59      0.73      0.65        66\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00        15\n",
            "          33       0.00      0.00      0.00        10\n",
            "          34       0.00      0.00      0.00         8\n",
            "          35       0.00      0.00      0.00         2\n",
            "          36       0.52      0.36      0.43        36\n",
            "          37       0.00      0.00      0.00         1\n",
            "          38       0.46      0.71      0.56        72\n",
            "          39       1.00      0.24      0.38        21\n",
            "          40       0.00      0.00      0.00         3\n",
            "          41       0.18      0.12      0.14        60\n",
            "          42       0.50      0.30      0.38        30\n",
            "          43       0.08      0.03      0.04        34\n",
            "          44       0.60      0.47      0.53        19\n",
            "          45       0.83      0.89      0.86        97\n",
            "          46       0.75      0.85      0.80       162\n",
            "          47       0.33      0.06      0.11        16\n",
            "          48       0.00      0.00      0.00        14\n",
            "          49       0.00      0.00      0.00         3\n",
            "          50       0.68      0.88      0.76       186\n",
            "          51       0.53      0.50      0.52        46\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.00      0.00      0.00        19\n",
            "          54       0.60      0.20      0.30        15\n",
            "          55       0.33      0.14      0.20        35\n",
            "          56       0.60      0.28      0.38        32\n",
            "          57       0.00      0.00      0.00        15\n",
            "          58       0.63      0.52      0.57        33\n",
            "          59       1.00      0.58      0.74        12\n",
            "          60       0.43      0.54      0.48       124\n",
            "          61       0.00      0.00      0.00         4\n",
            "          62       0.00      0.00      0.00        12\n",
            "          63       0.43      0.71      0.54       104\n",
            "          64       0.00      0.00      0.00        16\n",
            "          65       0.29      0.18      0.22        62\n",
            "          66       0.70      0.86      0.78        36\n",
            "          67       0.58      0.78      0.67        27\n",
            "          68       0.00      0.00      0.00         4\n",
            "          69       0.00      0.00      0.00         1\n",
            "          70       0.00      0.00      0.00         4\n",
            "          71       0.00      0.00      0.00         4\n",
            "          72       0.44      0.16      0.24        50\n",
            "          73       0.22      0.26      0.24       254\n",
            "          74       0.00      0.00      0.00         6\n",
            "          75       0.00      0.00      0.00         2\n",
            "          76       0.39      0.34      0.36        50\n",
            "          77       0.55      0.46      0.50        24\n",
            "          78       0.40      0.03      0.06        63\n",
            "          79       0.00      0.00      0.00         7\n",
            "          80       0.00      0.00      0.00        23\n",
            "          81       0.00      0.00      0.00         3\n",
            "          82       0.25      0.06      0.10        16\n",
            "          83       0.00      0.00      0.00         2\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.00      0.00      0.00        10\n",
            "          86       0.40      0.17      0.24        23\n",
            "          87       0.61      0.80      0.70        91\n",
            "          88       0.58      0.57      0.57        44\n",
            "          89       0.00      0.00      0.00         6\n",
            "          90       0.27      0.37      0.31        27\n",
            "          91       0.43      0.56      0.49        55\n",
            "          92       0.61      0.68      0.65        44\n",
            "          93       1.00      0.22      0.36         9\n",
            "          94       0.00      0.00      0.00         8\n",
            "          95       0.11      0.02      0.03        59\n",
            "          96       0.00      0.00      0.00         7\n",
            "          97       0.32      0.32      0.32        75\n",
            "          98       0.33      0.21      0.26        28\n",
            "          99       0.42      0.32      0.36        82\n",
            "         100       0.00      0.00      0.00         8\n",
            "         101       0.52      0.76      0.62        74\n",
            "         102       1.00      0.08      0.15        12\n",
            "         103       0.00      0.00      0.00         2\n",
            "         104       0.31      0.40      0.35       221\n",
            "         105       0.00      0.00      0.00         8\n",
            "         106       0.48      0.46      0.47        65\n",
            "\n",
            "    accuracy                           0.52      4197\n",
            "   macro avg       0.28      0.22      0.23      4197\n",
            "weighted avg       0.46      0.52      0.46      4197\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastText - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83        39\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         6\n",
            "           5       0.00      0.00      0.00         8\n",
            "           6       0.35      0.25      0.29        24\n",
            "           7       0.00      0.00      0.00         9\n",
            "           8       0.00      0.00      0.00        10\n",
            "           9       0.50      0.05      0.08        22\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         1\n",
            "          12       0.22      0.12      0.15        17\n",
            "          13       0.00      0.00      0.00         7\n",
            "          14       0.00      0.00      0.00        19\n",
            "          15       0.54      0.81      0.65       120\n",
            "          16       0.40      0.15      0.22        13\n",
            "          17       0.17      0.09      0.11        35\n",
            "          18       0.60      0.25      0.35        12\n",
            "          19       0.00      0.00      0.00         2\n",
            "          20       0.80      0.16      0.27        25\n",
            "          21       0.00      0.00      0.00        25\n",
            "          22       0.64      0.69      0.67        13\n",
            "          23       0.00      0.00      0.00         3\n",
            "          24       0.00      0.00      0.00        10\n",
            "          25       0.00      0.00      0.00        23\n",
            "          26       0.53      0.65      0.58        78\n",
            "          27       0.71      0.72      0.72        47\n",
            "          28       0.60      0.90      0.72       678\n",
            "          29       0.00      0.00      0.00         9\n",
            "          30       0.56      0.74      0.64        66\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00        15\n",
            "          33       0.00      0.00      0.00        10\n",
            "          34       0.00      0.00      0.00         8\n",
            "          35       0.00      0.00      0.00         2\n",
            "          36       0.50      0.36      0.42        36\n",
            "          37       0.00      0.00      0.00         1\n",
            "          38       0.46      0.72      0.56        72\n",
            "          39       1.00      0.14      0.25        21\n",
            "          40       0.00      0.00      0.00         3\n",
            "          41       0.18      0.12      0.14        60\n",
            "          42       0.47      0.27      0.34        30\n",
            "          43       0.13      0.06      0.08        34\n",
            "          44       0.60      0.47      0.53        19\n",
            "          45       0.83      0.89      0.86        97\n",
            "          46       0.76      0.87      0.81       162\n",
            "          47       0.00      0.00      0.00        16\n",
            "          48       0.00      0.00      0.00        14\n",
            "          49       0.00      0.00      0.00         3\n",
            "          50       0.67      0.88      0.76       186\n",
            "          51       0.52      0.48      0.50        46\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.50      0.05      0.10        19\n",
            "          54       0.75      0.20      0.32        15\n",
            "          55       0.20      0.06      0.09        35\n",
            "          56       0.54      0.22      0.31        32\n",
            "          57       0.00      0.00      0.00        15\n",
            "          58       0.64      0.48      0.55        33\n",
            "          59       0.86      0.50      0.63        12\n",
            "          60       0.43      0.56      0.48       124\n",
            "          61       0.00      0.00      0.00         4\n",
            "          62       0.00      0.00      0.00        12\n",
            "          63       0.42      0.72      0.53       104\n",
            "          64       0.00      0.00      0.00        16\n",
            "          65       0.31      0.18      0.22        62\n",
            "          66       0.72      0.86      0.78        36\n",
            "          67       0.58      0.78      0.67        27\n",
            "          68       0.00      0.00      0.00         4\n",
            "          69       0.00      0.00      0.00         1\n",
            "          70       0.00      0.00      0.00         4\n",
            "          71       0.00      0.00      0.00         4\n",
            "          72       0.48      0.20      0.28        50\n",
            "          73       0.23      0.28      0.25       254\n",
            "          74       0.00      0.00      0.00         6\n",
            "          75       0.00      0.00      0.00         2\n",
            "          76       0.36      0.30      0.33        50\n",
            "          77       0.61      0.46      0.52        24\n",
            "          78       0.25      0.02      0.03        63\n",
            "          79       0.00      0.00      0.00         7\n",
            "          80       0.00      0.00      0.00        23\n",
            "          81       0.00      0.00      0.00         3\n",
            "          82       0.20      0.06      0.10        16\n",
            "          83       0.00      0.00      0.00         2\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.00      0.00      0.00        10\n",
            "          86       0.38      0.13      0.19        23\n",
            "          87       0.61      0.79      0.69        91\n",
            "          88       0.58      0.59      0.58        44\n",
            "          89       0.00      0.00      0.00         6\n",
            "          90       0.29      0.41      0.34        27\n",
            "          91       0.45      0.60      0.51        55\n",
            "          92       0.63      0.66      0.64        44\n",
            "          93       1.00      0.11      0.20         9\n",
            "          94       0.00      0.00      0.00         8\n",
            "          95       0.11      0.02      0.03        59\n",
            "          96       0.00      0.00      0.00         7\n",
            "          97       0.31      0.29      0.30        75\n",
            "          98       0.29      0.14      0.19        28\n",
            "          99       0.39      0.30      0.34        82\n",
            "         100       0.00      0.00      0.00         8\n",
            "         101       0.50      0.76      0.61        74\n",
            "         102       1.00      0.08      0.15        12\n",
            "         103       0.00      0.00      0.00         2\n",
            "         104       0.31      0.40      0.35       221\n",
            "         105       0.00      0.00      0.00         8\n",
            "         106       0.47      0.46      0.47        65\n",
            "\n",
            "    accuracy                           0.52      4197\n",
            "   macro avg       0.27      0.22      0.22      4197\n",
            "weighted avg       0.45      0.52      0.46      4197\n",
            "\n",
            "\n",
            "Comparativa Embeddings No Contextuales:\n",
            "          Val Accuracy  Test Accuracy  Val F1 (weighted)  Test F1 (weighted)\n",
            "Word2Vec      0.514892       0.516559           0.461421            0.464531\n",
            "FastText      0.516321       0.515368           0.461289            0.460717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Topic\n",
        "results_topic = run_non_contextual_embeddings(df_train, \"topic\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Embeddings No Contextuales | Target: source =====\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        28\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.00      0.00      0.00        16\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00        28\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.00      0.00      0.00        35\n",
            "           8       0.00      0.00      0.00         9\n",
            "           9       0.24      0.21      0.22        91\n",
            "          10       0.00      0.00      0.00         3\n",
            "          11       0.20      0.05      0.08        20\n",
            "          12       0.00      0.00      0.00        45\n",
            "          13       0.00      0.00      0.00         9\n",
            "          14       0.00      0.00      0.00         7\n",
            "          15       0.50      0.02      0.04        50\n",
            "          16       0.00      0.00      0.00        19\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.00      0.00      0.00         9\n",
            "          19       0.38      0.46      0.42       157\n",
            "          20       0.00      0.00      0.00        13\n",
            "          21       0.39      0.53      0.45       335\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.00      0.00      0.00        29\n",
            "          24       0.00      0.00      0.00        15\n",
            "          25       0.00      0.00      0.00         6\n",
            "          26       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         9\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.00      0.00      0.00        11\n",
            "          30       0.26      0.12      0.16       165\n",
            "          31       0.00      0.00      0.00        10\n",
            "          32       0.00      0.00      0.00         6\n",
            "          33       0.23      0.36      0.28       242\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.00      0.00      0.00        10\n",
            "          36       0.00      0.00      0.00        22\n",
            "          37       0.00      0.00      0.00        53\n",
            "          38       0.00      0.00      0.00         9\n",
            "          39       0.00      0.00      0.00         2\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         3\n",
            "          42       0.00      0.00      0.00         2\n",
            "          43       0.00      0.00      0.00         4\n",
            "          44       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         1\n",
            "          47       0.75      0.23      0.35        13\n",
            "          48       0.00      0.00      0.00        13\n",
            "          49       0.00      0.00      0.00         3\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.00      0.00      0.00         1\n",
            "          52       0.00      0.00      0.00        13\n",
            "          53       0.22      0.44      0.29       122\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         2\n",
            "          56       0.00      0.00      0.00         6\n",
            "          57       0.00      0.00      0.00        20\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.00      0.00      0.00         3\n",
            "          60       0.46      0.19      0.26       167\n",
            "          61       0.00      0.00      0.00         8\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00        69\n",
            "          64       0.00      0.00      0.00         1\n",
            "          65       0.00      0.00      0.00         8\n",
            "          66       0.00      0.00      0.00         4\n",
            "          67       0.00      0.00      0.00         1\n",
            "          68       0.00      0.00      0.00         1\n",
            "          69       0.21      0.27      0.24       242\n",
            "          70       0.00      0.00      0.00         4\n",
            "          71       0.27      0.55      0.37       300\n",
            "          72       0.00      0.00      0.00         8\n",
            "          73       0.00      0.00      0.00         1\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.21      0.05      0.08        58\n",
            "          76       0.30      0.21      0.25        98\n",
            "          77       0.00      0.00      0.00         6\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.38      0.14      0.21        70\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00        18\n",
            "          82       0.00      0.00      0.00        22\n",
            "          83       0.00      0.00      0.00         3\n",
            "          84       0.00      0.00      0.00        27\n",
            "          85       0.00      0.00      0.00        14\n",
            "          86       0.00      0.00      0.00         3\n",
            "          87       0.00      0.00      0.00         4\n",
            "          88       0.00      0.00      0.00        28\n",
            "          89       0.00      0.00      0.00        66\n",
            "          90       0.44      0.63      0.52       163\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         3\n",
            "          93       0.00      0.00      0.00         4\n",
            "          94       0.00      0.00      0.00         3\n",
            "          95       0.00      0.00      0.00        15\n",
            "          96       0.00      0.00      0.00         3\n",
            "          97       0.00      0.00      0.00        24\n",
            "          98       0.00      0.00      0.00         4\n",
            "          99       0.00      0.00      0.00         8\n",
            "         100       0.29      0.24      0.26       153\n",
            "         101       0.14      0.18      0.16       216\n",
            "         102       0.00      0.00      0.00        19\n",
            "         103       0.00      0.00      0.00         8\n",
            "         104       0.00      0.00      0.00         8\n",
            "         105       0.21      0.40      0.28       174\n",
            "         106       0.00      0.00      0.00         1\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.00      0.00      0.00         4\n",
            "         109       0.00      0.00      0.00        13\n",
            "         110       0.33      0.67      0.45       349\n",
            "         111       0.00      0.00      0.00         1\n",
            "         112       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.29      4178\n",
            "   macro avg       0.06      0.05      0.05      4178\n",
            "weighted avg       0.23      0.29      0.24      4178\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastText - Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        28\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.00      0.00      0.00        16\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00        28\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.00      0.00      0.00        35\n",
            "           8       0.00      0.00      0.00         9\n",
            "           9       0.25      0.21      0.23        91\n",
            "          10       0.00      0.00      0.00         3\n",
            "          11       0.25      0.05      0.08        20\n",
            "          12       0.00      0.00      0.00        45\n",
            "          13       0.00      0.00      0.00         9\n",
            "          14       0.00      0.00      0.00         7\n",
            "          15       0.33      0.02      0.04        50\n",
            "          16       0.00      0.00      0.00        19\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.00      0.00      0.00         9\n",
            "          19       0.36      0.45      0.40       157\n",
            "          20       0.00      0.00      0.00        13\n",
            "          21       0.38      0.53      0.44       335\n",
            "          22       0.00      0.00      0.00        10\n",
            "          23       0.00      0.00      0.00        29\n",
            "          24       0.00      0.00      0.00        15\n",
            "          25       0.00      0.00      0.00         6\n",
            "          26       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         9\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.00      0.00      0.00        11\n",
            "          30       0.27      0.13      0.18       165\n",
            "          31       0.00      0.00      0.00        10\n",
            "          32       0.00      0.00      0.00         6\n",
            "          33       0.21      0.31      0.25       242\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.00      0.00      0.00        10\n",
            "          36       0.00      0.00      0.00        22\n",
            "          37       0.00      0.00      0.00        53\n",
            "          38       0.00      0.00      0.00         9\n",
            "          39       0.00      0.00      0.00         2\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         3\n",
            "          42       0.00      0.00      0.00         2\n",
            "          43       0.00      0.00      0.00         4\n",
            "          44       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         1\n",
            "          47       0.67      0.15      0.25        13\n",
            "          48       0.00      0.00      0.00        13\n",
            "          49       0.00      0.00      0.00         3\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.00      0.00      0.00         1\n",
            "          52       0.00      0.00      0.00        13\n",
            "          53       0.21      0.42      0.28       122\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         2\n",
            "          56       0.00      0.00      0.00         6\n",
            "          57       0.00      0.00      0.00        20\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.00      0.00      0.00         3\n",
            "          60       0.44      0.16      0.24       167\n",
            "          61       0.00      0.00      0.00         8\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00        69\n",
            "          64       0.00      0.00      0.00         1\n",
            "          65       0.00      0.00      0.00         8\n",
            "          66       0.00      0.00      0.00         4\n",
            "          67       0.00      0.00      0.00         1\n",
            "          68       0.00      0.00      0.00         1\n",
            "          69       0.21      0.26      0.23       242\n",
            "          70       0.00      0.00      0.00         4\n",
            "          71       0.27      0.54      0.36       300\n",
            "          72       0.00      0.00      0.00         8\n",
            "          73       0.00      0.00      0.00         1\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.14      0.03      0.06        58\n",
            "          76       0.31      0.20      0.25        98\n",
            "          77       0.00      0.00      0.00         6\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.40      0.14      0.21        70\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00        18\n",
            "          82       0.00      0.00      0.00        22\n",
            "          83       0.00      0.00      0.00         3\n",
            "          84       0.00      0.00      0.00        27\n",
            "          85       0.00      0.00      0.00        14\n",
            "          86       0.00      0.00      0.00         3\n",
            "          87       0.00      0.00      0.00         4\n",
            "          88       0.00      0.00      0.00        28\n",
            "          89       0.00      0.00      0.00        66\n",
            "          90       0.44      0.64      0.52       163\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         3\n",
            "          93       0.00      0.00      0.00         4\n",
            "          94       0.00      0.00      0.00         3\n",
            "          95       0.00      0.00      0.00        15\n",
            "          96       0.00      0.00      0.00         3\n",
            "          97       0.00      0.00      0.00        24\n",
            "          98       0.00      0.00      0.00         4\n",
            "          99       0.00      0.00      0.00         8\n",
            "         100       0.27      0.22      0.24       153\n",
            "         101       0.14      0.19      0.16       216\n",
            "         102       0.00      0.00      0.00        19\n",
            "         103       0.00      0.00      0.00         8\n",
            "         104       0.00      0.00      0.00         8\n",
            "         105       0.22      0.40      0.29       174\n",
            "         106       0.00      0.00      0.00         1\n",
            "         107       0.00      0.00      0.00        42\n",
            "         108       0.00      0.00      0.00         4\n",
            "         109       0.00      0.00      0.00        13\n",
            "         110       0.31      0.65      0.42       349\n",
            "         111       0.00      0.00      0.00         1\n",
            "         112       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.28      4178\n",
            "   macro avg       0.05      0.05      0.05      4178\n",
            "weighted avg       0.22      0.28      0.23      4178\n",
            "\n",
            "\n",
            "Comparativa Embeddings No Contextuales:\n",
            "          Val Accuracy  Test Accuracy  Val F1 (weighted)  Test F1 (weighted)\n",
            "Word2Vec      0.280450       0.289373           0.230755            0.237319\n",
            "FastText      0.274707       0.281474           0.224662            0.230272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Para source\n",
        "results_source = run_non_contextual_embeddings(df_train, \"source\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Tabla Comparativa de Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Deep Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendimiento de Modelos de Deep Learning (LSTM & GRU)\n",
            "\n",
            "Estrategia de Embedding   Modelo  Accuracy  Macro-F1\n",
            "            Finetuneado     LSTM    0.4861    0.2399\n",
            "            Finetuneado      GRU    0.3090    0.0401\n",
            "         No Finetuneado     LSTM    0.2342    0.0354\n",
            "         No Finetuneado      GRU    0.1615    0.0026\n",
            "      Word2Vec (Frozen) Word2Vec    0.4934    0.2578\n",
            "   Word2Vec (Fine-tune) Word2Vec    0.1816    0.0059\n",
            "     Word2Vec (Scratch) Word2Vec    0.1615    0.0026\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data_finetune = {\n",
        "    \"Modelo\": [\"LSTM\", \"GRU\"],\n",
        "    \"Estrategia de Embedding\": [\"Finetuneado\", \"Finetuneado\"],\n",
        "    \"Accuracy\": [0.486061, 0.309030  ],\n",
        "    \"Macro-F1\": [0.239902, 0.040144]\n",
        "}\n",
        "\n",
        "data_random = {\n",
        "    \"Modelo\": [\"LSTM\", \"GRU\"],\n",
        "    \"Estrategia de Embedding\": [\"No Finetuneado\", \"No Finetuneado\"],\n",
        "    \"Accuracy\": [0.234215, 0.161544  ],\n",
        "    \"Macro-F1\": [0.035418, 0.002600]\n",
        "}\n",
        "\n",
        "\n",
        "data_word2vec = {\n",
        "    \"Modelo\": [\"Word2Vec\", \"Word2Vec\", \"Word2Vec\"],\n",
        "    \"Estrategia de Embedding\": [\n",
        "        \"Word2Vec (Frozen)\",\n",
        "        \"Word2Vec (Fine-tune)\",\n",
        "        \"Word2Vec (Scratch)\"\n",
        "    ],\n",
        "    \"Accuracy\": [0.493448, 0.181558, 0.161544],\n",
        "    \"Macro-F1\": [0.257780, 0.005892, 0.002600]\n",
        "}\n",
        "\n",
        "# Crear DataFrames\n",
        "df_finetune = pd.DataFrame(data_finetune)\n",
        "df_random = pd.DataFrame(data_random)\n",
        "df_word2vec = pd.DataFrame(data_word2vec)\n",
        "\n",
        "# Unir todos los DataFrames\n",
        "df_deep_learning = pd.concat(\n",
        "    [df_finetune, df_random, df_word2vec],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Redondear métricas para presentación\n",
        "df_deep_learning_display = df_deep_learning.round(4)\n",
        "\n",
        "# Reordenar columnas\n",
        "column_order = [\"Estrategia de Embedding\", \"Modelo\", \"Accuracy\", \"Macro-F1\"]\n",
        "df_deep_learning_display = df_deep_learning_display[column_order]\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Rendimiento de Modelos de Deep Learning (LSTM & GRU)\\n\")\n",
        "print(df_deep_learning_display.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Comparación de Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tradicionales (Shallow Learning)\n",
        "results_traditional = {\n",
        "    \"Modelo/Técnica\": [\"TF-IDF\", \"Bag-of-Words (BoW)\"],\n",
        "    \"Test Accuracy\": [0.7100, 0.6300],\n",
        "    \"Test F1 (weighted)\": [0.7001, 0.6337]  # macro-F1 ponderado\n",
        "}\n",
        "\n",
        "# No contextuales (Word2Vec / FastText)\n",
        "results_non_contextual = {\n",
        "    \"Modelo/Técnica\": [\"Word2Vec\", \"FastText\"],\n",
        "    \"Test Accuracy\": [0.5368, 0.5440],\n",
        "    \"Test F1 (weighted)\": [0.5347, 0.5418]\n",
        "}\n",
        "\n",
        "# Contextuales (Sentence Transformers / BERT)\n",
        "results_contextual = {\n",
        "    \"Modelo/Técnica\": [\"Sentence Transformers\", \"BERT\"],\n",
        "    \"Test Accuracy\": [0.5351, 0.3659],\n",
        "    \"Test F1 (weighted)\": [0.5351, 0.3659]\n",
        "}\n",
        "\n",
        "# --- Crear DataFrames ---\n",
        "df_traditional = pd.DataFrame(results_traditional).assign(Tipo_Embedding=\"Tradicional\")\n",
        "df_non_contextual = pd.DataFrame(results_non_contextual).assign(Tipo_Embedding=\"No Contextual\")\n",
        "df_contextual = pd.DataFrame(results_contextual).assign(Tipo_Embedding=\"Contextual\")\n",
        "\n",
        "# --- Concatenar todos los DataFrames ---\n",
        "df_results = pd.concat([df_traditional, df_non_contextual, df_contextual], ignore_index=True)\n",
        "\n",
        "# --- Ordenar columnas y renombrar ---\n",
        "df_results = df_results[[\"Tipo_Embedding\", \"Modelo/Técnica\", \"Test Accuracy\", \"Test F1 (weighted)\"]]\n",
        "df_results.rename(columns={\"Tipo_Embedding\": \"Tipo de Embedding\",\n",
        "                           \"Test Accuracy\": \"Accuracy\",\n",
        "                           \"Test F1 (weighted)\": \"Macro-F1\"}, inplace=True)\n",
        "\n",
        "# Redondear métricas\n",
        "df_results[[\"Accuracy\", \"Macro-F1\"]] = df_results[[\"Accuracy\", \"Macro-F1\"]].round(4)\n",
        "\n",
        "# --- Mostrar tabla ---\n",
        "print(\"Resultados Consolidados de Modelos de Representación de Texto (Test Set)\\n\")\n",
        "print(df_results.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
