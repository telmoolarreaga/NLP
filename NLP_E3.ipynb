{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqfT1kYc_EOn"
      },
      "source": [
        "# **1. Shallow Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clases en 'bias': bias\n",
            "2    10240\n",
            "0     9750\n",
            "1     7988\n",
            "Name: count, dtype: int64\n",
            "Entrenando Logistic Regression...\n",
            "Entrenando SVM (Linear)...\n"
          ]
        }
      ],
      "source": [
        "# Cargamos el  dataset tokenizado\n",
        "df_train = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "\n",
        "# Revisamos la columna de etiquetas\n",
        "print(\"Clases en 'bias':\", df_train[\"bias\"].value_counts())\n",
        "\n",
        "# Creamos la  carpeta para modelos si no existe\n",
        "os.makedirs(\"data/models\", exist_ok=True)\n",
        "\n",
        "# Calculamos TF-IDF sobre df_train \n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_train[\"tfidf_joined\"])\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "# Separamos el  train/validation \n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "#  Definimos los 4 modelos que vamos a utilizar\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM (Linear)\": SVC(kernel='linear'),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "}\n",
        "\n",
        "# Entrenamos y evaluamos los modelos\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Entrenando {name}...\")\n",
        "    model.fit(X_tr, y_tr)\n",
        "    y_pred = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred, average='macro')\n",
        "    results[name] = {\"Accuracy\": acc, \"Macro-F1\": f1}\n",
        "    # Guardar modelo\n",
        "    pickle.dump(model, open(f\"data/models/{name.replace(' ', '_').lower()}.pkl\", \"wb\"))\n",
        "\n",
        "#  Guardamos el  vectorizador\n",
        "pickle.dump(tfidf_vectorizer, open(\"data/features/tfidf_vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "# Resultados\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nResultados comparativos de Shallow Learning:\")\n",
        "print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para evaluar los modelos hemos usado Accuracy y Macro-F1, métricas adecuadas para problemas de clasificación con clases desbalanceadas.\n",
        "Los cuatro modelos presentan resultados parecidos. Sin embargo, XGBoost ofrece el mejor rendimiento, mostrando que puede capturar patrones complejos del sesgo ideológico mejor que modelos lineales o Random Forest.\n",
        "Por otro lado, los modelos lineales funcionan razonablemente bien, lo que indica que el sesgo tiene señales lineales claras en los términos más frecuentes.\n",
        "Elegimos estos cuatro modelos (Logistic Regression, SVM, Random Forest y XGBoost) para cubrir tanto enfoques lineales como basados en árboles, y utilizar el dataset tokenizado con TF-IDF para representar el texto en forma dispersa, adecuada para Shallow Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztbnwVJVWSgX"
      },
      "source": [
        "# **3. Modelos Deep**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta parte nos vamos a enfocar en la clasificación del sesgo. Los motivos son los siguientes:\n",
        "\n",
        "1- Detectar la orientación política de una noticia es la tarea principal del proyecto. Además, esta tarea permite evaluar cómo los modelos y embeddings capturan matices semánticos y patrones discursivos.\n",
        "2- Una vez concluida la clasificación de sesgo, se puede reutilizar la pipeline para las demás tareas de clasificación (medio y temática).\n",
        "3- La columna bias presenta un número moderadamente equilibrado de ejemplos por clase, lo que permite ajustar la arquitectura y los hiperparámetros de manera controlada antes de afrontar tareas más complejas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las razones de haber elegido las combinaciones de embeddings y arquitecturas de redes neuronales para abordar la clasificación del sesgo ideológicoson las siguientes:\n",
        "\n",
        "1- Comparación de embeddings no contextuales y contextuales:\n",
        "    -Los embeddings no contextuales como, Word2Vecy y FastText, permiten capturar relaciones semánticas entre palabras de manera estática.\n",
        "    -Los embeddings contextuales como, Sentence Transformers y BERT, capturan el significado de las palabras según su contexto en la frase, lo que es clave para detectar matices ideológicos más complejos.\n",
        "\n",
        "2- Exploración de diferentes estrategias de embeddings:\n",
        "    -Word2Vec congelado: usar embeddings preentrenados sin actualizar durante el entrenamiento, para evaluar la capacidad de vectores fijos.\n",
        "    -Word2Vec fine-tune: ajustar los vectores durante el entrenamiento para adaptarlos al corpus específico.\n",
        "    -Word2Vec “from scratch”: entrenar desde cero sobre el dataset, para capturar patrones propios del corpus.\n",
        "    -Para los embeddings contextuales, se compara Sentence Transformers preentrenado frente a BERT, con fine-tuning parcial o total según la arquitectura de la red.\n",
        "\n",
        "3- Elección de arquitecturas de redes neuronales:\n",
        "    -Redes totalmente conectadas (Dense): adecuadas para embeddings agregados o promedio de vectores de palabras.\n",
        "    -Redes recurrentes (LSTM/GRU): capturan secuencias y dependencias entre palabras, esenciales para comprender el flujo discursivo en los artículos.\n",
        "    -CNN para texto: permiten identificar patrones locales de n-gramas que son relevantes en la clasificación de sesgo.\n",
        "\n",
        "4- Razonamiento general:\n",
        "    -Combinar diferentes tipos de embeddings y arquitecturas permite evaluar cuál representa mejor la información semántica y estilística para cada tarea.\n",
        "    -Esta estrategia también permite analizar cómo el fine-tuning de embeddings impacta en la capacidad del modelo de captar señales ideológicas, frente a vectores preentrenados fijos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 708ms/step - accuracy: 0.4235 - loss: 1.0572 - val_accuracy: 0.4870 - val_loss: 0.9976\n",
            "Epoch 2/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 658ms/step - accuracy: 0.5253 - loss: 0.9565 - val_accuracy: 0.4982 - val_loss: 0.9742\n",
            "Epoch 3/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 589ms/step - accuracy: 0.5179 - loss: 0.9679 - val_accuracy: 0.4089 - val_loss: 1.0904\n",
            "Epoch 4/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 648ms/step - accuracy: 0.5913 - loss: 0.8691 - val_accuracy: 0.5213 - val_loss: 0.9993\n",
            "Epoch 5/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 747ms/step - accuracy: 0.6951 - loss: 0.7005 - val_accuracy: 0.5259 - val_loss: 0.9890\n",
            "Epoch 6/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 689ms/step - accuracy: 0.7779 - loss: 0.5381 - val_accuracy: 0.5381 - val_loss: 1.0824\n",
            "Epoch 7/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 694ms/step - accuracy: 0.8482 - loss: 0.3815 - val_accuracy: 0.5406 - val_loss: 1.2616\n",
            "Epoch 8/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 708ms/step - accuracy: 0.9052 - loss: 0.2507 - val_accuracy: 0.5420 - val_loss: 1.4579\n",
            "Epoch 9/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 684ms/step - accuracy: 0.9388 - loss: 0.1658 - val_accuracy: 0.5398 - val_loss: 1.7190\n",
            "Epoch 10/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 762ms/step - accuracy: 0.9613 - loss: 0.1075 - val_accuracy: 0.5352 - val_loss: 1.9724\n",
            "Epoch 1/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 600ms/step - accuracy: 0.4057 - loss: 1.0765 - val_accuracy: 0.4380 - val_loss: 1.0571\n",
            "Epoch 2/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 607ms/step - accuracy: 0.4961 - loss: 0.9955 - val_accuracy: 0.4796 - val_loss: 0.9941\n",
            "Epoch 3/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 636ms/step - accuracy: 0.5872 - loss: 0.8674 - val_accuracy: 0.5080 - val_loss: 0.9678\n",
            "Epoch 4/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 564ms/step - accuracy: 0.6747 - loss: 0.7306 - val_accuracy: 0.5216 - val_loss: 1.0024\n",
            "Epoch 5/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 553ms/step - accuracy: 0.7569 - loss: 0.5707 - val_accuracy: 0.5218 - val_loss: 1.1106\n",
            "Epoch 6/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 578ms/step - accuracy: 0.8337 - loss: 0.4123 - val_accuracy: 0.5250 - val_loss: 1.2220\n",
            "Epoch 7/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 581ms/step - accuracy: 0.8907 - loss: 0.2813 - val_accuracy: 0.5348 - val_loss: 1.5378\n",
            "Epoch 8/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 575ms/step - accuracy: 0.9290 - loss: 0.1905 - val_accuracy: 0.5350 - val_loss: 1.6782\n",
            "Epoch 9/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 532ms/step - accuracy: 0.9532 - loss: 0.1284 - val_accuracy: 0.5382 - val_loss: 2.0142\n",
            "Epoch 10/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 580ms/step - accuracy: 0.9700 - loss: 0.0864 - val_accuracy: 0.5357 - val_loss: 2.3297\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 204ms/step\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 154ms/step\n",
            "Resultados comparativos de Deep Learning:\n",
            "      Accuracy  Macro-F1\n",
            "LSTM  0.535204  0.535763\n",
            "GRU   0.535740  0.532800\n"
          ]
        }
      ],
      "source": [
        "# Cargamos el dataset tokenizado\n",
        "df_train = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
        "y = df_train[\"bias\"].values\n",
        "\n",
        "# Codificamos los labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_cat = to_categorical(y_encoded)\n",
        "\n",
        "# Split train/validation\n",
        "X_tr_text, X_val_text, y_tr, y_val = train_test_split(\n",
        "    df_train[\"tokens\"], y_cat, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Cargamos el Word2Vec preentrenado de la anterior entrega\n",
        "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
        "embedding_dim = w2v_model.vector_size\n",
        "\n",
        "# Creamos el vocabulario e índices\n",
        "word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "vocab_size = len(word_index) + 1  # +1 para padding\n",
        "\n",
        "# Convertimos los tokens a índices\n",
        "def tokens_to_indices(tokens, word_index):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "X_tr_idx = [tokens_to_indices(t, word_index) for t in X_tr_text]\n",
        "X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
        "\n",
        "# Aplciamos padding\n",
        "max_seq_len = 200\n",
        "X_tr_pad = pad_sequences(X_tr_idx, maxlen=max_seq_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "# Creamos la matriz de embedding \n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Definimos y entrenaos los modelos \n",
        "\n",
        "def build_rnn(model_type='LSTM'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size,\n",
        "                        output_dim=embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=max_seq_len,\n",
        "                        trainable=True))  # Fine-tune embeddings\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# LSTM\n",
        "lstm_model = build_rnn('LSTM')\n",
        "lstm_history = lstm_model.fit(X_tr_pad, y_tr,\n",
        "                              validation_data=(X_val_pad, y_val),\n",
        "                              epochs=10,\n",
        "                              batch_size=64)\n",
        "\n",
        "# GRU\n",
        "gru_model = build_rnn('GRU')\n",
        "gru_history = gru_model.fit(X_tr_pad, y_tr,\n",
        "                            validation_data=(X_val_pad, y_val),\n",
        "                            epochs=10,\n",
        "                            batch_size=64)\n",
        "\n",
        "# Evaluamos los modelos\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lstm = lstm_model.predict(X_val_pad, batch_size=64)\n",
        "y_pred_gru = gru_model.predict(X_val_pad, batch_size=64)\n",
        "\n",
        "y_pred_lstm_labels = np.argmax(y_pred_lstm, axis=1)\n",
        "y_pred_gru_labels = np.argmax(y_pred_gru, axis=1)\n",
        "y_val_labels = np.argmax(y_val, axis=1)\n",
        "\n",
        "# Métricas\n",
        "results = {\n",
        "    'LSTM': {\n",
        "        'Accuracy': accuracy_score(y_val_labels, y_pred_lstm_labels),\n",
        "        'Macro-F1': f1_score(y_val_labels, y_pred_lstm_labels, average='macro')\n",
        "    },\n",
        "    'GRU': {\n",
        "        'Accuracy': accuracy_score(y_val_labels, y_pred_gru_labels),\n",
        "        'Macro-F1': f1_score(y_val_labels, y_pred_gru_labels, average='macro')\n",
        "    }\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"Resultados comparativos de Deep Learning:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-Rendimiento general:\n",
        "   -Ambos modelos muestran resultados muy similares, con valores en torno al 53–54%, tanto en Accuracy como en Macro-F1. Esto indica que:\n",
        "       -Los dos modelos capturan de forma parecida los patrones secuenciales del sesgo ideológico.\n",
        "       -No existe una ventaja clara de ninguno de los dos modelos neuronales en este dataset.\n",
        "-Interpretación:\n",
        "    -El rendimiento indica que el sesgo ideológico es una tarea difícil incluso para modelos neuronales. \n",
        "    -Puede que los textos no tengan suficiente señal secuencial para que LSTM/GRU destaquen claramente.\n",
        "-Conclusión:\n",
        "    -Ambos modelos presentan un rendimiento equivalente, pero al ser  ligeramente superior, hemos decidido usar LSTM  como baseline de deep learning. Sin embargo, estas arquitecturas probablemente no capturan matices ideológicos complejos, por lo que se es necesario explorar  modelos más potentes como BERT o RoBERTa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvy29jCJWbqI"
      },
      "source": [
        "# **5. Tabla Comaprativa de Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7iaL0qQWhoh"
      },
      "source": [
        "# **6. Interpretabilidad**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
