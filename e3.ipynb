{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d16e4a",
   "metadata": {},
   "source": [
    "Shallow Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cebd094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Usuario\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ed06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_pipeline(df, target_col):\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    # Preparar texto\n",
    "    if \"text_joined\" not in df.columns:\n",
    "        df[\"text_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    texts = df[\"text_joined\"].astype(str).tolist()\n",
    "    labels = df[target_col].tolist()\n",
    "\n",
    "    # Codificar etiquetas\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "\n",
    "    # Train/validation split\n",
    "    X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "        texts, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Vectorización TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))\n",
    "    X_train = vectorizer.fit_transform(X_train_text)\n",
    "    X_val = vectorizer.transform(X_val_text)\n",
    "\n",
    "    # Definición de modelos\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "        \"LinearSVC\": LinearSVC(),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=150, n_jobs=-1),\n",
    "        \"XGBoost\": XGBClassifier(n_estimators=200, eval_metric=\"mlogloss\", tree_method=\"hist\", n_jobs=-1)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Entrenar, evaluar y guardar modelos\n",
    "    os.makedirs(\"data/models\", exist_ok=True)\n",
    "    for name, model in models.items():\n",
    "        print(f\"Entrenando {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        results[name] = {\n",
    "            \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "            \"Macro-F1\": f1_score(y_val, y_pred, average=\"macro\")\n",
    "        }\n",
    "        # Guardar modelo\n",
    "        pickle.dump(model, open(f\"data/models/{name.replace(' ', '_').lower()}.pkl\", \"wb\"))\n",
    "\n",
    "    # Guardar vectorizador\n",
    "    os.makedirs(\"data/features\", exist_ok=True)\n",
    "    pickle.dump(vectorizer, open(\"data/features/tfidf_vectorizer.pkl\", \"wb\"))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c8fa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Logistic Regression...\n",
      "Entrenando LinearSVC...\n",
      "Entrenando Random Forest...\n",
      "Entrenando XGBoost...\n",
      "                     Accuracy  Macro-F1\n",
      "Logistic Regression  0.702109  0.700091\n",
      "LinearSVC            0.698713  0.696551\n",
      "Random Forest        0.683881  0.679829\n",
      "XGBoost              0.748392  0.746699\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
    "\n",
    "# Shallow learning para bias\n",
    "results_bias = shallow_pipeline(df, \"bias\")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar resultados\n",
    "\n",
    "print(pd.DataFrame(results_bias).T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcab046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Logistic Regression...\n"
     ]
    }
   ],
   "source": [
    "# Shallow learning para topic\n",
    "results_topic = shallow_pipeline(df, \"topic\")\n",
    "\n",
    "print(pd.DataFrame(results_topic).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow learning para source\n",
    "results_source = shallow_pipeline(df, \"source\")\n",
    "print(pd.DataFrame(results_source).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424aa507",
   "metadata": {},
   "source": [
    "Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5745e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# -----------------------------\n",
    "# Función para entrenar y evaluar\n",
    "# -----------------------------\n",
    "def deep_learning_task(task_label, df, w2v_model, max_seq_len=100, epochs=3, batch_size=64):\n",
    "    print(f\"\\n=== Tarea: {task_label} ===\")\n",
    "    \n",
    "    # Datos y etiquetas\n",
    "    texts = df[\"tokens\"].tolist()\n",
    "    labels = df[task_label].tolist()\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "    \n",
    "    # Split train/val\n",
    "    X_tr_text, X_val_text, y_tr, y_val = train_test_split(\n",
    "        texts, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Word2Vec embeddings\n",
    "    embedding_dim = w2v_model.vector_size\n",
    "    word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
    "    vocab_size = len(word_index) + 1\n",
    "    \n",
    "    def tokens_to_indices(tokens, word_index):\n",
    "        return [word_index[t] for t in tokens if t in word_index]\n",
    "    \n",
    "    X_tr_idx = [tokens_to_indices(t, word_index) for t in X_tr_text]\n",
    "    X_val_idx = [tokens_to_indices(t, word_index) for t in X_val_text]\n",
    "    \n",
    "    X_tr_pad = pad_sequences(X_tr_idx, maxlen=max_seq_len, padding='post')\n",
    "    X_val_pad = pad_sequences(X_val_idx, maxlen=max_seq_len, padding='post')\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    \n",
    "    # Función para crear modelo\n",
    "    def build_rnn(model_type='LSTM', embedding_matrix=None, trainable=True):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_seq_len,\n",
    "                            trainable=trainable))\n",
    "        if model_type == 'LSTM':\n",
    "            model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "        elif model_type == 'GRU':\n",
    "            model.add(GRU(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "        model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "        model.compile(optimizer=Adam(1e-3),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    # Entrenamiento y evaluación\n",
    "    results = {}\n",
    "    for model_type in ['LSTM', 'GRU']:\n",
    "        for emb_type, trainable, emb_matrix in [\n",
    "            ('Word2Vec Frozen', False, embedding_matrix),\n",
    "            ('Word2Vec Fine-tune', True, embedding_matrix),\n",
    "            ('Word2Vec Scratch', True, np.random.normal(size=(vocab_size, embedding_dim)))\n",
    "        ]:\n",
    "            print(f\"\\nEntrenando {model_type} con {emb_type}...\")\n",
    "            model = build_rnn(model_type=model_type, embedding_matrix=emb_matrix, trainable=trainable)\n",
    "            model.fit(X_tr_pad, y_tr, validation_data=(X_val_pad, y_val),\n",
    "                      epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "            preds = np.argmax(model.predict(X_val_pad, batch_size=batch_size), axis=1)\n",
    "            acc = accuracy_score(y_val, preds)\n",
    "            f1 = f1_score(y_val, preds, average='macro')\n",
    "            results[f\"{model_type} + {emb_type}\"] = (acc, f1)\n",
    "            print(f\"{model_type} + {emb_type} → Accuracy: {acc:.4f} | Macro-F1: {f1:.4f}\")\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    results_df = pd.DataFrame(results, index=['Accuracy','Macro-F1']).T\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Resultados: {task_label}\")\n",
    "    print(\"==============================\")\n",
    "    print(results_df)\n",
    "    return results_df\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar dataset y Word2Vec\n",
    "# -----------------------------\n",
    "df = pd.read_pickle(\"data/data_clean/train_tokenized.pkl\")\n",
    "w2v_model = Word2Vec.load(\"data/embeddings/word2vec.model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc572ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bias = deep_learning_task(\"bias\", df, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_medio = deep_learning_task(\"source\", df, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tema = deep_learning_task(\"topic\", df, w2v_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
